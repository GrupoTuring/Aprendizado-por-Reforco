{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "O algoritmo de Q-Learning é um dos algoritmos do grupo TD (Temporal difference, diferença temporal), que estimam a função de valor por meio de uma estimativa a partir de outra estimativa, processo chamado _bootstrapping_.\n",
    "\n",
    "## Motivação\n",
    "\n",
    "Assim como nos métodos de Monte Carlo, os métodos TD tem vantagens sobre os métodos de Programação Dinâmica (DP) por não dependerem de um modelo do ambiente para seu funcionamento, sendo capazes de aprender diretamente com a experiência.\n",
    "\n",
    "No entanto, os métodos TD têm uma vantagem sobre os métodos de Monte Carlo (MC), pois são capazes de aprendizado _online_, ou seja, aprendem com a passagem do episódio, enquanto os métodos de MC precisam chegar no final do episódio para iniciar o aprendizado. Em episódios longos ou em ambientes que não estão divididos em episódios, os métodos MC se tornam inviáveis.\n",
    "\n",
    "## Teoria\n",
    "\n",
    "O objetivo do algoritmo é encontrar a função de *Valor Estado-Ação* da política ótima, ou seja, a função que melhor representa os valores *q* para cada par estado-ação do ambiente. A equação da função se encontra abaixo, onde o valor q é dado pelo valor esperado do retorno G, dado um par estado-ação:\n",
    "\n",
    "![funcao q](https://camo.githubusercontent.com/003498f344a099b4d34d45aa2bc0841e562fa01e/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f715f5c706928732c61292673706163653b3d2673706163653b5c6d6174686f707b5c6d61746862627b457d5f5c70697d5b7b475f747d7c7b535f743d732c2673706163653b415f743d617d5d)\n",
    "\n",
    "O algoritmo de Q-Learning é extremamente versátil dentro dos métodos TD, por ser um algoritmo _off-policy_, ou seja, seu aprendizado não depende da política que está sendo seguida, diferente de outros métodos TD, como SARSA. Com isso, o treinamento é mais eficiente, pois pode usar experiências armazenadas em seu treinamento, não somente o que o agente está observando no episódio em si.\n",
    "\n",
    "## Política\n",
    "\n",
    "A política adotada é a chamada ε-greedy (ε-guloso). Um número aleatório no intervalo entre 0 e 1 é comparado com ε. Se o número aleatório for menor do que o valor de ε, a ação tomada vai ser aleatória, de forma a explorar o ambiente. Caso contrário, a ação vai ser a que possui maior valor Q, de acordo com as estimativas atuais que o algoritmo possui. Dessa forma, o agente tomará uma ação aleatória com probabilidade ε.\n",
    "\n",
    "## Algoritmo\n",
    "\n",
    "O algoritmo consiste na atualização das estimativas dos valores Q(S, A) de acordo com a seguinte expressão:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?Q(S,&space;A)&space;\\leftarrow&space;Q(S,&space;A)&space;&plus;&space;\\alpha&space;[R&space;&plus;&space;\\gamma&space;\\max_{a}Q(S',&space;a)&space;-&space;Q(S,&space;A)]\" title=\"Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma \\max_{a}Q(S', a) - Q(S, A)]\" />\n",
    "\n",
    "De forma resumida, o valor Q(S,A) é atualizado fazendo uma \"correção\" com \"taxa de aprendizado\" &alpha;, considerando a recompensa R recebida com a escolha da ação e a ação futura que maximiza o valor Q do próximo estado (S'), descontada de um fator &gamma;. É interessante ressaltar que todas as atualizações são feitas com as estimativas, tanto do presente quanto do futuro, que vão se aprimorando até se aproximar da função q da política ótima. É possível enxergar a expressão que multiplica &alpha; como um erro, pois temos R + &gamma; maxQ(S', a) como nosso _objetivo_ e Q(S, a) nossa _estimativa_. A diferença dos dois fatores é portanto um erro. No contexto de algoritmos TD, este erro é chamado _TD-error_.\n",
    "\n",
    "O funcionamento do algoritmo ao longo dos episódios pode ser visto no pseudocódigo a seguir:\n",
    "\n",
    "![Algoritmo de Q-Learning](imgs/q-learning.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código\n",
    "\n",
    "\n",
    "Inicialmente, iremos definir a classe do nosso agente, com as principais funções necessárias para seu funcionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Classe do agente que seguirá o algoritmo de Q-Learning\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon, decay, epsilon_min, alpha, gamma, n_actions):\n",
    "        \"\"\"\n",
    "        Inicializa os parâmetros do algoritmo, assim como a  'tabela' Q\n",
    "\n",
    "         Parâmetros\n",
    "        ----------\n",
    "        epsilon: float\n",
    "            Probabilidade de tomar ação aleatória.\n",
    "        decay: float\n",
    "            Taxa de decaimento do epsilon por episódio.\n",
    "        epsilon_min: float\n",
    "            Valor mínimo de epsilon.\n",
    "        alpha: float\n",
    "            Taxa de aprendizado.\n",
    "        gamma: float\n",
    "            Fator de desconto.\n",
    "        n_actions: int\n",
    "            Número de ações possíveis no ambiente\n",
    "        \"\"\"\n",
    "\n",
    "        self.Q = {} #tabela Q\n",
    "        self.n_actions = n_actions #número possível de ações, fornecido pelo env\n",
    "\n",
    "        # parâmetros para o epsilon-decay\n",
    "        self.epsilon = epsilon #epsilon inicial\n",
    "        self.decay = decay #taxa de decaimento\n",
    "        self.epsilon_min = epsilon_min #epsilon minimo\n",
    "\n",
    "        self.alpha = alpha #taxa de aprendizado\n",
    "        self.gamma = gamma #peso das recompensas futuras\n",
    "\n",
    "    def choose_action(self, env, state):\n",
    "        \"\"\"\n",
    "        Escolhe a ação do agente, de acordo com a política epsilon-greedy\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        env: Environment\n",
    "            Ambiente sendo utilizado.\n",
    "        state: np.array\n",
    "            Estado atual do agente\n",
    "\n",
    "        Retorna\n",
    "        ----------\n",
    "        action: int\n",
    "            Ação a ser tomada.\n",
    "        \"\"\"\n",
    "        if state not in self.Q.keys(): #acrescenta estado na tabela\n",
    "            self.Q[state] = [0] * self.n_actions\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = env.action_space.sample() #ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state]) #melhor ação\n",
    "        return action\n",
    "\n",
    "    def epsilon_decay(self):\n",
    "        \"\"\"\n",
    "        Função que faz o decaimento de epsilon a cada episódio\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon*self.decay, self.epsilon_min)\n",
    "\n",
    "    def updates_Q(self, state, action, next_state, reward):\n",
    "        \"\"\"\n",
    "        Função que atualiza a tabela Q, de acordo com a equação do Q-Learning\n",
    "        \"\"\"\n",
    "        if state not in self.Q.keys():\n",
    "            self.Q[state] = [0] * self.n_actions\n",
    "        if next_state not in self.Q.keys():\n",
    "            self.Q[next_state] = [0] * self.n_actions\n",
    "\n",
    "        self.Q[state][action] = self.Q[state][action] + self.alpha*(reward + self.gamma*np.max(self.Q[next_state]) - self.Q[state][action])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já possuímos o algoritmo de Q-Learning programado, podemos inicializar o ambiente que será utilizado. No caso, usaremos o Taxi-V3 do gym. Além disso, verificamos o número de ações existentes no ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos então o nosso agente, passando os parâmetros necessários para sua inicalização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(n_actions= n_actions,\n",
    "                       epsilon= 0.99,\n",
    "                       decay= 0.9999,\n",
    "                       epsilon_min= 0.01,\n",
    "                       alpha = 0.1,\n",
    "                       gamma = 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com nosso agente definido, entramos então no loop de treinamento, onde serão executados 10 000 episódios do ambiente, de acordo com o código a seguir: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000: return -776, average returns -693.9, epsilon 0.90\n",
      "Episode 2000: return -391, average returns -412.0, epsilon 0.81\n",
      "Episode 3000: return -488, average returns -223.6, epsilon 0.73\n",
      "Episode 4000: return -76, average returns -137.1, epsilon 0.66\n",
      "Episode 5000: return -153, average returns -96.0, epsilon 0.60\n",
      "Episode 6000: return -34, average returns -71.7, epsilon 0.54\n",
      "Episode 7000: return -49, average returns -52.4, epsilon 0.49\n",
      "Episode 8000: return 3, average returns -40.9, epsilon 0.44\n",
      "Episode 9000: return -32, average returns -33.1, epsilon 0.40\n",
      "Episode 10000: return -27, average returns -25.1, epsilon 0.36\n",
      "Episode 11000: return -13, average returns -19.5, epsilon 0.33\n",
      "Episode 12000: return -9, average returns -16.6, epsilon 0.30\n",
      "Episode 13000: return -1, average returns -12.7, epsilon 0.27\n",
      "Episode 14000: return -7, average returns -9.3, epsilon 0.24\n",
      "Episode 15000: return -6, average returns -7.8, epsilon 0.22\n",
      "Episode 16000: return 3, average returns -5.5, epsilon 0.20\n",
      "Episode 17000: return -50, average returns -3.7, epsilon 0.18\n",
      "Episode 18000: return -19, average returns -2.3, epsilon 0.16\n",
      "Episode 19000: return -7, average returns -1.8, epsilon 0.15\n",
      "Episode 20000: return -40, average returns -0.6, epsilon 0.13\n",
      "Episode 21000: return 0, average returns 0.6, epsilon 0.12\n",
      "Episode 22000: return -2, average returns 1.6, epsilon 0.11\n",
      "Episode 23000: return 5, average returns 2.2, epsilon 0.10\n",
      "Episode 24000: return 4, average returns 2.8, epsilon 0.09\n",
      "Episode 25000: return 2, average returns 2.8, epsilon 0.08\n",
      "Episode 26000: return 7, average returns 4.0, epsilon 0.07\n",
      "Episode 27000: return 4, average returns 3.9, epsilon 0.07\n",
      "Episode 28000: return 9, average returns 4.9, epsilon 0.06\n",
      "Episode 29000: return 10, average returns 5.0, epsilon 0.05\n",
      "Episode 30000: return -7, average returns 5.0, epsilon 0.05\n",
      "Episode 31000: return -7, average returns 5.5, epsilon 0.04\n",
      "Episode 32000: return 7, average returns 5.5, epsilon 0.04\n",
      "Episode 33000: return 9, average returns 5.9, epsilon 0.04\n",
      "Episode 34000: return 4, average returns 6.1, epsilon 0.03\n",
      "Episode 35000: return 8, average returns 6.2, epsilon 0.03\n",
      "Episode 36000: return 6, average returns 6.3, epsilon 0.03\n",
      "Episode 37000: return 7, average returns 6.7, epsilon 0.02\n",
      "Episode 38000: return 8, average returns 6.7, epsilon 0.02\n",
      "Episode 39000: return 14, average returns 6.9, epsilon 0.02\n",
      "Episode 40000: return 5, average returns 6.9, epsilon 0.02\n",
      "Episode 41000: return 12, average returns 7.0, epsilon 0.02\n",
      "Episode 42000: return 5, average returns 7.0, epsilon 0.01\n",
      "Episode 43000: return 14, average returns 7.0, epsilon 0.01\n",
      "Episode 44000: return 7, average returns 7.2, epsilon 0.01\n",
      "Episode 45000: return 13, average returns 7.3, epsilon 0.01\n",
      "Episode 46000: return 8, average returns 7.4, epsilon 0.01\n",
      "Episode 47000: return -3, average returns 7.5, epsilon 0.01\n",
      "Episode 48000: return 13, average returns 7.3, epsilon 0.01\n",
      "Episode 49000: return 13, average returns 7.6, epsilon 0.01\n",
      "Episode 50000: return 6, average returns 7.6, epsilon 0.01\n",
      "Episode 51000: return 7, average returns 7.3, epsilon 0.01\n",
      "Episode 52000: return 8, average returns 7.5, epsilon 0.01\n",
      "Episode 53000: return 4, average returns 7.4, epsilon 0.01\n",
      "Episode 54000: return 10, average returns 7.4, epsilon 0.01\n",
      "Episode 55000: return 7, average returns 7.4, epsilon 0.01\n",
      "Episode 56000: return 5, average returns 7.4, epsilon 0.01\n",
      "Episode 57000: return 5, average returns 7.5, epsilon 0.01\n",
      "Episode 58000: return 6, average returns 7.3, epsilon 0.01\n",
      "Episode 59000: return 10, average returns 7.5, epsilon 0.01\n",
      "Episode 60000: return 9, average returns 7.4, epsilon 0.01\n",
      "Episode 61000: return 9, average returns 7.5, epsilon 0.01\n",
      "Episode 62000: return 7, average returns 7.4, epsilon 0.01\n",
      "Episode 63000: return 7, average returns 7.4, epsilon 0.01\n",
      "Episode 64000: return 7, average returns 7.4, epsilon 0.01\n",
      "Episode 65000: return 9, average returns 7.4, epsilon 0.01\n",
      "Episode 66000: return 12, average returns 7.4, epsilon 0.01\n",
      "Episode 67000: return 5, average returns 7.5, epsilon 0.01\n",
      "Episode 68000: return 7, average returns 7.2, epsilon 0.01\n",
      "Episode 69000: return 10, average returns 7.4, epsilon 0.01\n",
      "Episode 70000: return 6, average returns 7.5, epsilon 0.01\n",
      "Episode 71000: return 10, average returns 7.4, epsilon 0.01\n",
      "Episode 72000: return 7, average returns 7.6, epsilon 0.01\n",
      "Episode 73000: return 7, average returns 7.3, epsilon 0.01\n",
      "Episode 74000: return 15, average returns 7.5, epsilon 0.01\n",
      "Episode 75000: return 4, average returns 7.3, epsilon 0.01\n",
      "Episode 76000: return 6, average returns 7.4, epsilon 0.01\n",
      "Episode 77000: return 6, average returns 7.5, epsilon 0.01\n",
      "Episode 78000: return 5, average returns 7.3, epsilon 0.01\n",
      "Episode 79000: return 6, average returns 7.4, epsilon 0.01\n",
      "Episode 80000: return 8, average returns 7.5, epsilon 0.01\n",
      "Episode 81000: return 5, average returns 7.6, epsilon 0.01\n",
      "Episode 82000: return 12, average returns 7.3, epsilon 0.01\n",
      "Episode 83000: return 8, average returns 7.5, epsilon 0.01\n",
      "Episode 84000: return 9, average returns 7.4, epsilon 0.01\n",
      "Episode 85000: return 8, average returns 7.4, epsilon 0.01\n",
      "Episode 86000: return 11, average returns 7.4, epsilon 0.01\n",
      "Episode 87000: return 7, average returns 7.4, epsilon 0.01\n",
      "Episode 88000: return 10, average returns 7.5, epsilon 0.01\n",
      "Episode 89000: return 11, average returns 7.6, epsilon 0.01\n",
      "Episode 90000: return 6, average returns 7.3, epsilon 0.01\n",
      "Episode 91000: return 3, average returns 7.3, epsilon 0.01\n",
      "Episode 92000: return 7, average returns 7.5, epsilon 0.01\n",
      "Episode 93000: return 6, average returns 7.4, epsilon 0.01\n",
      "Episode 94000: return 4, average returns 7.2, epsilon 0.01\n",
      "Episode 95000: return 11, average returns 7.6, epsilon 0.01\n",
      "Episode 96000: return 11, average returns 7.4, epsilon 0.01\n",
      "Episode 97000: return 7, average returns 7.4, epsilon 0.01\n",
      "Episode 98000: return 7, average returns 7.5, epsilon 0.01\n",
      "Episode 99000: return 11, average returns 7.4, epsilon 0.01\n",
      "Episode 100000: return 10, average returns 7.3, epsilon 0.01\n"
     ]
    }
   ],
   "source": [
    "returns_list = []\n",
    "\n",
    "for episode in range(1, 100001): #episodios\n",
    "    #reinicia variaveis e o estado no comeco de cada episodio\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    returns = 0\n",
    "    \n",
    "    \n",
    "    while not done: #enquanto o episodio nao termina\n",
    "        action = agent.choose_action(env, state)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        agent.updates_Q(state, action, next_state, reward)\n",
    "\n",
    "        #atualiza valores do retorno e do estado atual\n",
    "        returns += reward\n",
    "        state = next_state\n",
    "    \n",
    "    agent.epsilon_decay() #decai o epsilon\n",
    "    returns_list.append(returns) \n",
    "\n",
    "    if episode %1000 == 0:\n",
    "        print(f\"Episode {episode}: return {returns}, average returns {np.mean(returns_list[-1000:]):.1f}, epsilon {agent.epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que o tivemos bons resultados após o treinamento, tendo um retorno médio bem superior ao inicial. A evolução do retorno médio ao longo do treino pode ser conferida no gráfico a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = pd.Series(returns_list)\n",
    "\n",
    "plt.plot(d.rolling(1000).mean()) #usa pandas para calcular media movel\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Retorno medio')\n",
    "plt.title('Resultados do Treino')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
