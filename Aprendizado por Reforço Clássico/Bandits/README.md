
# üé∞ Bandits (Roletas)

  
## Resumo do Problema

Um dos problemas mais cl√°ssicos e simples em aprendizado por refor√ßo √© o problema do $k$-armed Bandit (em portugu√™s, Roleta de $k$-alavancas). Nele uma intelig√™ncia artificial iria para um cassino e encontraria uma roleta com $k$ alavancas, l√° ela teria que aprender - por meio de aprendizado por refor√ßo - a escolher a alavanca que lhe da mais dinheiro.

  

## Especificando o Problema

Este problema pode ser generalizado para qualquer situa√ß√£o em que um agente √© apresentado a um n√∫mero $k$ de escolhas. Ap√≥s cada escolha esse agente recebe uma recompensa dentro de uma **distribui√ß√£o probabil√≠stica estacion√°ria** baseado na a√ß√£o escolhida. A imagem a seguir tenta ilustrar o problema:
  
![gr√°fico representado a distribui√ß√£o normal de um 10 armed bandit](s&b_bandit.png)

No nosso problema de $k$-armed Bandit para cada das $k$ a√ß√µes h√° uma recompensa m√©dia **esperada**; esse valor **esperado** geralmente √© chamado de o **valor** da a√ß√£o. Ou seja, definimos o **valor** de uma a√ß√£o arbitr√°ria $a$, denotado de $q_*(a)$, como uma recompensa em um tempo $t$ ($R_t$) dado que a a√ß√£o em $t$ ($A_t$) foi $a$ como:

$q_*(a) = \mathop{\mathbb{E}}[R_t | A_t = a]$ (Esse $\mathop{\mathbb{E}}$ significa o valor esperado, √© como se fosse a "recompensa m√©dia" - a recompensa com maior probabilidade de acontecer dado $a$.)

Se nosso agente soubesse todos os valores esperados, o problema seria facilmente resolvido: ele simplesmente escolheria a a√ß√£o com o maior valor, o problema √©  justamente que ele n√£o sabe esses valores. E para descobri-los ele necessitar√° realizar o que chamamos de **explora√ß√£o e explota√ß√£o**.

Na **explora√ß√£o** ele tentar√° conhecer melhor os valores de cada a√ß√£o, para que na **explota√ß√£o** ele j√° conhe√ßa uma variedade de valores diferentes e assim poder√° escolher os melhores. Uma analogia com o mundo real seria o menu de um restaurante: imagine que voc√™ pediu um prato l√° e acabou gostando deste prato, voc√™ poderia sempre pedi-lo quando fosse nesse restaurante e acabaria feliz, por√©m, se n√£o se arriscar a pedir nenhum outro prato nuca saber√° se pode haver um prato do qual voc√™ acabe gostando mais! 

## Estimando os $Q$-Valores

  

Como nosso agente n√£o conhece os $q$-valores reais cabe a ele tentar estima-los de alguma maneira. Como estamos buscando um **valor esperado** - ou seja, a recompensa m√©dia - basta calcularmos a m√©dia das recompensas recebidas por nosso agente naquela a√ß√£o:

$Q_{n+1} =\frac{1}{n} \sum_{i=1}^{n} R_i$

Por√©m, como na computa√ß√£o seria custoso executar uma somat√≥ria toda vez que gostar√≠amos de atualizar $Q$ podemos fazer algumas manipula√ß√µes alg√©bricas e cair na seguinte equa√ß√£o:

$Q_{n+1} = Q_n + \frac{1}{n}(R_n - Q_n)$

Que √© a equa√ß√£o que utilizaremos em nosso algoritmos para estimar $Q$! Lembre-se que $n$ nesse caso vai ser o *n√∫mero de vezes que aquela a√ß√£o ocorreu*. Ent√£o teremos um $n$ para cada a√ß√£o.

## Algoritmos Para Este Problema

[Algoritmo Guloso](Algoritmo%20Guloso) - Um algoritmo que s√≥ faz **explota√ß√£o**.

[Algoritmo $\epsilon$-Guloso](Algoritmo%20Epsilon-Guloso) - Um algoritmo que mostra como **explora√ß√£o** √© importante.

[Algoritmo com UCB](Limite%20de%20Confian√ßa%20Superior) - Um algoritmo que utiliza um par√¢metro de **confian√ßa** para **explorar e explotar**.

[Algoritmo de Softmax]() - :construction_worker: :construction:

[Algoritmo de Elimina√ß√£o M√©dia]() - :construction_worker: :construction:
