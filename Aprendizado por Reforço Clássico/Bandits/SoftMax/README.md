# üéØ Algoritmo com Softmax
Veja a implementa√ß√£o do algoritmo no [notebook!](softmax.ipynb)

## Ideia do Algoritmo

Nos outros algoritmos de Bandits tivemos m√©todos que estipulavam o valor *Q* das a√ß√µes do agente, o qual utilizava esses valores para tomar suas a√ß√µes. Por mais que este seja um m√©todo bem eficaz, ele n√£o √© √∫nico. Este algoritmo tem como finalidade usar um par√¢metro de *prefer√™ncia H*<sub>t</sub>(&alpha;) (prefer√™ncia estimada da a√ß√£o &alpha; no tempo *t*). Quanto maior essa prefer√™ncia maior ser√° a probabilidade do agente tomar essa a√ß√£o.

Para transformarmos esses valores de prefer√™ncia em valores probabil√≠sticos, utilizamos a fun√ß√£o *Softmax*:

<img src="https://latex.codecogs.com/svg.latex?Pr[A_t&space;=&space;a]&space;\doteq&space;\frac{e^{H_t(a)}}{\sum^{k}_{b=1}e^{H_t(b)}}&space;\doteq&space;\pi_t(a)" title="Pr[A_t = a] \doteq \frac{e^{H_t(a)}}{\sum^{k}_{b=1}e^{H_t(b)}} \doteq \pi_t(a)" />

Aqui tamb√©m vemos a utiliza√ß√£o de uma nota√ß√£o bem utilizada em Aprendizado Por Refor√ßo a *&pi;<sub>t</sub>*(&alpha;), que significa a probabilidade da a√ß√£o &alpha; no tempo *t*.

## Entendendo a Fun√ß√£o

Se voc√™ j√° estudou outras √°reas de Aprendizado de M√°quina j√° deve ter se deparado com a fun√ß√£o *Softmax*. O que ela basicamente faz √© receber um vetor (lista) de valores num√©ricos e os transforma em valores probabil√≠sticos. Em outras palavras, quanto maior for o valor da *prefer√™ncia* daquela a√ß√£o, depois que essa lista de valores passar pela fun√ß√£o *Softmax*, maior ser√° a probabilidade do agente escolher aquela a√ß√£o.

Isso se deve pois os valores de prefer√™ncia *H<sub>t</sub>*(&alpha;) est√£o sendo elevados pela fun√ß√£o exponencial (ou seja,  valores grandes de *H<sub>t</sub>*(&alpha;) ficam ainda mais prov√°veis) e sendo divididos pela soma desses exponenciais, normalizando os valores e transformando a soma deles em 1, assim, transformando-os em probabilidades.

## Atualizando as prefer√™ncias *H*

Como n√£o estamos usando mais a estima√ß√£o de *Q* valores, n√£o podemos usar a mesma fun√ß√£o para atualizar as prefer√™ncias *H*. Para essa fun√ß√£o utilizamos a ideia de **M√©todo do Gradiente** com um par√¢metro definido pelo usu√°rio _**&alpha;**_, que serve para controlar o quanto o algoritmo ajustara os valores das prefer√™ncias *H*.

<img src="https://latex.codecogs.com/svg.latex?H_{t&plus;1}(A_t)&space;=&space;H_t(A_t)&space;&plus;&space;\alpha&space;(R_t&space;-&space;\bar{R}_t)(1-\pi_t(A_t))" title="H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t - \bar{R}_t)(1-\pi_t(A_t))" />


 e para todo &alpha; ‚â† *A<sub>t</sub>*

<img src="https://latex.codecogs.com/svg.latex?H_{t&plus;1}(a)&space;=&space;H_t(a)&space;&plus;&space;\alpha&space;(R_t&space;-&space;\bar{R}_t)\pi_t(a)" title="H_{t+1}(a) = H_t(a) + \alpha (R_t - \bar{R}_t)\pi_t(a)" />

<img src="https://latex.codecogs.com/svg.latex?\bar{R_t}" title="\bar{R_t}" /> √© a m√©dia das recompensas at√© o tempo *t*.

## Pseudo C√≥digo do Algoritmo

![Pseudo Algoritmo](imgs/algoritmo.svg)
