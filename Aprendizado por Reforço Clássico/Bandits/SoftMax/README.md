# üéØ Algoritmo com Softmax
Veja a implementa√ß√£o do algoritmo no [notebook!](softmax.ipynb)

## Ideia do Algoritmo

Nos outros algoritmos de Bandits tivemos m√©todos que estipulavam o valor *Q* das a√ß√µes do agente, o qual utilizava esses valores para tomar suas a√ß√µes. Por mais que este seja um m√©todo bem eficaz, ele n√£o √© √∫nico. Este algoritmo tem como finalidade usar um par√¢metro de *prefer√™ncia H*<sub>t</sub>(&alpha;) (prefer√™ncia estimada da a√ß√£o &alpha; no tempo *t*). Quanto maior essa prefer√™ncia maior ser√° a probabilidade do agente tomar essa a√ß√£o.

Para transformarmos esses valores de prefer√™ncia em valores de probabil√≠sticos, utilizamos a fun√ß√£o *Softmax*:

<a href="https://www.codecogs.com/eqnedit.php?latex=Pr[A_t&space;=&space;a]&space;\doteq&space;\frac{e^{H_t(a)}}{\sum^{k}_{b=1}e^{H_t(b)}}&space;\doteq&space;\pi_t(a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Pr[A_t&space;=&space;a]&space;\doteq&space;\frac{e^{H_t(a)}}{\sum^{k}_{b=1}e^{H_t(b)}}&space;\doteq&space;\pi_t(a)" title="Pr[A_t = a] \doteq \frac{e^{H_t(a)}}{\sum^{k}_{b=1}e^{H_t(b)}} \doteq \pi_t(a)" /></a>

Aqui tamb√©m vemos tamb√©m a utiliza√ß√£o de uma nota√ß√£o bem utilizada em Aprendizado Por Refor√ßo a *&pi;<sub>t</sub>*(&alpha;), que significa a probabilidade da a√ß√£o &alpha; no tempo *t*.

## Entendendo a Fun√ß√£o

Se voc√™ j√° estudou outras √°reas de Aprendizado de M√°quina j√° deve ter se deparado com a fun√ß√£o *Softmax*. O que ela basicamente faz √© receber um vetor (lista) de valores num√©ricos e os transforma em valores probabil√≠sticos. Em outras palavras, quanto maior for o valor da *prefer√™ncia* daquela a√ß√£o, depois que essa lista de valores passar pela fun√ß√£o *Softmax*, maior ser√° a probabilidade do agente escolher aquela a√ß√£o.

Isso se deve pois os valores de prefer√™ncia *H<sub>t</sub>*(&alpha;) est√£o sendo elevados pela fun√ß√£o exponencial (ou seja,  valores grandes de *H<sub>t</sub>*(&alpha;) ficam ainda mais prov√°veis) e sendo divididos pela soma desses exponenciais, normalizando os valores e transformando a soma deles em 1, assim, transformando-os em probabilidades.

## Atualizando as prefer√™ncias *H*

Como n√£o estamos usando mais a estima√ß√£o de *Q* valores, n√£o podemos usar a mesma fun√ß√£o para atualizar as *H* prefer√™ncias. Para essa fun√ß√£o utilizamos a ideia de **M√©todo do Gradiente** com um par√¢metro definido pelo usu√°rio _**&alpha;**_, que serve para controlar o quanto o algoritmo ajustara os valores das prefer√™ncias *H*.

<a href="https://www.codecogs.com/eqnedit.php?latex=H_{t&plus;1}(A_t)&space;=&space;H_t(A_t)&space;&plus;&space;\alpha&space;(R_t&space;-&space;\bar{R}_t)(1-\pi_t(A_t))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?H_{t&plus;1}(A_t)&space;=&space;H_t(A_t)&space;&plus;&space;\alpha&space;(R_t&space;-&space;\bar{R}_t)(1-\pi_t(A_t))" title="H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t - \bar{R}_t)(1-\pi_t(A_t))" /></a> 

 e para todo <a href="https://www.codecogs.com/eqnedit.php?latex=a&space;\neq&space;A_t" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a&space;\neq&space;A_t" title="a \neq A_t" /></a>:

<a href="https://www.codecogs.com/eqnedit.php?latex=H_{t&plus;1}(a)&space;=&space;H_t(a)&space;&plus;&space;\alpha&space;(R_t&space;-&space;\bar{R}_t)\pi_t(a)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?H_{t&plus;1}(a)&space;=&space;H_t(a)&space;&plus;&space;\alpha&space;(R_t&space;-&space;\bar{R}_t)\pi_t(a)" title="H_{t+1}(a) = H_t(a) + \alpha (R_t - \bar{R}_t)\pi_t(a)" /></a> 

<a href="https://www.codecogs.com/eqnedit.php?latex=\bar{R_t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\bar{R_t}" title="\bar{R_t}" /></a> √© a m√©dia das recompensas at√© o tempo *t*.

## Pseudo C√≥digo do Algoritmo

```
Inicialize de a=1 at√© k:
  H(a) <- 0
Percorra com t para sempre:
  Fa√ßa de a = 1 at√© k:
    pi(a) <- softmax(H(a))
  A <- fa√ßa uma escolha aleat√≥ria com as probabilidades de pi
  R <- bandit(A) #puxa a alavanca A
  R_m√©dio <- (R_m√©dio + R)/t
  Fa√ßa de a = 1 at√© k:
    Se a = A:
      H(a) <- H(a) + alpha(R - R_m√©dio) * (1 - pi(a))
    Se n√£o:
      H(a) <- H(a) - alpha(R - R_m√©dio) * pi(a)
```
