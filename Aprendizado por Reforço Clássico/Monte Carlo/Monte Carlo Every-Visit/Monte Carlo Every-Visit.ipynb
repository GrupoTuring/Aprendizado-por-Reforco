{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèî Monte Carlo Every Visit\n",
    "\n",
    "**Monte Carlo Every-Visit** √© um algoritmo de controle por Monte Carlo, ou seja, ele estima nossa fun√ß√£o de valor *q(s, a)* a partir dos retornos m√©dios de cada par estado-a√ß√£o, e toma a√ß√µes no ambiente com base nessas estimativas. \n",
    "\n",
    "Entretanto, esse algoritmo difere de outros m√©todos de Monte Carlo por utilizar todos os retornos de um par estado-a√ß√£o durante um epis√≥dio. Isso significa que, quando o nosso agente visita um estado repetido e toma uma mesma a√ß√£o, o c√°lculo da fun√ß√£o de valor levar√° em conta o retorno de todas as vezes que essa a√ß√£o foi tomada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pol√≠tica Œµ-greedy\n",
    "\n",
    "Para garantir que os m√©todos de Monte Carlo convirjam para a fun√ß√£o de valor real, √© necess√°rio seguir uma pol√≠tica que explore todas as a√ß√µes de todos os estados. Entretanto, tamb√©m √© interessante que o agente tente conseguir cada vez mais recompensas, para maximizar sua perfomance.\n",
    "\n",
    "Assim foi desenvolvida a pol√≠tica *Œµ-greedy*, que escolhe a pr√≥xima a√ß√£o com base em um par√¢metro *Œµ*, normalmente pequeno. A cada decis√£o, a pol√≠tica tem uma probabilidade *Œµ* de escolher uma a√ß√£o aleat√≥ria, aumentando a explora√ß√£o, e uma probabilidade *1 - Œµ* de escolher a a√ß√£o associada ao maior *Q*. Dessa forma, ela estabelece um equil√≠brio entre a explora√ß√£o de a√ß√µes e a explota√ß√£o de recompensas. Essa pol√≠tica √© dada por:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\pi(a|S_t)&space;\\leftarrow&space;\\begin{cases}&space;1&space;-&space;\\varepsilon&space;&plus;&space;\\varepsilon/\\left|\\mathcal{A}(S_t)\\right|,&space;&&space;\\mbox{se&space;}&space;a&space;=&space;\\underset{a}{\\mathrm{argmax}}&space;\\,&space;Q(S_t,a)&space;\\\\&space;\\varepsilon/\\left|\\mathcal{A}(S_t)\\right|,&space;&&space;\\mbox{se&space;}&space;a&space;\\neq&space;\\underset{a}{\\mathrm{argmax}}&space;\\,&space;Q(S_t,a)&space;\\end{cases}\" title=\"\\pi(a|S_t) \\leftarrow \\begin{cases} 1 - \\varepsilon + \\varepsilon/\\left|\\mathcal{A}(S_t)\\right|, & \\mbox{se } a = \\underset{a}{\\mathrm{argmax}} \\, Q(S_t,a) \\\\ \\varepsilon/\\left|\\mathcal{A}(S_t)\\right|, & \\mbox{se } a \\neq \\underset{a}{\\mathrm{argmax}} \\, Q(S_t,a) \\end{cases}\" />\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\left|\\mathcal{A}(S_t)\\right|&space;\\rightarrow&space;\\textrm{quantidade&space;de&space;acoes&space;possiveis}\" title=\"\\left|\\mathcal{A}(S_t)\\right| \\rightarrow \\textrm{quantidade de acoes possiveis}\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo\n",
    "\n",
    "Primeiramente, devemos inicializar a nossa tabela *Q(s, a)* com valores arbitr√°rios para cada par estado-a√ß√£o. Nesse caso, vamos optar por superestimar os valores Q de modo a incentivar a explora√ß√£o do agente.\n",
    "\n",
    "Tamb√©m inicializamos uma tabela *N(s, a)* que guarda a quantidade de retornos obtidos de cada par estado-a√ß√£o, para fazer o c√°lculo da m√©dia m√≥vel dos retornos.\n",
    "\n",
    "Para cada epis√≥dio, vamos escolher a√ß√µes seguindo nossa pol√≠tica Œµ-greedy e guardar os estados, a√ß√µes e recompensas para cada instante *t*. Ao final, calculamos o retorno *G* de cada instante come√ßando pelo t√©rmino atualizando os valores *Q* correspondentes.\n",
    "\n",
    "Para estimar a m√©dia dos retornos, podemos utilizar a *m√©dia m√≥vel*, de forma a realizar os c√°lculos na hora sem precisar guardar uma lista com todos os retornos. Ao inv√©s disso, precisamos guardar apenas a m√©dia anterior e a quantidade total de elementos *n*.\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?{\\overline&space;{x}}_{novo}&space;=&space;\\frac{(n&space;-&space;1){\\overline&space;{x}}_{anterior}&space;&plus;&space;x_n}{n}\" title=\"{\\overline {x}}_{novo} = \\frac{(n - 1){\\overline {x}}_{anterior} + x_n}{n}\" />\n",
    "\n",
    "Por fim, podemos ver abaixo um exemplo em pseudo-c√≥digo do funcionamento do algoritmo de Monte Carlo Every-Visit:\n",
    "\n",
    "![On-policy every-visit MC control](imgs/MC.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C√≥digo\n",
    "\n",
    "Antes de come√ßar a programar nosso algoritmo, devemos importar a biblioteca ***gym*** e criar o ambiente *FrozenLake-v0*, que usaremos para testar o Monte Carlo Every-Visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando a biblioteca gym\n",
    "import gym\n",
    "\n",
    "# Criando o ambiente\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, vamos definir uma fun√ß√£o argmax, que retornar√° o √≠ndice do elemento de maior valor dentro de um vetor. Usaremos essa fun√ß√£o para escolher a a√ß√£o com maior valor *Q* dentro da nossa tabela de valores.\n",
    "\n",
    "√â importante definir nossa pr√≥pria fun√ß√£o de argmax pois, em casos de empate, ela deve decidir aleatoriamente entre um dos √≠ndices dos elementos de maior valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(array):\n",
    "    \"\"\"Retorna o argumento m√°ximo de uma lista.\"\"\"\n",
    "    return np.random.choice(np.flatnonzero(array == array.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente\n",
    "\n",
    "Agora sim podemos criar o nosso agente, uma classe com os m√©todos *step*, *store_experience* e *update*:\n",
    "  - `step(self, state, epsilon=0)` - Escolhe a pr√≥xima a√ß√£o a se tomar com base no estado atual.\n",
    "  \n",
    "  - `store_experience(self, state, action, reward)` - Guarda a tupla (*estado*, *a√ß√£o, *recompensa*) atual, para depois realizar o c√°lculo dos retornos.\n",
    "  \n",
    "  - `update(self)` - Atualiza a fun√ß√£o valor com base nos retornos do epis√≥dio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class MonteCarloAgent(object):\n",
    "    \"\"\"Agente de Monte Carlo.\"\"\"\n",
    "    def __init__(self, gamma, action_space):\n",
    "        \"\"\"Inicializa o agente.\n",
    "\n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        gamma: float\n",
    "            Fator de desconto.\n",
    "        action_space: Space\n",
    "            Espa√ßo de a√ß√£o do ambiente.\n",
    "        \"\"\"\n",
    "        # Dicion√°rio com as nossas estimativas de Q para cada par estado-a√ß√£o\n",
    "        self.q_values = defaultdict(lambda: np.ones(action_space.n))\n",
    "        \n",
    "        # Dicion√°rio com a quantidade de vezes que visitamos cada par estado-a√ß√£o\n",
    "        self.times_visited = defaultdict(lambda: np.zeros(action_space.n))\n",
    "        \n",
    "        # Lista para guardar as transi√ß√µes de um epis√≥dio\n",
    "        self.experiences = []\n",
    "        self.gamma = gamma\n",
    "        self.action_space = action_space\n",
    "        \n",
    "    def step(self, state, epsilon=0):\n",
    "        \"\"\"Escolhe uma a√ß√£o.\n",
    "\n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        state: np.array\n",
    "            Estado no instante atual.\n",
    "        epsilon: float\n",
    "            Probabilidade de escolher uma a√ß√£o aleatoriamente. (default 0)\n",
    "            \n",
    "        Retorna\n",
    "        -------\n",
    "        action: int\n",
    "            A√ß√£o escolhida.\n",
    "        \"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            action = argmax(self.q_values[state])\n",
    "        return action\n",
    "    \n",
    "    def store_experience(self, state, action, reward):\n",
    "        \"\"\"Guarda uma transi√ß√£o do epis√≥dio.\n",
    "\n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        state: np.array\n",
    "            Estado da transi√ß√£o.\n",
    "        action: int\n",
    "            A√ß√£o tomada.\n",
    "        reward: float\n",
    "            Recompensa recebida.\n",
    "        \"\"\"\n",
    "        self.experiences.append((state, action, reward))\n",
    "        \n",
    "    def update(self):\n",
    "        \"\"\"Atualiza os valores q do agente.\"\"\"\n",
    "        g = 0\n",
    "        for state, action, reward in reversed(self.experiences):\n",
    "            g = self.gamma*g + reward\n",
    "            self.times_visited[state][action] += 1\n",
    "            self.q_values[state][action] = ((self.times_visited[state][action]-1) * self.q_values[state][action] + g)/self.times_visited[state][action]\n",
    "            \n",
    "        self.experiences = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que definimos o nosso agente, podemos instanci√°-lo e trein√°-lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando nosso agente com um gamma igual a 0.9\n",
    "agent = MonteCarloAgent(0.9, env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento\n",
    "\n",
    "Para realizar o treinamento, vamos definir a seguinte fun√ß√£o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def train(agent, env, episodes):\n",
    "    \"\"\"Treina um agente de Monte Carlo.\n",
    "\n",
    "    Par√¢metros\n",
    "    ----------\n",
    "    agent: MonteCarloAgent\n",
    "        Agente de Monte Carlo.\n",
    "    env: gym.Env\n",
    "        Ambiente do Gym.\n",
    "    episodes: int\n",
    "        Quantidade de epis√≥dios para serem treinados.\n",
    "    \"\"\"\n",
    "    # Guarda os retornos dos √∫ltimos epis√≥dios\n",
    "    returns = deque(maxlen=1000)\n",
    "\n",
    "    # Guarda uma m√©dia dos retornos\n",
    "    mean_returns = []\n",
    "\n",
    "    for episode in range(1, episodes+1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        ep_return = 0\n",
    "\n",
    "        while not done:\n",
    "            # Escolhe uma a√ß√£o com epsilon = 0.1\n",
    "            action = agent.step(state, epsilon=0.1)\n",
    "\n",
    "            # Toma a a√ß√£o escolhida\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Guarda a transi√ß√£o do estado\n",
    "            agent.store_experience(state, action, reward)\n",
    "\n",
    "            state = next_state\n",
    "            ep_return += reward\n",
    "\n",
    "        # Atualiza o agente ao final de cada epis√≥dio\n",
    "        agent.update()\n",
    "\n",
    "        returns.append(ep_return)\n",
    "\n",
    "        mean_returns.append(np.mean(returns))\n",
    "\n",
    "        # Mostra a taxa de sucesso do agente a cada 500 epis√≥dios\n",
    "        if episode % 500 == 0:\n",
    "            print(f\"Episode: {episode:5d} Success Rate: {mean_returns[-1]:5.4f}\\r\", end=\"\")\n",
    "            \n",
    "    return mean_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, vamos trein√°-lo em 50000 epis√≥dios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50000 Success Rate: 0.4130\r"
     ]
    }
   ],
   "source": [
    "mean_returns = train(agent, env, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que nosso agente terminou de treinar, podemos test√°-lo em 1000 epis√≥dios com epsilon = 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.7290\r"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Guarda o retornos dos 1000 epis√≥dios\n",
    "returns = deque(maxlen=1000)\n",
    "\n",
    "for episode in range(1, 1001):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    ret = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Escolhe uma a√ß√£o com epsilon = 0\n",
    "        action = agent.step(state, epsilon=0)\n",
    "        \n",
    "        # Toma a a√ß√£o escolhida\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        state = next_state\n",
    "        ret += reward\n",
    "        \n",
    "    returns.append(ret)\n",
    "\n",
    "# Mostra a taxa de sucesso ao longo dos 1000 epis√≥dios\n",
    "print(f\"Success Rate: {np.mean(returns):5.4f}\\r\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso agente conseguiu uma taxa de sucesso maior que 70%!\n",
    "\n",
    "Como h√° muita aleatoriedade no ambiente, essa taxa de sucesso √© uma das melhores poss√≠veis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimento com 20 treinamentos\n",
    "\n",
    "Uma das grandes desvantagens dos m√©todos de Monte Carlo √© sua alta vari√¢ncia. Logo, faz sentido analisar o desempenho do nosso algoritmo ao longo de v√°rios treinamentos diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40000 Success Rate: 0.2480\r"
     ]
    }
   ],
   "source": [
    "train_returns = []\n",
    "\n",
    "for training in range(20):\n",
    "    # Cria um novo agente\n",
    "    agent = MonteCarloAgent(0.9, env.action_space)\n",
    "    \n",
    "    # Treina ele por 40000 epis√≥dios\n",
    "    train_returns.append(train(agent, env, 40000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curva de Aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.mean(train_returns, axis=0), 'r')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
