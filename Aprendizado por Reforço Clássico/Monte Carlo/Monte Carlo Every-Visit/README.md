# üèî Monte Carlo Every Visit

**Monte Carlo Every-Visit** √© um algoritmo de controle por Monte Carlo, ou seja, ele estima nossa fun√ß√£o de valor *q(s, a)* a partir dos retornos m√©dios de cada par estado-a√ß√£o, e toma a√ß√µes no ambiente com base nessas estimativas. 

Entretanto, esse algoritmo difere de outros m√©todos de Monte Carlo por utilizar todos os retornos de um par estado-a√ß√£o durante um epis√≥dio. Isso significa que, quando o nosso agente visita um estado repetido e toma uma mesma a√ß√£o, o c√°lculo da fun√ß√£o de valor levar√° em conta o retorno de todas as vezes que essa a√ß√£o foi tomada.

## Pol√≠tica Œµ-greedy

Para garantir que os m√©todos de Monte Carlo convirjam para a fun√ß√£o de valor real, √© necess√°rio seguir uma pol√≠tica que explore todas as a√ß√µes de todos os estados. Entretanto, tamb√©m √© interessante que o agente tente conseguir cada vez mais recompensas, para maximizar sua perfomance.

Assim foi desenvolvida a pol√≠tica *Œµ-greedy*, que escolhe a pr√≥xima a√ß√£o com base em um par√¢metro *Œµ*, normalmente pequeno. A cada decis√£o, a pol√≠tica tem uma probabilidade *Œµ* de escolher uma a√ß√£o aleat√≥ria, aumentando a explora√ß√£o, e uma probabilidade *1 - Œµ* de escolher a a√ß√£o associada ao maior *Q*. Dessa forma, ela estabelece um equil√≠brio entre a explora√ß√£o de a√ß√µes e a explota√ß√£o de recompensas. Essa pol√≠tica √© dada por:

<img src="https://latex.codecogs.com/svg.latex?\pi(a|S_t)&space;\leftarrow&space;\begin{cases}&space;1&space;-&space;\varepsilon&space;&plus;&space;\varepsilon/\left|\mathcal{A}(S_t)\right|,&space;&&space;\mbox{se&space;}&space;a&space;=&space;\underset{a}{\mathrm{argmax}}&space;\,&space;Q(S_t,a)&space;\\&space;\varepsilon/\left|\mathcal{A}(S_t)\right|,&space;&&space;\mbox{se&space;}&space;a&space;\neq&space;\underset{a}{\mathrm{argmax}}&space;\,&space;Q(S_t,a)&space;\end{cases}" title="\pi(a|S_t) \leftarrow \begin{cases} 1 - \varepsilon + \varepsilon/\left|\mathcal{A}(S_t)\right|, & \mbox{se } a = \underset{a}{\mathrm{argmax}} \, Q(S_t,a) \\ \varepsilon/\left|\mathcal{A}(S_t)\right|, & \mbox{se } a \neq \underset{a}{\mathrm{argmax}} \, Q(S_t,a) \end{cases}" />

<img src="https://latex.codecogs.com/svg.latex?\left|\mathcal{A}(S_t)\right|&space;\rightarrow&space;\textrm{quantidade&space;de&space;acoes&space;possiveis}" title="\left|\mathcal{A}(S_t)\right| \rightarrow \textrm{quantidade de acoes possiveis}" />

## Algoritmo

Primeiramente, devemos inicializar a nossa tabela *Q(s, a)* com valores arbitr√°rios para cada par estado-a√ß√£o. Nesse caso, vamos optar por superestimar os valores Q de modo a incentivar a explora√ß√£o do agente.

Tamb√©m inicializamos uma tabela *N(s, a)* que guarda a quantidade de retornos obtidos de cada par estado-a√ß√£o, para fazer o c√°lculo da m√©dia m√≥vel dos retornos.

Para cada epis√≥dio, vamos escolher a√ß√µes seguindo nossa pol√≠tica Œµ-greedy e guardar os estados, a√ß√µes e recompensas para cada instante *t*. Ao final, calculamos o retorno *G* de cada instante come√ßando pelo t√©rmino atualizando os valores *Q* correspondentes.

Para estimar a m√©dia dos retornos, podemos utilizar a *m√©dia m√≥vel*, de forma a realizar os c√°lculos na hora sem precisar guardar uma lista com todos os retornos. Ao inv√©s disso, precisamos guardar apenas a m√©dia anterior e a quantidade total de elementos *n*.

<img src="https://latex.codecogs.com/svg.latex?{\overline&space;{x}}_{novo}&space;=&space;\frac{(n&space;-&space;1){\overline&space;{x}}_{anterior}&space;&plus;&space;x_n}{n}" title="{\overline {x}}_{novo} = \frac{(n - 1){\overline {x}}_{anterior} + x_n}{n}" />

Por fim, podemos ver abaixo um exemplo em pseudo-c√≥digo do funcionamento do algoritmo de Monte Carlo Every-Visit:

![On-policy every-visit MC control](imgs/MC.png)