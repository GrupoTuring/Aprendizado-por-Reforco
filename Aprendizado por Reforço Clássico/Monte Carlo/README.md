# üèî M√©todos de Monte Carlo

Os m√©todos de **Monte Carlo** s√£o algoritmos de Aprendizado por Refor√ßo que estimam as fun√ß√µes de valor com base em suas *experi√™ncias*, obtidas atrav√©s da intera√ß√£o com o ambiente. Nesses m√©todos, os valores s√£o obtidos a partir do c√°lculo da m√©dia dos retornos de cada epis√≥dio.

## Motiva√ß√£o

Apesar de muito eficazes, os algoritmos de **Programa√ß√£o Din√¢mica** (**DP**) desenvolvidos anteriormente requerem um conhecimento pr√©vio completo do funcionamento do nosso ambiente, que raramente √© obtido em problemas reais. Dessa forma, foi necess√°rio elaborar algoritmos de Aprendizado por Refor√ßo que conseguissem aprender pol√≠ticas √≥timas apenas por meio da intera√ß√£o com o ambiente.

Assim surgiram os **m√©todos de Monte Carlo**, que se baseiam no c√°lculo da m√©dia de v√°rios retornos para obter estimativas das fun√ß√µes de valor. Nesse sentido, esses algoritmos se aproximam bastante do c√°lculo dos valores no problema dos Bandits.

## Teoria

Primeiramente, para entender melhor esses m√©todos, precisamos relembrar de como definimos nossa fun√ß√£o de valor ***q***:

<img src="https://latex.codecogs.com/svg.latex?q_\pi(s,a)&space;=&space;\mathop{\mathbb{E}_\pi}[{G_t}|{S_t=s,&space;A_t=a}]" title="q_\pi(s,a) = \mathop{\mathbb{E}_\pi}[{G_t}|{S_t=s, A_t=a}]" />

O **Valor Estado-A√ß√£o** para uma determinada pol√≠tica √© definido como a esperan√ßa do retorno ***G*** quando se toma uma determinada a√ß√£o *a* em um determinado estado *s* seguindo essa pol√≠tica. Dessa forma, √© natural pensar que poder√≠amos estimar esse ***q*** ao tomar uma m√©dia de todos os retornos que observarmos depois de tomar uma a√ß√£o *a* em um estado *s*.

E √© a partir dessa intui√ß√£o que nascem os m√©todos de Monte Carlo, nos quais s√£o calculadas as m√©dias dos retornos obtidos ao tomar cada a√ß√£o para estimar os valores *q(s, a)*. Portanto, a ideia do algoritmo √© a seguinte:

 - Rodar um epis√≥dio
 - Calcular o retorno *G* em cada instante de tempo desse epis√≥dio

   <img src="https://latex.codecogs.com/svg.latex?G_t&space;=&space;R_{t&plus;1}&space;&plus;&space;\gamma&space;R_{t&plus;2}&space;&plus;&space;\gamma^2&space;R_{t&plus;3}&space;&plus;&space;..." title="G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ..." />
 - Atualizar as estimativas *Q* de cada par estado-a√ß√£o com a m√©dia dos retornos dos epis√≥dios amostrados.
 
   <img src="https://latex.codecogs.com/svg.latex?Q(s,a)&space;\leftarrow&space;\operatorname{media}(Retornos(s,&space;a))" title="Q(s,a) \leftarrow \operatorname{media}(Retornos(s, a))" />
 - Repetir o processo para v√°rios epis√≥dios at√© obter uma pol√≠tica √≥tima.

## Algoritmos

Existem v√°rias implementa√ß√µes poss√≠veis dos m√©todos de Monte Carlo, como:

 - [üèî Monte Carlo Every-Visit](Monte%20Carlo%20Every-Visit)