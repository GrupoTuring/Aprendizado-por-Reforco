# üèî M√©todos de Monte Carlo

Os m√©todos de **Monte Carlo** s√£o algoritmos de Aprendizado por Refor√ßo que estimam as fun√ß√µes de valor com base em suas *experi√™ncias*, obtidas atrav√©s da intera√ß√£o com o ambiente. Nesses m√©todos, os valores s√£o obtidos a partir do c√°lculo da m√©dia dos retornos de cada epis√≥dio.

## Motiva√ß√£o

Apesar de muito eficazes, os algoritmos de **Programa√ß√£o Din√¢mica** (**DP**) desenvolvidos anteriormente requerem um conhecimento pr√©vio completo do funcionamento do nosso ambiente, que raramente √© obtido em problemas reais. Dessa forma, foi necess√°rio elaborar algoritmos de Aprendizado por Refor√ßo que conseguissem aprender pol√≠ticas √≥timas apenas por meio da intera√ß√£o com o ambiente.

Assim surgiram os **m√©todos de Monte Carlo**, que se baseiam no c√°lculo da m√©dia de v√°rios retornos para obter estimativas das fun√ß√µes de valor. Nesse sentido, esses algoritmos se aproximam bastante do c√°lculo dos valores no problema dos Bandits.

## Teoria

Primeiramente, para entender melhor esses m√©todos, precisamos relembrar de como definimos nossa fun√ß√£o de valor ***q***:

<img src="https://latex.codecogs.com/svg.latex?q_\pi(s,a)&space;=&space;\mathop{\mathbb{E}_\pi}[{G_t}|{S_t=s,&space;A_t=a}]" title="q_\pi(s,a) = \mathop{\mathbb{E}_\pi}[{G_t}|{S_t=s, A_t=a}]" />

O **Valor Estado-A√ß√£o** para uma determinada pol√≠tica √© definido como a esperan√ßa do retorno ***G*** quando se toma uma determinada a√ß√£o *a* em um determinado estado *s* seguindo essa pol√≠tica. Dessa forma, √© natural pensar que poder√≠amos estimar esse ***q*** ao tomar uma m√©dia de todos os retornos que observarmos depois de tomar uma a√ß√£o *a* em um estado *s*.

E √© a partir dessa intui√ß√£o que nascem os m√©todos de Monte Carlo, nos quais s√£o calculadas as m√©dias dos retornos obtidos ao tomar cada a√ß√£o para estimar os valores *q(s, a)*. Portanto, a ideia do algoritmo √© a seguinte:

 - Rodar um epis√≥dio
 - Calcular o retorno *G* em cada instante de tempo desse epis√≥dio

   <img src="https://latex.codecogs.com/svg.latex?G_t&space;=&space;R_{t&plus;1}&space;&plus;&space;\gamma&space;R_{t&plus;2}&space;&plus;&space;\gamma^2&space;R_{t&plus;3}&space;&plus;&space;..." title="G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ..." />
 - Atualizar as estimativas *Q* de cada par estado-a√ß√£o com a m√©dia dos retornos dos epis√≥dios amostrados.
 
   <img src="https://latex.codecogs.com/svg.latex?Q(s,a)&space;\leftarrow&space;\operatorname{media}(Retornos(s,&space;a))" title="Q(s,a) \leftarrow \operatorname{media}(Retornos(s, a))" />
 - Repetir o processo para v√°rios epis√≥dios at√© obter uma pol√≠tica √≥tima.

## Pol√≠tica Œµ-greedy

Para garantir que os m√©todos de Monte Carlo convirjam para a fun√ß√£o de valor real, √© necess√°rio seguir uma pol√≠tica que explore todas as a√ß√µes de todos os estados. Entretanto, tamb√©m √© interessante que o agente tente conseguir cada vez mais recompensas, para maximizar sua perfomance.

Assim foi desenvolvida a pol√≠tica *Œµ-greedy*, que escolhe a pr√≥xima a√ß√£o com base em um par√¢metro *Œµ*, normalmente pequeno. A cada decis√£o, a pol√≠tica tem uma probabilidade *Œµ* de escolher uma a√ß√£o aleat√≥ria, aumentando a explora√ß√£o, e uma probabilidade *1 - Œµ* de escolher a a√ß√£o associada ao maior *Q*. Dessa forma, ela estabelece um equil√≠brio entre a explora√ß√£o de a√ß√µes e a explota√ß√£o de recompensas. Essa pol√≠tica √© dada por:

<img src="https://latex.codecogs.com/svg.latex?\pi(a|S_t)&space;\leftarrow&space;\begin{cases}&space;1&space;-&space;\varepsilon&space;&plus;&space;\varepsilon/\left|\mathcal{A}(S_t)\right|,&space;&&space;\mbox{se&space;}&space;a&space;=&space;\underset{a}{\mathrm{argmax}}&space;\,&space;Q(S_t,a)&space;\\&space;\varepsilon/\left|\mathcal{A}(S_t)\right|,&space;&&space;\mbox{se&space;}&space;a&space;\neq&space;\underset{a}{\mathrm{argmax}}&space;\,&space;Q(S_t,a)&space;\end{cases}" title="\pi(a|S_t) \leftarrow \begin{cases} 1 - \varepsilon + \varepsilon/\left|\mathcal{A}(S_t)\right|, & \mbox{se } a = \underset{a}{\mathrm{argmax}} \, Q(S_t,a) \\ \varepsilon/\left|\mathcal{A}(S_t)\right|, & \mbox{se } a \neq \underset{a}{\mathrm{argmax}} \, Q(S_t,a) \end{cases}" />

<img src="https://latex.codecogs.com/svg.latex?\left|\mathcal{A}(S_t)\right|&space;\rightarrow&space;\textrm{quantidade&space;de&space;acoes&space;possiveis}" title="\left|\mathcal{A}(S_t)\right| \rightarrow \textrm{quantidade de acoes possiveis}" />

## Algoritmo

Primeiramente, devemos inicializar a nossa tabela *Q(s, a)* com valores arbitr√°rios para cada par estado-a√ß√£o. Nesse caso, vamos optar por superestimar os valores Q de modo a incentivar a explora√ß√£o do agente.

Tamb√©m inicializamos uma tabela *N(s, a)* que guarda a quantidade de retornos obtidos de cada par estado-a√ß√£o, para fazer o c√°lculo da m√©dia m√≥vel dos retornos.

Para cada epis√≥dio, vamos escolher a√ß√µes seguindo nossa pol√≠tica Œµ-greedy e guardar os estados, a√ß√µes e recompensas para cada instante *t*. Ao final, calculamos o retorno *G* de cada instante come√ßando pelo t√©rmino atualizando os valores *Q* correspondentes.

Para estimar a m√©dia dos retornos, podemos utilizar a *m√©dia m√≥vel*, de forma a realizar os c√°lculos na hora sem precisar guardar uma lista com todos os retornos. Ao inv√©s disso, precisamos guardar apenas a m√©dia anterior e a quantidade total de elementos *n*.

<img src="https://latex.codecogs.com/svg.latex?{\overline&space;{x}}_{novo}&space;=&space;\frac{(n&space;-&space;1){\overline&space;{x}}_{anterior}&space;&plus;&space;x_n}{n}" title="{\overline {x}}_{novo} = \frac{(n - 1){\overline {x}}_{anterior} + x_n}{n}" />

Por fim, podemos ver abaixo um exemplo em pseudo-c√≥digo do funcionamento do algoritmo de Monte Carlo Every-Visit:

![On-policy every-visit MC control](/img/MC.png)