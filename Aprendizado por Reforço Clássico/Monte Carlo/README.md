# üèî M√©todos de Monte Carlo

Os m√©todos de **Monte Carlo** s√£o algoritmos de Aprendizado por Refor√ßo que estimam as fun√ß√µes de valor com base em suas *experi√™ncias*, obtidas atrav√©s da intera√ß√£o com o ambiente. Nesses m√©todos, os valores s√£o obtidos a partir do c√°lculo da m√©dia dos retornos de cada epis√≥dio.

## Motiva√ß√£o

Apesar de muito eficazes, os algoritmos de **Programa√ß√£o Din√¢mica** (**DP**) desenvolvidos anteriormente requerem um conhecimento pr√©vio completo do funcionamento do nosso ambiente, que raramente √© obtido em problemas reais. Dessa forma, foi necess√°rio elaborar algoritmos de Aprendizado por Refor√ßo que conseguissem aprender pol√≠ticas √≥timas apenas por meio da intera√ß√£o com o ambiente.

Assim surgiram os **m√©todos de Monte Carlo**, que se baseiam no c√°lculo da m√©dia de v√°rios retornos para obter estimativas das fun√ß√µes de valor. Nesse sentido, esses algoritmos se aproximam bastante do c√°lculo dos valores no problema dos Bandits.

## Teoria

Primeiramente, para entender melhor esses m√©todos, precisamos relembrar de como definimos nossa fun√ß√£o de valor ***q***:

<a href="https://www.codecogs.com/eqnedit.php?latex=q_\pi(s,a)&space;=&space;\mathop{\mathbb{E}_\pi}[{G_t}|{S_t=s,&space;A_t=a}]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?q_\pi(s,a)&space;=&space;\mathop{\mathbb{E}_\pi}[{G_t}|{S_t=s,&space;A_t=a}]" title="q_\pi(s,a) = \mathop{\mathbb{E}_\pi}[{G_t}|{S_t=s, A_t=a}]" /></a>

O **Valor Estado-A√ß√£o** para uma determinada pol√≠tica √© definido como a esperan√ßa do retorno ***G*** quando se toma uma determinada a√ß√£o *a* em um determinado estado *s* seguindo essa pol√≠tica. Dessa forma, √© natural pensar que poder√≠amos estimar esse ***q*** ao tomar uma m√©dia de todos os retornos que observarmos depois de tomar uma a√ß√£o *a* em um estado *s*.

## Algoritmo