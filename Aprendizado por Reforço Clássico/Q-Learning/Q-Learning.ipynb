{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598806918371",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "O algoritmo de Q-Learning é um dos algoritmos do grupo TD (Temporal difference, diferença temporal), que estimam a função de valor por meio de uma estimativa a partir de outra estimativa, processo chamado _bootstrapping_\n",
    "\n",
    "## Motivação\n",
    "\n",
    "Assim como nos métodos de Monte Carlo, os métodos TD tem vantagens sobre os métodos de Programação Dinâmica (DP) por não dependerem de um modelo do ambiente para seu funcionamento, sendo capazes de aprender diretamente com a experiência\n",
    "\n",
    "Porém, os métodos TD tem uma vantagem sobre os métodos de Monte Carlo (MC), pois são capazes de aprendizado _online_, ou seja, aprendem com a passagem do episódio, enquanto os métodos de MC precisam chegar no final do episódio para iniciar o aprendizado. Em episódios longos ou em ações contínuas, sem episódios, os métodos MC se tornam inviáveis.\n",
    "\n",
    "## Teoria\n",
    "\n",
    "O objetivo do algoritmo é encontrar a função de *Valor Estado-Ação* da política ótima, ou seja, a função que melhor representa os valores *q* para cada par estado-ação do ambiente. A equação da função se encontra abaixo, onde o valor q é dado pelo valor esperado do retorno G, dado um par estado-ação:\n",
    "\n",
    "![funcao q](https://camo.githubusercontent.com/003498f344a099b4d34d45aa2bc0841e562fa01e/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f715f5c706928732c61292673706163653b3d2673706163653b5c6d6174686f707b5c6d61746862627b457d5f5c70697d5b7b475f747d7c7b535f743d732c2673706163653b415f743d617d5d)\n",
    "\n",
    "O algoritmo de Q-Learning é extremamente versátil dentro dos métodos TD, por ser um algoritmo _off-policy_, ou seja, seu aprendizado não depende da política que está sendo seguida, diferente de outros métodos TD, como SARSA. Com isso, o treinamento é acelerado, pois só depende que os pares estado-ação sejam visitados.\n",
    "\n",
    "## Política\n",
    "\n",
    "A política adotada é a chamada ε-greedy (ε-guloso). Um número aleatório no intervalo entre 0 e 1 é comparado com ε. Se o número aleatório for menor do que o valor de ε, a ação tomada vai ser aleatória, de forma a explorar o ambiente. Caso contrário, a ação vai ser a que possui maior valor Q, de acordo com as estimativas atuais que o algoritmo possui. Dessa forma, o agente tomará uma ação aleatória com probabilidade ε.\n",
    "\n",
    "## Algoritmo\n",
    "\n",
    "O algoritmo consiste na atualização das estimativas dos valores Q(S, A) de acordo com a seguinte expressão:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?Q(S,&space;A)&space;\\leftarrow&space;Q(S,&space;A)&space;&plus;&space;\\alpha&space;[R&space;&plus;&space;\\gamma&space;\\max_{a}Q(S',&space;a)&space;-&space;Q(S,&space;A)]\" title=\"Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma \\max_{a}Q(S', a) - Q(S, A)]\" />\n",
    "\n",
    "De forma resumida, o valor Q(S,A) é atualizado fazendo uma \"correção\" com \"taxa de aprendizado\" &alpha;, considerando a recompensa R recebida com a escolha da ação e a ação futura que maximiza o valor Q do próximo estado (S'), descontada de um fator &gamma;. É interessante ressaltar que todas as atualizações são feitas com as estimativas, tanto do presente quanto do futuro, que vão se aprimorando até se aproximar da função q da política ótima.\n",
    "\n",
    "O funcionamento do algoritmo ao longo dos episódios pode ser visto no pseudocódigo a seguir:\n",
    "\n",
    "![Algoritmo de Q-Learning](imgs/q-learning.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código\n",
    "\n",
    "\n",
    "Inicialmente, iremos definir a classe do nosso agente, com as principais funções necessárias para seu funcionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Classe do agente que seguirá o algoritmo de Q-Learning\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon, decay, epsilon_min, alpha, gamma, n_actions):\n",
    "        \"\"\"\n",
    "        Inicializa os parâmetros do algoritmo, assim como a  'tabela' Q\n",
    "\n",
    "         Parâmetros\n",
    "        ----------\n",
    "        epsilon: float\n",
    "            Probabilidade de tomar ação aleatória.\n",
    "        decay: float\n",
    "            Taxa de decaimento do epsilon por episódio.\n",
    "        epsilon_min: float\n",
    "            Valor mínimo de epsilon.\n",
    "        alpha: float\n",
    "            Taxa de aprendizado.\n",
    "        gamma: float\n",
    "            Fator de desconto.\n",
    "        n_actions: int\n",
    "            Número de ações possíveis no ambiente\n",
    "        \"\"\"\n",
    "\n",
    "        self.Q = {} #tabela Q\n",
    "        self.n_actions = n_actions #número possível de ações, fornecido pelo env\n",
    "\n",
    "        # parâmetros para o epsilon-decay\n",
    "        self.epsilon = epsilon #epsilon inicial\n",
    "        self.decay = decay #taxa de decaimento\n",
    "        self.epsilon_min = epsilon_min #epsilon minimo\n",
    "\n",
    "        self.alpha = alpha #taxa de aprendizado\n",
    "        self.gamma = gamma #peso das recompensas futuras\n",
    "\n",
    "    def choose_action(self, env, state):\n",
    "        \"\"\"\n",
    "        Escolhe a ação do agente, de acordo com a política epsilon-greedy\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        env: Environment\n",
    "            Ambiente sendo utilizado.\n",
    "        state: np.array\n",
    "            Estado atual do agente\n",
    "\n",
    "        Retorna\n",
    "        ----------\n",
    "        action: int\n",
    "            Ação a ser tomada.\n",
    "        \"\"\"\n",
    "        if state not in self.Q.keys(): #a\n",
    "            self.Q[state] = [0] * self.n_actions\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = env.action_space.sample() #ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state]) #melhor ação\n",
    "        return action\n",
    "\n",
    "    def epsilon_decay(self):\n",
    "        \"\"\"\n",
    "        Função que faz o decaimento de epsilon a cada episódio\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon*self.decay, self.epsilon_min)\n",
    "\n",
    "    def updates_Q(self, state, action, next_state, reward):\n",
    "        \"\"\"\n",
    "        Função que atualiza a tabela Q, de acordo com a equação do Q-Learning\n",
    "        \"\"\"\n",
    "        #state = tuple(state)\n",
    "        #next_state = tuple(next_state)\n",
    "        if state not in self.Q.keys():\n",
    "            self.Q[state] = [0] * self.n_actions\n",
    "        if next_state not in self.Q.keys():\n",
    "            self.Q[next_state] = [0] * self.n_actions\n",
    "\n",
    "        self.Q[state][action] = self.Q[state][action] + self.alpha*(reward + self.gamma*np.max(self.Q[next_state]) - self.Q[state][action])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já possuímos o algoritmo de Q-Learning programado, podemos inicializar o ambiente que será utilizado. No caso, usaremos o Taxi-V3 do gym. Além disso, verificamos o número de ações existentes no ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6\n"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos então o nosso agente, passando os parâmetros necessários para sua inicalização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(n_actions= n_actions,\n",
    "                       epsilon= 0.99,\n",
    "                       decay= 0.9999,\n",
    "                       epsilon_min= 0.01,\n",
    "                       alpha = 0.1,\n",
    "                       gamma = 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com nosso agente definido, entramos então no loop de treinamento, onde serão executados 10 000 episódios do ambiente, de acordo com o código a seguir: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Episode 1000: return -536, average returns -689.9, epsilon 0.90\nEpisode 2000: return -116, average returns -417.3, epsilon 0.81\nEpisode 3000: return -228, average returns -229.2, epsilon 0.73\nEpisode 4000: return -66, average returns -139.4, epsilon 0.66\nEpisode 5000: return -50, average returns -98.5, epsilon 0.60\nEpisode 6000: return -32, average returns -69.9, epsilon 0.54\nEpisode 7000: return -90, average returns -53.0, epsilon 0.49\nEpisode 8000: return -39, average returns -40.4, epsilon 0.44\nEpisode 9000: return -5, average returns -31.0, epsilon 0.40\nEpisode 10000: return -11, average returns -25.2, epsilon 0.36\nEpisode 11000: return -7, average returns -20.4, epsilon 0.33\nEpisode 12000: return 8, average returns -16.0, epsilon 0.30\nEpisode 13000: return -12, average returns -13.3, epsilon 0.27\nEpisode 14000: return -4, average returns -10.4, epsilon 0.24\nEpisode 15000: return -5, average returns -7.7, epsilon 0.22\nEpisode 16000: return 2, average returns -4.8, epsilon 0.20\nEpisode 17000: return 8, average returns -3.7, epsilon 0.18\nEpisode 18000: return 9, average returns -2.8, epsilon 0.16\nEpisode 19000: return -25, average returns -1.2, epsilon 0.15\nEpisode 20000: return 4, average returns 0.1, epsilon 0.13\nEpisode 21000: return 4, average returns 0.5, epsilon 0.12\nEpisode 22000: return -20, average returns 0.8, epsilon 0.11\nEpisode 23000: return -15, average returns 2.0, epsilon 0.10\nEpisode 24000: return -24, average returns 2.7, epsilon 0.09\nEpisode 25000: return 8, average returns 3.2, epsilon 0.08\nEpisode 26000: return 7, average returns 4.0, epsilon 0.07\nEpisode 27000: return 6, average returns 4.2, epsilon 0.07\nEpisode 28000: return 11, average returns 4.3, epsilon 0.06\nEpisode 29000: return 8, average returns 5.0, epsilon 0.05\nEpisode 30000: return 6, average returns 5.3, epsilon 0.05\nEpisode 31000: return -7, average returns 5.4, epsilon 0.04\nEpisode 32000: return 11, average returns 5.9, epsilon 0.04\nEpisode 33000: return 4, average returns 6.2, epsilon 0.04\nEpisode 34000: return 6, average returns 6.2, epsilon 0.03\nEpisode 35000: return 11, average returns 6.4, epsilon 0.03\nEpisode 36000: return 9, average returns 6.6, epsilon 0.03\nEpisode 37000: return 5, average returns 6.5, epsilon 0.02\nEpisode 38000: return 11, average returns 6.5, epsilon 0.02\nEpisode 39000: return 5, average returns 6.8, epsilon 0.02\nEpisode 40000: return 11, average returns 6.7, epsilon 0.02\nEpisode 41000: return 9, average returns 7.0, epsilon 0.02\nEpisode 42000: return 8, average returns 7.2, epsilon 0.01\nEpisode 43000: return 8, average returns 7.2, epsilon 0.01\nEpisode 44000: return 4, average returns 7.4, epsilon 0.01\nEpisode 45000: return 8, average returns 7.2, epsilon 0.01\nEpisode 46000: return 9, average returns 7.4, epsilon 0.01\nEpisode 47000: return 9, average returns 7.6, epsilon 0.01\nEpisode 48000: return 6, average returns 7.5, epsilon 0.01\nEpisode 49000: return 10, average returns 7.3, epsilon 0.01\nEpisode 50000: return -2, average returns 7.3, epsilon 0.01\nEpisode 51000: return 9, average returns 7.4, epsilon 0.01\nEpisode 52000: return 7, average returns 7.6, epsilon 0.01\nEpisode 53000: return 7, average returns 7.7, epsilon 0.01\nEpisode 54000: return 6, average returns 7.5, epsilon 0.01\nEpisode 55000: return 4, average returns 7.4, epsilon 0.01\nEpisode 56000: return 8, average returns 7.2, epsilon 0.01\nEpisode 57000: return 8, average returns 7.3, epsilon 0.01\nEpisode 58000: return 8, average returns 7.4, epsilon 0.01\nEpisode 59000: return 4, average returns 7.6, epsilon 0.01\nEpisode 60000: return 7, average returns 7.2, epsilon 0.01\nEpisode 61000: return 4, average returns 7.4, epsilon 0.01\nEpisode 62000: return 9, average returns 7.2, epsilon 0.01\nEpisode 63000: return 6, average returns 7.4, epsilon 0.01\nEpisode 64000: return 7, average returns 7.5, epsilon 0.01\nEpisode 65000: return 3, average returns 7.7, epsilon 0.01\nEpisode 66000: return 10, average returns 7.4, epsilon 0.01\nEpisode 67000: return 10, average returns 7.6, epsilon 0.01\nEpisode 68000: return 11, average returns 7.6, epsilon 0.01\nEpisode 69000: return 13, average returns 7.5, epsilon 0.01\nEpisode 70000: return 7, average returns 7.4, epsilon 0.01\nEpisode 71000: return 4, average returns 7.4, epsilon 0.01\nEpisode 72000: return 10, average returns 7.5, epsilon 0.01\nEpisode 73000: return 5, average returns 7.4, epsilon 0.01\nEpisode 74000: return 9, average returns 7.6, epsilon 0.01\nEpisode 75000: return 7, average returns 7.7, epsilon 0.01\nEpisode 76000: return 7, average returns 7.4, epsilon 0.01\nEpisode 77000: return 11, average returns 7.5, epsilon 0.01\nEpisode 78000: return 10, average returns 7.3, epsilon 0.01\nEpisode 79000: return 4, average returns 7.5, epsilon 0.01\nEpisode 80000: return 6, average returns 7.4, epsilon 0.01\nEpisode 81000: return 8, average returns 7.3, epsilon 0.01\nEpisode 82000: return 12, average returns 7.5, epsilon 0.01\nEpisode 83000: return 9, average returns 7.5, epsilon 0.01\nEpisode 84000: return 8, average returns 7.4, epsilon 0.01\nEpisode 85000: return 8, average returns 7.5, epsilon 0.01\nEpisode 86000: return 4, average returns 7.4, epsilon 0.01\nEpisode 87000: return 8, average returns 7.4, epsilon 0.01\nEpisode 88000: return 7, average returns 7.4, epsilon 0.01\nEpisode 89000: return 6, average returns 7.4, epsilon 0.01\nEpisode 90000: return 6, average returns 7.5, epsilon 0.01\nEpisode 91000: return 9, average returns 7.3, epsilon 0.01\nEpisode 92000: return 10, average returns 7.4, epsilon 0.01\nEpisode 93000: return 8, average returns 7.4, epsilon 0.01\nEpisode 94000: return 8, average returns 7.4, epsilon 0.01\nEpisode 95000: return 10, average returns 7.2, epsilon 0.01\nEpisode 96000: return 9, average returns 7.6, epsilon 0.01\nEpisode 97000: return 14, average returns 7.4, epsilon 0.01\nEpisode 98000: return 4, average returns 7.4, epsilon 0.01\nEpisode 99000: return -2, average returns 7.3, epsilon 0.01\nEpisode 100000: return 7, average returns 7.1, epsilon 0.01\n"
    }
   ],
   "source": [
    "returns_list = []\n",
    "\n",
    "for episode in range(1, 100001): #episodios\n",
    "    #reinicia variaveis e o estado no comeco de cada episodio\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    returns = 0\n",
    "    \n",
    "    \n",
    "    while not done: #enquanto o episodio nao termina\n",
    "        action = agent.choose_action(env, state)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "       #next_state = discretize(env, next_state)\n",
    "\n",
    "        agent.updates_Q(state, action, next_state, reward)\n",
    "\n",
    "        #atualiza valores do retorno e do estado atual\n",
    "        returns += reward\n",
    "        state = next_state\n",
    "    \n",
    "    agent.epsilon_decay() #decai o epsilon\n",
    "    returns_list.append(returns) \n",
    "\n",
    "    if episode %1000 == 0:\n",
    "        print(f\"Episode {episode}: return {returns}, average returns {np.mean(returns_list[-1000:]):.1f}, epsilon {agent.epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que o tivemos bons resultados após o treinamento, tendo um retorno médio bem superior ao inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}