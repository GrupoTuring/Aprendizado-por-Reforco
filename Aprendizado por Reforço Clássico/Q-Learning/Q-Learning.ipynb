{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598806918371",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, epsilon, decay, epsilon_min, alpha, gamma, n_actions):\n",
    "        self.Q = {}\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def choose_action(self, env, state):\n",
    "        #state = tuple(state)\n",
    "        if state not in self.Q.keys():\n",
    "            self.Q[state] = [0] * self.n_actions\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action\n",
    "\n",
    "    def epsilon_decay(self):\n",
    "        self.epsilon = max(self.epsilon*self.decay, self.epsilon_min)\n",
    "\n",
    "    def updates_Q(self, state, action, next_state, reward):\n",
    "        #state = tuple(state)\n",
    "        #next_state = tuple(next_state)\n",
    "        if state not in self.Q.keys():\n",
    "            self.Q[state] = [0] * self.n_actions\n",
    "        if next_state not in self.Q.keys():\n",
    "            self.Q[next_state] = [0] * self.n_actions\n",
    "\n",
    "        self.Q[state][action] = self.Q[state][action] + self.alpha*(reward + self.gamma*np.max(self.Q[next_state]) - self.Q[state][action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6\n"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(n_actions= n_actions,\n",
    "                       epsilon= 0.99,\n",
    "                       decay= 0.9999,\n",
    "                       epsilon_min= 0.01,\n",
    "                       alpha = 0.1,\n",
    "                       gamma = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Episode 1000: return -536, average returns -689.9, epsilon 0.90\nEpisode 2000: return -116, average returns -417.3, epsilon 0.81\nEpisode 3000: return -228, average returns -229.2, epsilon 0.73\nEpisode 4000: return -66, average returns -139.4, epsilon 0.66\nEpisode 5000: return -50, average returns -98.5, epsilon 0.60\nEpisode 6000: return -32, average returns -69.9, epsilon 0.54\nEpisode 7000: return -90, average returns -53.0, epsilon 0.49\nEpisode 8000: return -39, average returns -40.4, epsilon 0.44\nEpisode 9000: return -5, average returns -31.0, epsilon 0.40\nEpisode 10000: return -11, average returns -25.2, epsilon 0.36\nEpisode 11000: return -7, average returns -20.4, epsilon 0.33\nEpisode 12000: return 8, average returns -16.0, epsilon 0.30\nEpisode 13000: return -12, average returns -13.3, epsilon 0.27\nEpisode 14000: return -4, average returns -10.4, epsilon 0.24\nEpisode 15000: return -5, average returns -7.7, epsilon 0.22\nEpisode 16000: return 2, average returns -4.8, epsilon 0.20\nEpisode 17000: return 8, average returns -3.7, epsilon 0.18\nEpisode 18000: return 9, average returns -2.8, epsilon 0.16\nEpisode 19000: return -25, average returns -1.2, epsilon 0.15\nEpisode 20000: return 4, average returns 0.1, epsilon 0.13\nEpisode 21000: return 4, average returns 0.5, epsilon 0.12\nEpisode 22000: return -20, average returns 0.8, epsilon 0.11\nEpisode 23000: return -15, average returns 2.0, epsilon 0.10\nEpisode 24000: return -24, average returns 2.7, epsilon 0.09\nEpisode 25000: return 8, average returns 3.2, epsilon 0.08\nEpisode 26000: return 7, average returns 4.0, epsilon 0.07\nEpisode 27000: return 6, average returns 4.2, epsilon 0.07\nEpisode 28000: return 11, average returns 4.3, epsilon 0.06\nEpisode 29000: return 8, average returns 5.0, epsilon 0.05\nEpisode 30000: return 6, average returns 5.3, epsilon 0.05\nEpisode 31000: return -7, average returns 5.4, epsilon 0.04\nEpisode 32000: return 11, average returns 5.9, epsilon 0.04\nEpisode 33000: return 4, average returns 6.2, epsilon 0.04\nEpisode 34000: return 6, average returns 6.2, epsilon 0.03\nEpisode 35000: return 11, average returns 6.4, epsilon 0.03\nEpisode 36000: return 9, average returns 6.6, epsilon 0.03\nEpisode 37000: return 5, average returns 6.5, epsilon 0.02\nEpisode 38000: return 11, average returns 6.5, epsilon 0.02\nEpisode 39000: return 5, average returns 6.8, epsilon 0.02\nEpisode 40000: return 11, average returns 6.7, epsilon 0.02\nEpisode 41000: return 9, average returns 7.0, epsilon 0.02\nEpisode 42000: return 8, average returns 7.2, epsilon 0.01\nEpisode 43000: return 8, average returns 7.2, epsilon 0.01\nEpisode 44000: return 4, average returns 7.4, epsilon 0.01\nEpisode 45000: return 8, average returns 7.2, epsilon 0.01\nEpisode 46000: return 9, average returns 7.4, epsilon 0.01\nEpisode 47000: return 9, average returns 7.6, epsilon 0.01\nEpisode 48000: return 6, average returns 7.5, epsilon 0.01\nEpisode 49000: return 10, average returns 7.3, epsilon 0.01\nEpisode 50000: return -2, average returns 7.3, epsilon 0.01\nEpisode 51000: return 9, average returns 7.4, epsilon 0.01\nEpisode 52000: return 7, average returns 7.6, epsilon 0.01\nEpisode 53000: return 7, average returns 7.7, epsilon 0.01\nEpisode 54000: return 6, average returns 7.5, epsilon 0.01\nEpisode 55000: return 4, average returns 7.4, epsilon 0.01\nEpisode 56000: return 8, average returns 7.2, epsilon 0.01\nEpisode 57000: return 8, average returns 7.3, epsilon 0.01\nEpisode 58000: return 8, average returns 7.4, epsilon 0.01\nEpisode 59000: return 4, average returns 7.6, epsilon 0.01\nEpisode 60000: return 7, average returns 7.2, epsilon 0.01\nEpisode 61000: return 4, average returns 7.4, epsilon 0.01\nEpisode 62000: return 9, average returns 7.2, epsilon 0.01\nEpisode 63000: return 6, average returns 7.4, epsilon 0.01\nEpisode 64000: return 7, average returns 7.5, epsilon 0.01\nEpisode 65000: return 3, average returns 7.7, epsilon 0.01\nEpisode 66000: return 10, average returns 7.4, epsilon 0.01\nEpisode 67000: return 10, average returns 7.6, epsilon 0.01\nEpisode 68000: return 11, average returns 7.6, epsilon 0.01\nEpisode 69000: return 13, average returns 7.5, epsilon 0.01\nEpisode 70000: return 7, average returns 7.4, epsilon 0.01\nEpisode 71000: return 4, average returns 7.4, epsilon 0.01\nEpisode 72000: return 10, average returns 7.5, epsilon 0.01\nEpisode 73000: return 5, average returns 7.4, epsilon 0.01\nEpisode 74000: return 9, average returns 7.6, epsilon 0.01\nEpisode 75000: return 7, average returns 7.7, epsilon 0.01\nEpisode 76000: return 7, average returns 7.4, epsilon 0.01\nEpisode 77000: return 11, average returns 7.5, epsilon 0.01\nEpisode 78000: return 10, average returns 7.3, epsilon 0.01\nEpisode 79000: return 4, average returns 7.5, epsilon 0.01\nEpisode 80000: return 6, average returns 7.4, epsilon 0.01\nEpisode 81000: return 8, average returns 7.3, epsilon 0.01\nEpisode 82000: return 12, average returns 7.5, epsilon 0.01\nEpisode 83000: return 9, average returns 7.5, epsilon 0.01\nEpisode 84000: return 8, average returns 7.4, epsilon 0.01\nEpisode 85000: return 8, average returns 7.5, epsilon 0.01\nEpisode 86000: return 4, average returns 7.4, epsilon 0.01\nEpisode 87000: return 8, average returns 7.4, epsilon 0.01\nEpisode 88000: return 7, average returns 7.4, epsilon 0.01\nEpisode 89000: return 6, average returns 7.4, epsilon 0.01\nEpisode 90000: return 6, average returns 7.5, epsilon 0.01\nEpisode 91000: return 9, average returns 7.3, epsilon 0.01\nEpisode 92000: return 10, average returns 7.4, epsilon 0.01\nEpisode 93000: return 8, average returns 7.4, epsilon 0.01\nEpisode 94000: return 8, average returns 7.4, epsilon 0.01\nEpisode 95000: return 10, average returns 7.2, epsilon 0.01\nEpisode 96000: return 9, average returns 7.6, epsilon 0.01\nEpisode 97000: return 14, average returns 7.4, epsilon 0.01\nEpisode 98000: return 4, average returns 7.4, epsilon 0.01\nEpisode 99000: return -2, average returns 7.3, epsilon 0.01\nEpisode 100000: return 7, average returns 7.1, epsilon 0.01\n"
    }
   ],
   "source": [
    "returns_list = []\n",
    "\n",
    "for episode in range(1, 100001):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    returns = 0\n",
    "    \n",
    "    #state = discretize(env, state)\n",
    "    while not done:\n",
    "        action = agent.choose_action(env, state)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "       #next_state = discretize(env, next_state)\n",
    "\n",
    "        agent.updates_Q(state, action, next_state, reward)\n",
    "\n",
    "        returns += reward\n",
    "        state = next_state\n",
    "    \n",
    "    agent.epsilon_decay()\n",
    "    returns_list.append(returns)\n",
    "\n",
    "    if episode %1000 == 0:\n",
    "        print(f\"Episode {episode}: return {returns}, average returns {np.mean(returns_list[-1000:]):.1f}, epsilon {agent.epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}