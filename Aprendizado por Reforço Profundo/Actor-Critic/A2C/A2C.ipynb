{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2C.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjGWyD9Hyfg-"
      },
      "source": [
        "# A2C - Advantage Actor Critic\n",
        "\n",
        "Como vimos na aula de Policy Gradients, existem algoritmos capazes de aprender a política diretamente, utilizando gradiente ascendente neste processo. Porém, modelos como REINFORCE sofrem com grande variância. Para tentar solucionar este problema, surge a ideia de utilizar uma estimativa do retorno. Cada uma dessas estimativas leva a diferentes algoritmos Actor-Critic. Hoje, vamos abordar uma delas, a Advantage (vantagem):\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta log \\pi_\\theta(s,a)\\ G_t]\\ \\ \\ \\text{REINFORCE}$$\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta log \\pi_\\theta(s,a)\\ Q^w(s,a)]\\ \\ \\ \\text{Q Actor-Critic}$$\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta log \\pi_\\theta(s,a)\\ A^w(s,a)]\\ \\ \\ \\text{Advantage Actor-Critic}$$\n",
        "\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) =  \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta log \\pi_\\theta(s,a)\\ \\delta]\\ \\ \\ \\text{TD Actor-Critic}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## O Ator e o Crítico\n",
        "\n",
        "Antes de entrarmos no A2C em si, é interessante explorar o que cada parte do algoritmo faz. Resumindo, o **crítico** atualiza as estimativas da função valor enquanto o **ator** busca atualizar a política na direção sugerida pelo crítico.\n",
        "\n",
        "![interacao actor-critic](https://miro.medium.com/max/390/1*-GfRVLWhcuSYhG25rN0IbA.png)\n",
        "\n",
        "Para entender um pouco melhor, de uma forma mais alto nível, [recomendo essa história em quadrinhos](https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752) (sim, existem historias em quadrinhos de RL)\n",
        "\n",
        "Enxergando de outra forma, temos que o crítico é a função valor, medindo o quão boas são as ações tomadas, enquanto o ator é a função política, controlando as ações.\n",
        "\n",
        "* Ator: $\\pi(s, a, \\theta)$\n",
        "* Crítico: $\\hat{q}(s, a, \\omega)$\n",
        "\n",
        "\n",
        "## Advantage\n",
        "\n",
        "Uma das baselines que pode ser desenvolvida é a chamada vantagem, que é definida da seguinte forma:\n",
        "\n",
        "$$A(s, a) = Q(s, a) - V(s)$$\n",
        "\n",
        "Em resumo, a vantagem busca medir de forma *relativa* o valor de uma ação comparando com o valor médio das ações naquele estado. Se o valor da vantagem é positivo, levamos o nosso gradiente nesta direção.\n",
        "\n",
        "Uma das formas de estimar o valor de A(s, a) é usando o nosso velho amigo **bootstrapping**, onde podemos estimar o Q da seguinte forma:\n",
        "\n",
        "$$Q(s,a) = r + \\gamma V(s')$$\n",
        "\n",
        "Reescrevendo a vantagem, temos:\n",
        "\n",
        "$$A(s,a) = r + \\gamma V(s') - V(s)$$\n",
        "\n",
        "Com essa fórmula, precisamos estimar somente V e não Q. Existem outras formas de estimar a vantagem, como a [Generalized Advantage Estimation](https://arxiv.org/pdf/1506.02438.pdf)\n",
        "\n",
        "Dessa forma, podemos estimar o gradiente do Advantage Actor-Critic pela seguinte equação:\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) \\approx \\sum_{t = 0}^{T-1}  \\nabla_\\theta log \\pi_\\theta(s,a)(r_{t+1} + \\gamma V(s_{t+1}) - V(s_t))$$\n",
        "\n",
        "### A3C vs A2C\n",
        "\n",
        "O algoritmo inicial foi o A3C (Asynchronous Advantage Actor Critic), por meio de um paper da Deepmind ([Paper de A3C](https://arxiv.org/abs/1602.01783)). Segundo a OpenAI, a parte assíncrona do A3C não traz grande vantagem e é menos eficiente, por isso a ideia do A2C. Não tenho capacidade de entrar no mérito, então vou só pontuar as diferenças.\n",
        "\n",
        "\n",
        "Ambos os algoritmos atuam em múltiplos ambientes simultâneos, mas enquanto o A3C é atualizado cada vez que um deles termina uma batch de experiências, por isso assíncrono, o A2C espera todos os agentes obterem uma batch para então atualizar com todas simultaneamente, usando a média do update. As diferenças podem ser conferidas [neste link da OpenAI](https://openai.com/blog/baselines-acktr-a2c/).\n",
        "\n",
        "## Algoritmo do A2C\n",
        "\n",
        "O pseudocódigo do A2C pode ser visto na imagem a seguir:\n",
        "\n",
        "![pseudocode](https://i.postimg.cc/w38KTJQb/A2C-v6.png)\n",
        "\n",
        "É importante ressaltar que essa versão utiliza duas redes neurais, uma para o crítico e outra para o ator, mas implementações com uma só rede são possíveis e serão mostradas abaixo.\n",
        "\n",
        "\n",
        "## On Policy x Off Policy\n",
        "\n",
        "Agora que já vimos diversos algoritmos, desde o Q learning tabular até métodos Actor-Critic, é legal dar um passo atrás para avaliar a diferença entre os dois diferentes tipos de algoritmo em respeito à política:\n",
        "\n",
        "On Policy:\n",
        "  - Mais simples\n",
        "  - Mais rápido\n",
        "\n",
        "Off Policy:\n",
        "  - Sample Efficient \n",
        "    - Eficiente com poucas experiências\n",
        "    - Reaproveita experiências passadas\n",
        "  - Pode ser utilizado de maneira offline\n",
        "  - Maior variância\n",
        "    - Ruído decorrente de aprender uma política com dados de outra\n",
        "  - Mais lento\n",
        "\n",
        "## Referências\n",
        "https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
        "\n",
        "https://towardsdatascience.com/introduction-to-actor-critic-7642bdb2b3d2\n",
        "\n",
        "https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/\n",
        "\n",
        "http://incompleteideas.net/book/first/ebook/node66.html#:~:text=Actor%2Dcritic%20methods%20are%20TD,independent%20of%20the%20value%20function.&text=This%20scalar%20signal%20is%20the,%3A%20The%20actor%2Dcritic%20architecture.\n",
        "\n",
        "https://openai.com/blog/baselines-acktr-a2c/\n",
        "\n",
        "https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#id14\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btLijj8y7gLH"
      },
      "source": [
        "# Código\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkTEyq25ms2B"
      },
      "source": [
        "## Ator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOdt9KwNmcFR"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"Rede do Ator.\"\"\"\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        \"\"\"Inicializa a rede.\n",
        "        \n",
        "        Parâmetros\n",
        "        ----------\n",
        "        observation_shape: int\n",
        "        Formato do estado do ambiente.\n",
        "        \n",
        "        action_shape: int\n",
        "        Número de ações do ambiente.\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(observation_shape, 128)\n",
        "        self.linear2 = nn.Linear(128, 128)\n",
        "        self.linear3 = nn.Linear(128, action_shape)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Calcula a probabilidade de ação para o estado atual.\n",
        "        \n",
        "        Parâmetros\n",
        "        ----------\n",
        "        state: np.array\n",
        "        Estado atual.\n",
        "        \n",
        "        Retorna\n",
        "        -------\n",
        "        probs: Categorical\n",
        "        Distribuição de probabilidade das ações.\n",
        "        \"\"\"\n",
        "        dists = F.relu(self.linear1(state))\n",
        "        dists = F.relu(self.linear2(dists))\n",
        "        dists = F.softmax(self.linear3(dists), dim=1)\n",
        "        probs = Categorical(dists)\n",
        "\n",
        "        return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDbUOyZDmu6o"
      },
      "source": [
        "## Crítico"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2Ne0at-msW4"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"Rede do Crítico.\"\"\"\n",
        "    def __init__(self, observation_shape):\n",
        "        \"\"\"Inicializa a rede.\n",
        "        \n",
        "        Parâmetros\n",
        "        ----------\n",
        "        observation_shape: int\n",
        "        Formato do estado do ambiente.\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(observation_shape, 64)\n",
        "        self.linear2 = nn.Linear(64, 64)\n",
        "        self.linear3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Calcula o valor do estado atual.\n",
        "        \n",
        "        Parâmetros\n",
        "        ----------\n",
        "        state: np.array\n",
        "        Estado atual.\n",
        "        \n",
        "        Retorna\n",
        "        -------\n",
        "        v: float\n",
        "        Valor do estado atual.\n",
        "        \"\"\"\n",
        "        v = F.relu(self.linear1(state))\n",
        "        v = F.relu(self.linear2(v))\n",
        "        v = self.linear3(v)\n",
        "\n",
        "        return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZiSoaCSnX-3"
      },
      "source": [
        "## Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh5ztI22nXiX"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class ExperienceReplay:\n",
        "    \"\"\"Experience Replay Buffer para A2C.\"\"\"\n",
        "    def __init__(self, max_length, observation_space):\n",
        "        \"\"\"Cria um Replay Buffer.\n",
        "\n",
        "        Parâmetros\n",
        "        ----------\n",
        "        max_length: int\n",
        "            Tamanho máximo do Replay Buffer.\n",
        "        observation_space: int\n",
        "            Tamanho do espaço de observação.\n",
        "        \"\"\"\n",
        "        self.length = 0\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.states = np.zeros((max_length, observation_space), dtype=np.float32)\n",
        "        self.actions = np.zeros((max_length), dtype=np.int32)\n",
        "        self.rewards = np.zeros((max_length), dtype=np.float32)\n",
        "        self.next_states = np.zeros((max_length, observation_space), dtype=np.float32)\n",
        "        self.dones = np.zeros((max_length), dtype=np.float32)\n",
        "\n",
        "    def update(self, states, actions, rewards, next_states, dones):\n",
        "        \"\"\"Adiciona uma experiência ao Replay Buffer.\n",
        "\n",
        "        Parâmetros\n",
        "        ----------\n",
        "        state: np.array\n",
        "            Estado da transição.\n",
        "        action: int\n",
        "            Ação tomada.\n",
        "        reward: float\n",
        "            Recompensa recebida.\n",
        "        state: np.array\n",
        "            Estado seguinte.\n",
        "        done: int\n",
        "            Flag indicando se o episódio acabou.\n",
        "        \"\"\"\n",
        "        self.states[self.length] = states\n",
        "        self.actions[self.length] = actions\n",
        "        self.rewards[self.length] = rewards\n",
        "        self.next_states[self.length] = next_states\n",
        "        self.dones[self.length] = dones\n",
        "        self.length += 1\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Retorna um batch de experiências.\n",
        "        \n",
        "        Parâmetros\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            Tamanho do batch de experiências.\n",
        "\n",
        "        Retorna\n",
        "        -------\n",
        "        states: np.array\n",
        "            Batch de estados.\n",
        "        actions: np.array\n",
        "            Batch de ações.\n",
        "        rewards: np.array\n",
        "            Batch de recompensas.\n",
        "        next_states: np.array\n",
        "            Batch de estados seguintes.\n",
        "        dones: np.array\n",
        "            Batch de flags indicando se o episódio acabou.\n",
        "        \"\"\"\n",
        "        self.length = 0\n",
        "\n",
        "        return (self.states, self.actions, self.rewards, self.next_states, self.dones)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u-dalX7m-Co"
      },
      "source": [
        "## Agente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPhl6Qgnm9dX"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "class A2C:\n",
        "    def __init__(self, observation_space, action_space, p_lr=5e-4, v_lr=7e-4, gamma=0.99, entropy_coef=0.005, n_steps=5):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.entropy_coef = entropy_coef\n",
        "\n",
        "        self.n_steps = n_steps\n",
        "        self.memory = ExperienceReplay(n_steps, observation_space.shape[0])\n",
        "\n",
        "        self.actor = Actor(observation_space.shape[0], action_space.n).to(self.device)\n",
        "        self.critic = Critic(observation_space.shape[0]).to(self.device)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=p_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=v_lr)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
        "        probs = self.actor.forward(state)\n",
        "        \n",
        "        action = probs.sample().cpu().detach().item()\n",
        "        return action\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.update(state, action, reward, next_state, done)\n",
        "\n",
        "    def compute_advantages(self, rewards, dones, v, v2):\n",
        "        T = len(rewards)\n",
        "\n",
        "        returns = torch.zeros_like(rewards)\n",
        "        advantages = torch.zeros_like(rewards)\n",
        "        \n",
        "        next_return = torch.tensor(v2[-1], dtype=rewards.dtype)\n",
        "\n",
        "        not_dones = 1 - dones\n",
        "        advantages = rewards + not_dones * self.gamma * v2 - v\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "            returns[t] = next_return = rewards[t] + self.gamma * not_dones[t] * next_return\n",
        "\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8) # Normalização\n",
        "\n",
        "        return advantages, returns\n",
        "\n",
        "    def train(self):\n",
        "        if self.memory.length < self.n_steps:\n",
        "            return\n",
        "\n",
        "        (states, actions, rewards, next_states, dones) = self.memory.sample()\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.FloatTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).unsqueeze(-1).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).unsqueeze(-1).to(self.device)\n",
        "\n",
        "        v = self.critic.forward(states)\n",
        "        with torch.no_grad():\n",
        "          v2 = self.critic.forward(next_states)\n",
        "\n",
        "        advantages, returns = self.compute_advantages(rewards, dones, v, v2)\n",
        "\n",
        "        probs = self.actor.forward(states)\n",
        "        logp = -probs.log_prob(actions)\n",
        "        entropy = probs.entropy().mean()\n",
        "\n",
        "        policy_loss =   (logp.unsqueeze(-1) * advantages.detach()).mean()\n",
        "        value_loss =    F.mse_loss(v, returns.detach())\n",
        "        entropy_loss = -self.entropy_coef * entropy\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        (policy_loss + entropy_loss).backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        return float(policy_loss + entropy_loss + value_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu7U6Vmzoydm"
      },
      "source": [
        "## Treinando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1aZ7DypOfN"
      },
      "source": [
        "import math\n",
        "from collections import deque\n",
        "\n",
        "def train(agent, env, total_timesteps):\n",
        "    total_reward = 0\n",
        "    episode_returns = deque(maxlen=20)\n",
        "    avg_returns = []\n",
        "\n",
        "    state = env.reset()\n",
        "    timestep = 0\n",
        "    episode = 0\n",
        "\n",
        "    while timestep < total_timesteps:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        loss = agent.train()\n",
        "        timestep += 1\n",
        "\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            episode_returns.append(total_reward)\n",
        "            episode += 1\n",
        "            next_state = env.reset()\n",
        "\n",
        "        if episode_returns:\n",
        "            avg_returns.append(np.mean(episode_returns))\n",
        "\n",
        "        total_reward *= 1 - done\n",
        "        state = next_state\n",
        "\n",
        "        ratio = math.ceil(100 * timestep / total_timesteps)\n",
        "\n",
        "        avg_return = avg_returns[-1] if avg_returns else np.nan\n",
        "        \n",
        "        print(f\"\\r[{ratio:3d}%] timestep = {timestep}/{total_timesteps}, episode = {episode:3d}, avg_return = {avg_return:10.4f}\", end=\"\")\n",
        "\n",
        "    return avg_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OGWuPmeox9X"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMax8WJxo01m"
      },
      "source": [
        "agent = A2C(env.observation_space, env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcWebqIBo2PG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "03febf33-a844-449b-80f0-225a840f400e"
      },
      "source": [
        "returns = train(agent, env, 75000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[  1%] timestep = 1/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 2/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 3/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 4/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 5/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 6/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 7/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 8/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 9/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 10/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 11/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 12/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 13/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 14/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 15/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 16/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 17/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 18/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 19/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 20/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 21/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 22/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 23/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 24/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 25/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 26/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 27/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 28/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 29/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 30/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 31/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 32/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 33/75000, episode =   1, avg_return =    10.0000\r[  1%] timestep = 34/75000, episode =   2, avg_return =    17.0000\r[  1%] timestep = 35/75000, episode =   2, avg_return =    17.0000\r[  1%] timestep = 36/75000, episode =   2, avg_return =    17.0000\r[  1%] timestep = 37/75000, episode =   2, avg_return =    17.0000\r[  1%] timestep = 38/75000, episode =   2, avg_return =    17.0000\r[  1%] timestep = 39/75000, episode =   2, avg_return =    17.0000\r[  1%] timestep = 40/75000, episode =   2, avg_return =    17.0000\r[  1%] timestep = 41/75000, episode =   2, avg_return =    17.0000\r[  1%] timestep = 42/75000, episode =   2, avg_return =    17.0000\r[  1%] timestep = 43/75000, episode =   2, avg_return =    17.0000\r[  1%] timestep = 44/75000, episode =   2, avg_return =    17.0000\r[  1%] timestep = 45/75000, episode =   2, avg_return =    17.0000\r[  1%] timestep = 46/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 47/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 48/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 49/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 50/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 51/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 52/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 53/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 54/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 55/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 56/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 57/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 58/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 59/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 60/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 61/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 62/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 63/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 64/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 65/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 66/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 67/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 68/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 69/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 70/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 71/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 72/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 73/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 74/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 75/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 76/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 77/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 78/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 79/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 80/75000, episode =   3, avg_return =    15.3333\r[  1%] timestep = 81/75000, episode =   4, avg_return =    20.2500\r[  1%] timestep = 82/75000, episode =   4, avg_return =    20.2500\r[  1%] timestep = 83/75000, episode =   4, avg_return =    20.2500\r[  1%] timestep = 84/75000, episode =   4, avg_return =    20.2500\r[  1%] timestep = 85/75000, episode =   4, avg_return =    20.2500\r[  1%] timestep = 86/75000, episode =   4, avg_return =    20.2500\r[  1%] timestep = 87/75000, episode =   4, avg_return =    20.2500\r[  1%] timestep = 88/75000, episode =   4, avg_return =    20.2500\r[  1%] timestep = 89/75000, episode =   4, avg_return =    20.2500\r[  1%] timestep = 90/75000, episode =   4, avg_return =    20.2500\r[  1%] timestep = 91/75000, episode =   4, avg_return =    20.2500\r[  1%] timestep = 92/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 93/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 94/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 95/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 96/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 97/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 98/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 99/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 100/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 101/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 102/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 103/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 104/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 105/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 106/75000, episode =   5, avg_return =    18.4000\r[  1%] timestep = 107/75000, episode =   6, avg_return =    17.8333\r[  1%] timestep = 108/75000, episode =   6, avg_return =    17.8333\r[  1%] timestep = 109/75000, episode =   6, avg_return =    17.8333\r[  1%] timestep = 110/75000, episode =   6, avg_return =    17.8333\r[  1%] timestep = 111/75000, episode =   6, avg_return =    17.8333\r[  1%] timestep = 112/75000, episode =   6, avg_return =    17.8333\r[  1%] timestep = 113/75000, episode =   6, avg_return =    17.8333\r[  1%] timestep = 114/75000, episode =   6, avg_return =    17.8333\r[  1%] timestep = 115/75000, episode =   6, avg_return =    17.8333\r[  1%] timestep = 116/75000, episode =   6, avg_return =    17.8333\r[  1%] timestep = 117/75000, episode =   6, avg_return =    17.8333\r[  1%] timestep = 118/75000, episode =   6, avg_return =    17.8333\r[  1%] timestep = 119/75000, episode =   7, avg_return =    17.0000\r[  1%] timestep = 120/75000, episode =   7, avg_return =    17.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[100%] timestep = 75000/75000, episode = 831, avg_return =   167.9500"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYBC2Z14va6v"
      },
      "source": [
        "##  Rede Dividida"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41fE0sOivQ8S"
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.policy1 = nn.Linear(observation_shape, 64)\n",
        "        self.policy2 = nn.Linear(64, 64)\n",
        "        self.policy3 = nn.Linear(64, action_shape)\n",
        "        \n",
        "        self.value1 = nn.Linear(observation_shape, 64)\n",
        "        self.value2 = nn.Linear(64, 64)\n",
        "        self.value3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        dists = torch.tanh(self.policy1(state))\n",
        "        dists = torch.tanh(self.policy2(dists))\n",
        "        dists = F.softmax(self.policy3(dists), dim=-1)\n",
        "        probs = Categorical(dists)\n",
        "        \n",
        "        v = torch.tanh(self.value1(state))\n",
        "        v = torch.tanh(self.value2(v))\n",
        "        v = self.value3(v)\n",
        "\n",
        "        return probs, v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E55ppd0ovfH4"
      },
      "source": [
        "class SharedA2C:\n",
        "    def __init__(self, observation_space, action_space, lr=7e-4, gamma=0.99, lam=0.95, vf_coef=0.5, entropy_coef=0.005, n_steps=5):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.vf_coef = vf_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "\n",
        "        self.n_steps = n_steps\n",
        "        self.memory = ExperienceReplay(n_steps, observation_space.shape[0])\n",
        "\n",
        "        self.actorcritic = ActorCritic(observation_space.shape[0], action_space.n).to(self.device)\n",
        "        self.actorcritic_optimizer = optim.Adam(self.actorcritic.parameters(), lr=lr)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
        "        probs, _ = self.actorcritic.forward(state)\n",
        "        action = probs.sample().cpu().detach().item()\n",
        "        return action\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.update(state, action, reward, next_state, done)\n",
        "\n",
        "    def compute_gae(self, rewards, dones, v, v2):\n",
        "        T = len(rewards)\n",
        "\n",
        "        returns = torch.zeros_like(rewards)\n",
        "        gaes = torch.zeros_like(rewards)\n",
        "        \n",
        "        future_gae = torch.tensor(0.0, dtype=rewards.dtype)\n",
        "        next_return = torch.tensor(v2[-1], dtype=rewards.dtype)\n",
        "\n",
        "        not_dones = 1 - dones\n",
        "        deltas = rewards + not_dones * self.gamma * v2 - v\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "            returns[t] = next_return = rewards[t] + self.gamma * not_dones[t] * next_return\n",
        "            gaes[t] = future_gae = deltas[t] + self.gamma * self.lam * not_dones[t] * future_gae\n",
        "\n",
        "        gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8) # Normalização\n",
        "\n",
        "        return gaes, returns\n",
        "\n",
        "    def train(self):\n",
        "        if self.memory.length < self.n_steps:\n",
        "            return\n",
        "\n",
        "        (states, actions, rewards, next_states, dones) = self.memory.sample()\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.FloatTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).unsqueeze(-1).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).unsqueeze(-1).to(self.device)\n",
        "\n",
        "        probs, v = self.actorcritic.forward(states)\n",
        "        with torch.no_grad():\n",
        "          _, v2 = self.actorcritic.forward(next_states)\n",
        "\n",
        "        advantages, returns = self.compute_gae(rewards, dones, v, v2)\n",
        "\n",
        "        logp = -probs.log_prob(actions)\n",
        "        entropy = probs.entropy().mean()\n",
        "\n",
        "        policy_loss =   (logp.unsqueeze(-1) * advantages.detach()).mean()\n",
        "        value_loss =    self.vf_coef * F.mse_loss(v, returns.detach())\n",
        "        entropy_loss = -self.entropy_coef * entropy\n",
        "\n",
        "        self.actorcritic_optimizer.zero_grad()\n",
        "        (policy_loss + entropy_loss + value_loss).backward()\n",
        "        self.actorcritic_optimizer.step()\n",
        "\n",
        "        return policy_loss + entropy_loss + value_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGtTSQ-pwDrI"
      },
      "source": [
        "env = gym.make(\"CartPole-v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyUcshjiwDrL"
      },
      "source": [
        "agent = SharedA2C(env.observation_space, env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMrVG7wHwDrN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2046e3bc-d957-487b-e431-5215bca2679c"
      },
      "source": [
        "returns = train(agent, env, 75000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[  1%] timestep = 1/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 2/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 3/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 4/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 5/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 6/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 7/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 8/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 9/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 10/75000, episode =   0, avg_return =        nan\r[  1%] timestep = 11/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 12/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 13/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 14/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 15/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 16/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 17/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 18/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 19/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 20/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 21/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 22/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 23/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 24/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 25/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 26/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 27/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 28/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 29/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 30/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 31/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 32/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 33/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 34/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 35/75000, episode =   1, avg_return =    11.0000\r[  1%] timestep = 36/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 37/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 38/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 39/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 40/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 41/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 42/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 43/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 44/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 45/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 46/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 47/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 48/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 49/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 50/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 51/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 52/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 53/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 54/75000, episode =   2, avg_return =    18.0000\r[  1%] timestep = 55/75000, episode =   3, avg_return =    18.3333\r[  1%] timestep = 56/75000, episode =   3, avg_return =    18.3333\r[  1%] timestep = 57/75000, episode =   3, avg_return =    18.3333\r[  1%] timestep = 58/75000, episode =   3, avg_return =    18.3333\r[  1%] timestep = 59/75000, episode =   3, avg_return =    18.3333\r[  1%] timestep = 60/75000, episode =   3, avg_return =    18.3333\r[  1%] timestep = 61/75000, episode =   3, avg_return =    18.3333\r[  1%] timestep = 62/75000, episode =   3, avg_return =    18.3333\r[  1%] timestep = 63/75000, episode =   3, avg_return =    18.3333\r[  1%] timestep = 64/75000, episode =   3, avg_return =    18.3333\r[  1%] timestep = 65/75000, episode =   3, avg_return =    18.3333\r[  1%] timestep = 66/75000, episode =   3, avg_return =    18.3333\r[  1%] timestep = 67/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 68/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 69/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 70/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 71/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 72/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 73/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 74/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 75/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 76/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 77/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 78/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 79/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 80/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 81/75000, episode =   4, avg_return =    16.7500\r[  1%] timestep = 82/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 83/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 84/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 85/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 86/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 87/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 88/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 89/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 90/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 91/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 92/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 93/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 94/75000, episode =   5, avg_return =    16.4000\r[  1%] timestep = 95/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 96/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 97/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 98/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 99/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 100/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 101/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 102/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 103/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 104/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 105/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 106/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 107/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 108/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 109/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 110/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 111/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 112/75000, episode =   6, avg_return =    15.8333\r[  1%] timestep = 113/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 114/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 115/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 116/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 117/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 118/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 119/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 120/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 121/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 122/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 123/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 124/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 125/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 126/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 127/75000, episode =   7, avg_return =    16.1429\r[  1%] timestep = 128/75000, episode =   7, avg_return =    16.1429"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[100%] timestep = 75000/75000, episode = 625, avg_return =   496.3500"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxAjgrZ81D0e"
      },
      "source": [
        "## Múltiplos Ambientes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fmNerrB1cAU"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class MultipleExperienceReplay:\n",
        "    def __init__(self, max_length, env_num, observation_space):\n",
        "        self.length = 0\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.states = np.zeros((max_length, env_num, observation_space), dtype=np.float32)\n",
        "        self.actions = np.zeros((max_length, env_num), dtype=np.int32)\n",
        "        self.rewards = np.zeros((max_length, env_num), dtype=np.float32)\n",
        "        self.next_states = np.zeros((max_length, env_num, observation_space), dtype=np.float32)\n",
        "        self.dones = np.zeros((max_length, env_num), dtype=np.float32)\n",
        "\n",
        "    def update(self, states, actions, rewards, next_states, dones):\n",
        "        self.states[self.length] = states\n",
        "        self.actions[self.length] = actions\n",
        "        self.rewards[self.length] = rewards\n",
        "        self.next_states[self.length] = next_states\n",
        "        self.dones[self.length] = dones\n",
        "        self.length += 1\n",
        "\n",
        "    def sample(self):\n",
        "        self.length = 0\n",
        "\n",
        "        return (self.states, self.actions, self.rewards, self.next_states, self.dones)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99fj4r0U1Caq"
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.policy1 = nn.Linear(observation_shape, 64)\n",
        "        self.policy2 = nn.Linear(64, 64)\n",
        "        self.policy3 = nn.Linear(64, action_shape)\n",
        "        \n",
        "        self.value1 = nn.Linear(observation_shape, 64)\n",
        "        self.value2 = nn.Linear(64, 64)\n",
        "        self.value3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        dists = torch.tanh(self.policy1(state))\n",
        "        dists = torch.tanh(self.policy2(dists))\n",
        "        dists = F.softmax(self.policy3(dists), dim=-1)\n",
        "        probs = Categorical(dists)\n",
        "        \n",
        "        v = torch.tanh(self.value1(state))\n",
        "        v = torch.tanh(self.value2(v))\n",
        "        v = self.value3(v)\n",
        "\n",
        "        return probs, v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgpS39h31JDg"
      },
      "source": [
        "class SharedA2C:\n",
        "    def __init__(self, observation_space, action_space, env_num, lr=7e-4, gamma=0.99, lam=0.95, vf_coef=0.5, entropy_coef=0.005, n_steps=5):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.vf_coef = vf_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "\n",
        "        self.n_steps = n_steps\n",
        "        self.memory = MultipleExperienceReplay(n_steps, env_num, observation_space.shape[0])\n",
        "\n",
        "        self.actorcritic = ActorCritic(observation_space.shape[0], action_space.n).to(self.device)\n",
        "        self.actorcritic_optimizer = optim.Adam(self.actorcritic.parameters(), lr=lr)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.FloatTensor(state).to(self.device)\n",
        "        probs, _ = self.actorcritic.forward(state)\n",
        "        action = probs.sample().cpu().detach().numpy()\n",
        "        return action\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.update(state, action, reward, next_state, done)\n",
        "\n",
        "    def compute_gae(self, rewards, dones, v, v2):\n",
        "        T = len(rewards)\n",
        "\n",
        "        returns = torch.zeros_like(rewards)\n",
        "        gaes = torch.zeros_like(rewards)\n",
        "        \n",
        "        future_gae = torch.tensor(0.0, dtype=rewards.dtype)\n",
        "        next_return = torch.tensor(v2[-1], dtype=rewards.dtype)\n",
        "\n",
        "        not_dones = 1 - dones\n",
        "        deltas = rewards + not_dones * self.gamma * v2 - v\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "            returns[t] = next_return = rewards[t] + self.gamma * not_dones[t] * next_return\n",
        "            gaes[t] = future_gae = deltas[t] + self.gamma * self.lam * not_dones[t] * future_gae\n",
        "\n",
        "        gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8) # Normalização\n",
        "\n",
        "        return gaes, returns\n",
        "\n",
        "    def train(self):\n",
        "        if self.memory.length < self.n_steps:\n",
        "            return\n",
        "\n",
        "        (states, actions, rewards, next_states, dones) = self.memory.sample()\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.FloatTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).unsqueeze(-1).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).unsqueeze(-1).to(self.device)\n",
        "\n",
        "        probs, v = self.actorcritic.forward(states)\n",
        "        with torch.no_grad():\n",
        "          _, v2 = self.actorcritic.forward(next_states)\n",
        "\n",
        "        advantages, returns = self.compute_gae(rewards, dones, v, v2)\n",
        "\n",
        "        logp = -probs.log_prob(actions)\n",
        "        entropy = probs.entropy().mean()\n",
        "\n",
        "        policy_loss =   (logp.unsqueeze(-1) * advantages.detach()).mean()\n",
        "        value_loss =    self.vf_coef * F.mse_loss(v, returns.detach())\n",
        "        entropy_loss = -self.entropy_coef * entropy\n",
        "\n",
        "        self.actorcritic_optimizer.zero_grad()\n",
        "        (policy_loss + entropy_loss + value_loss).backward()\n",
        "        self.actorcritic_optimizer.step()\n",
        "\n",
        "        return policy_loss + entropy_loss + value_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhpeeumo1Jvg"
      },
      "source": [
        "env = gym.vector.make(\"CartPole-v1\", num_envs=8, asynchronous=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhILSotY2D4q"
      },
      "source": [
        "agent = SharedA2C(env.single_observation_space, env.single_action_space, env.num_envs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_Ype5FR2R4i"
      },
      "source": [
        "import time\n",
        "\n",
        "def vector_train(agent, env, total_timesteps):\n",
        "    total_rewards = [[] for _ in range(env.num_envs)]\n",
        "    avg_total_rewards = []\n",
        "\n",
        "    total_reward = np.zeros(env.num_envs)\n",
        "    observations = env.reset()\n",
        "    timestep = 0\n",
        "    episode = 0\n",
        "\n",
        "    t = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    while timestep < total_timesteps:\n",
        "        actions = agent.act(observations)\n",
        "        next_observations, rewards, dones, _ = env.step(actions)\n",
        "        agent.remember(observations, actions, rewards, next_observations, dones)\n",
        "        agent.train()\n",
        "        \n",
        "        timestep += len(observations)\n",
        "        t += 1\n",
        "\n",
        "        total_reward += rewards\n",
        "\n",
        "        for i in range(env.num_envs):\n",
        "            if dones[i]:\n",
        "                total_rewards[i].append((t, timestep, total_reward[i]))\n",
        "                episode += 1\n",
        "\n",
        "        if any(G for G in total_rewards):\n",
        "            episode_returns = sorted(\n",
        "                list(np.concatenate([G for G in total_rewards if G])),\n",
        "                key=lambda x: x[1]\n",
        "            )\n",
        "\n",
        "            avg_total_rewards.append(np.mean([G[-1] for G in episode_returns[-20:]]))\n",
        "\n",
        "        total_reward *= 1 - dones\n",
        "        observations = next_observations\n",
        "\n",
        "        ratio = math.ceil(100 * timestep / total_timesteps)\n",
        "        uptime = math.ceil(time.time() - start_time)\n",
        "\n",
        "        avg_return = avg_total_rewards[-1] if avg_total_rewards else np.nan\n",
        "\n",
        "        print(f\"\\r[{ratio:3d}% / {uptime:3d}s] timestep = {timestep}/{total_timesteps}, episode = {episode:3d}, avg_return = {avg_return:10.4f}\", end=\"\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    return avg_total_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9tdu0NF2bXd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5285a9d3-8c92-4340-9858-408851a77f8e"
      },
      "source": [
        "returns = vector_train(agent, env, 125000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[  1% /   1s] timestep = 8/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 16/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 24/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 32/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 40/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 48/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 56/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 64/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 72/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 80/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 88/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 96/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 104/125000, episode =   0, avg_return =        nan\r[  1% /   1s] timestep = 112/125000, episode =   1, avg_return =    14.0000\r[  1% /   1s] timestep = 120/125000, episode =   3, avg_return =    14.6667\r[  1% /   1s] timestep = 128/125000, episode =   3, avg_return =    14.6667\r[  1% /   1s] timestep = 136/125000, episode =   3, avg_return =    14.6667\r[  1% /   1s] timestep = 144/125000, episode =   5, avg_return =    16.0000\r[  1% /   1s] timestep = 152/125000, episode =   5, avg_return =    16.0000\r[  1% /   1s] timestep = 160/125000, episode =   5, avg_return =    16.0000\r[  1% /   1s] timestep = 168/125000, episode =   5, avg_return =    16.0000\r[  1% /   1s] timestep = 176/125000, episode =   5, avg_return =    16.0000\r[  1% /   1s] timestep = 184/125000, episode =   6, avg_return =    17.1667\r[  1% /   1s] timestep = 192/125000, episode =   6, avg_return =    17.1667\r[  1% /   1s] timestep = 200/125000, episode =   6, avg_return =    17.1667\r[  1% /   1s] timestep = 208/125000, episode =   6, avg_return =    17.1667\r[  1% /   1s] timestep = 216/125000, episode =   8, avg_return =    17.8750\r[  1% /   1s] timestep = 224/125000, episode =   8, avg_return =    17.8750\r[  1% /   1s] timestep = 232/125000, episode =   8, avg_return =    17.8750\r[  1% /   1s] timestep = 240/125000, episode =   8, avg_return =    17.8750\r[  1% /   1s] timestep = 248/125000, episode =   8, avg_return =    17.8750\r[  1% /   1s] timestep = 256/125000, episode =   8, avg_return =    17.8750\r[  1% /   1s] timestep = 264/125000, episode =   8, avg_return =    17.8750\r[  1% /   1s] timestep = 272/125000, episode =   8, avg_return =    17.8750\r[  1% /   1s] timestep = 280/125000, episode =   9, avg_return =    19.7778\r[  1% /   1s] timestep = 288/125000, episode =   9, avg_return =    19.7778\r[  1% /   1s] timestep = 296/125000, episode =   9, avg_return =    19.7778\r[  1% /   1s] timestep = 304/125000, episode =   9, avg_return =    19.7778\r[  1% /   1s] timestep = 312/125000, episode =   9, avg_return =    19.7778\r[  1% /   1s] timestep = 320/125000, episode =  11, avg_return =    20.4545\r[  1% /   1s] timestep = 328/125000, episode =  11, avg_return =    20.4545\r[  1% /   1s] timestep = 336/125000, episode =  11, avg_return =    20.4545\r[  1% /   1s] timestep = 344/125000, episode =  11, avg_return =    20.4545\r[  1% /   1s] timestep = 352/125000, episode =  11, avg_return =    20.4545\r[  1% /   1s] timestep = 360/125000, episode =  11, avg_return =    20.4545\r[  1% /   1s] timestep = 368/125000, episode =  11, avg_return =    20.4545\r[  1% /   1s] timestep = 376/125000, episode =  12, avg_return =    20.7500\r[  1% /   1s] timestep = 384/125000, episode =  12, avg_return =    20.7500\r[  1% /   1s] timestep = 392/125000, episode =  14, avg_return =    20.3571\r[  1% /   1s] timestep = 400/125000, episode =  14, avg_return =    20.3571\r[  1% /   1s] timestep = 408/125000, episode =  14, avg_return =    20.3571\r[  1% /   1s] timestep = 416/125000, episode =  14, avg_return =    20.3571\r[  1% /   1s] timestep = 424/125000, episode =  14, avg_return =    20.3571\r[  1% /   1s] timestep = 432/125000, episode =  14, avg_return =    20.3571\r[  1% /   1s] timestep = 440/125000, episode =  14, avg_return =    20.3571\r[  1% /   1s] timestep = 448/125000, episode =  14, avg_return =    20.3571\r[  1% /   1s] timestep = 456/125000, episode =  14, avg_return =    20.3571\r[  1% /   1s] timestep = 464/125000, episode =  14, avg_return =    20.3571\r[  1% /   1s] timestep = 472/125000, episode =  14, avg_return =    20.3571\r[  1% /   1s] timestep = 480/125000, episode =  15, avg_return =    19.7333\r[  1% /   1s] timestep = 488/125000, episode =  15, avg_return =    19.7333"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[100% /  72s] timestep = 125000/125000, episode = 840, avg_return =   500.0000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}