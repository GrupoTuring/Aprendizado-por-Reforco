{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdvU1bXObNVc"
   },
   "source": [
    "# Prioritized Experience Replay\n",
    "\n",
    "Baseado no paper [Prioritized Experience Replay, Schaul et al, 2015.](https://deepmind.com/research/publications/prioritized-experience-replay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QgJQrTIcyHB"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azD-UnD8bMq9"
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Class to help the deep q network to replay\n",
    "    the experiences it has had. It is implemented\n",
    "    with priority sampling.\"\"\"\n",
    "    def __init__(self, sdim, maxlen, alpha, offset):\n",
    "        self.device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._maxlen = maxlen\n",
    "        self._alpha  = alpha\n",
    "        self._offset = offset\n",
    "        self._memory = {\n",
    "            \"states\":   torch.zeros((maxlen,*sdim), dtype=torch.float64).to(self.device),\n",
    "            \"actions\":  torch.zeros(maxlen,         dtype=torch.int64  ).to(self.device),\n",
    "            \"rewards\":  torch.zeros(maxlen,         dtype=torch.float64).to(self.device),\n",
    "            \"nstates\":  torch.zeros((maxlen,*sdim), dtype=torch.float64).to(self.device),\n",
    "            \"dones\":    torch.zeros(maxlen,         dtype=torch.float64).to(self.device),\n",
    "            \"priority\": torch.zeros(maxlen,         dtype=torch.float64).to(self.device)\n",
    "        }\n",
    "        self._index = 0\n",
    "        self._len = 0\n",
    "\n",
    "    def _iindex(self):\n",
    "        \"\"\"Increases the index according to the\n",
    "        maximum lenght. If the index is greater \n",
    "        then the maximum lenght, it starts again.\"\"\"\n",
    "        self._index = (self._index+1) % self._maxlen\n",
    "\n",
    "    def push(self, s, a, r, s2, d):\n",
    "        if len(self) > 0: \n",
    "            mx = torch.max(self._memory[\"priority\"][:len(self)])\n",
    "        else:\n",
    "            mx = 1\n",
    "        if len(self) < self._maxlen:\n",
    "            self._len += 1\n",
    "        self._memory[\"states\"][self._index]  = torch.tensor(s, dtype=torch.float64)\n",
    "        self._memory[\"actions\"][self._index] = a\n",
    "        self._memory[\"rewards\"][self._index] = r\n",
    "        self._memory[\"nstates\"][self._index] = torch.tensor(s2, dtype=torch.float64)\n",
    "        self._memory[\"dones\"][self._index]   = d\n",
    "        self._memory[\"priority\"][self._index] = mx\n",
    "        self._iindex()\n",
    "\n",
    "    def sample(self, batch):\n",
    "        n = len(self)\n",
    "        indexes = random.choices(range(n),\n",
    "                                 weights = self._memory[\"priority\"].tolist()[:n],\n",
    "                                 k = batch)\n",
    "        to_return = (\n",
    "            self._memory[\"states\"  ][indexes],\n",
    "            self._memory[\"actions\" ][indexes],\n",
    "            self._memory[\"rewards\" ][indexes],\n",
    "            self._memory[\"nstates\" ][indexes],\n",
    "            self._memory[\"dones\"   ][indexes],\n",
    "            self._memory[\"priority\"][indexes]/self._memory[\"priority\"].sum(),\n",
    "            indexes)\n",
    "        return to_return\n",
    "\n",
    "    def update_priority(self, index, newp):\n",
    "        q = (newp.type(torch.float64) + self._offset)**self._alpha\n",
    "        self._memory[\"priority\"][index] = q\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HW1bSaxcV8B"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 build_net,\n",
    "                 hyperparameters,\n",
    "                 load = False, file_path = \"dqnsavep.pth\"):\n",
    "\n",
    "        self._alpha       = hyperparameters[\"a\"]\n",
    "        self._loss_param  = hyperparameters[\"b\"]\n",
    "        self._lp_increase = hyperparameters[\"beta_decay\"]\n",
    "        self._lr          = hyperparameters[\"lr\"]\n",
    "        self._batch       = hyperparameters[\"batch\"]\n",
    "        self._epsilon     = hyperparameters[\"e0\"]\n",
    "        self._epsilon_min = hyperparameters[\"ef\"]\n",
    "        self._gamma       = hyperparameters[\"gamma\"]\n",
    "        self._tau         = hyperparameters[\"tau\"]\n",
    "        self._steps       = 0\n",
    "        \n",
    "        self.device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._memory       = Memory(env.observation_space.shape, 100000, self._alpha, 0.0001)\n",
    "        self._action_space = env.action_space\n",
    "\n",
    "        self._decay = self._epsilon_min**(1/hyperparameters[\"decay\"])\n",
    "        self._file = file_path\n",
    "\n",
    "        self.dqn        = build_net().to(self.device)\n",
    "        self.target_dqn = build_net().to(self.device)\n",
    "        self._opt       = optim.Adam(self.dqn.parameters(), lr=self._lr)\n",
    "\n",
    "        if load:\n",
    "            self.dqn.load_state_dict(torch.load(load))\n",
    "            self.target_dqn.load_state_dict(torch.load(load))\n",
    "\n",
    "    def apply_decay(self):\n",
    "        self._epsilon = max(self._epsilon*self._decay, self._epsilon_min)\n",
    "\n",
    "    def __call__(self, state):\n",
    "        if random.random() < self._epsilon:\n",
    "            return 0, self._action_space.sample()\n",
    "        else:\n",
    "            state = torch.tensor(state).unsqueeze(0).to(self.device, dtype=torch.float64)\n",
    "            pred = self.dqn(state)[0]\n",
    "            value, index = pred.max(0)\n",
    "            return value.item(), index.item()\n",
    "\n",
    "    def save_on_memory(self, s, a, r, s2, d):\n",
    "        self._memory.push(s, a, r, s2, d)\n",
    "\n",
    "    def train(self):\n",
    "        self._steps += 1\n",
    "        if len(self._memory) < self._batch:\n",
    "            return -float(\"inf\")\n",
    "\n",
    "        s, a, r, s2, d, p, i = self._memory.sample(self._batch)\n",
    "\n",
    "        q_eval = self.dqn(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "        q_next = self.target_dqn(s2).max(1)[0].detach()\n",
    "        target = r + self._gamma*q_next*(1 - d)\n",
    "        N      = len(self._memory)\n",
    "        w      = (N * p)**(-self._loss_param)\n",
    "        w      = w/w.max()\n",
    "        self._loss_param *= 1 + self._lp_increase if self._loss_param < 1 else 1\n",
    "\n",
    "        loss = F.smooth_l1_loss(q_eval, target, reduction=\"none\")\n",
    "        self._memory.update_priority(i, torch.abs(loss))\n",
    "\n",
    "        weighted_loss = loss * w.detach()\n",
    "        final_loss    = torch.mean(weighted_loss)\n",
    "        self._opt.zero_grad()\n",
    "        final_loss.backward()\n",
    "        for param in self.dqn.parameters():\n",
    "            param.grad.data.clamp_(-100, 100)\n",
    "        self._opt.step()\n",
    "\n",
    "        for target_param, param in zip(self.target_dqn.parameters(), self.dqn.parameters()):\n",
    "            target_param.data.copy_(self._tau * param + (1 - self._tau) * target_param)\n",
    "\n",
    "        return final_loss\n",
    "\n",
    "    @property\n",
    "    def epsilon(self):\n",
    "        return self._epsilon\n",
    "\n",
    "    @property\n",
    "    def beta(self):\n",
    "        return self._loss_param\n",
    "\n",
    "    @property\n",
    "    def memory(self):\n",
    "        return self._memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6MBkzV-jn-f"
   },
   "outputs": [],
   "source": [
    "def build_net():\n",
    "    net = nn.Sequential(nn.Linear(4,32),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(32,32),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(32,2)).type(torch.float64)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "executionInfo": {
     "elapsed": 2131,
     "status": "error",
     "timestamp": 1598729790044,
     "user": {
      "displayName": "Fernando Kurike Matsumoto",
      "photoUrl": "",
      "userId": "13499161421829676514"
     },
     "user_tz": 180
    },
    "id": "2cT_BKrncchg",
    "outputId": "e01e0554-8c0f-4512-9eba-daeda97745e6"
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"a\":0.6,\n",
    "    \"b\":0.4,\n",
    "    \"beta_decay\": 2.5e-5,\n",
    "    \"lr\":1e-3,\n",
    "    \"gamma\":0.999,\n",
    "    \"e0\":1,\n",
    "    \"ef\":0.01,\n",
    "    \"decay\":150,\n",
    "    \"batch\":64,\n",
    "    \"tau\": 0.01,\n",
    "}\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = Agent(env, build_net, hparams)\n",
    "print(agent.dqn)\n",
    "print(hparams)\n",
    "\n",
    "episodes = 400\n",
    "history = []\n",
    "tts = 0\n",
    "for i in range(1, episodes + 1):\n",
    "    steps = 0\n",
    "    rewards = 0\n",
    "    total_loss = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        value, action = agent(state)\n",
    "        sstate, reward, done, _ = env.step(action)\n",
    "        agent.save_on_memory(state, action, reward, sstate, done)\n",
    "        loss = agent.train()\n",
    "        state = sstate\n",
    "        rewards += reward\n",
    "        steps += 1\n",
    "        total_loss += loss\n",
    "    # if i%100 == 0:\n",
    "    #     save(agent.dqn.state_dict(), f\"{str(i)}.pth\")\n",
    "    print(f\"episode [{i:04d}] - \"\n",
    "          f\"rewards [{rewards}] - \"\n",
    "          f\"duration [{steps:04d}] - \"\n",
    "          f\"epsilon [{100*agent.epsilon:.2f}%] - \"\n",
    "          f\"beta [{agent.beta:.2f}] - \"\n",
    "          f\"len [{len(agent.memory):06d}] - \"\n",
    "          f\"loss [{total_loss:.2f}]\")\n",
    "    history.append(rewards)\n",
    "    agent.apply_decay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtlBV1lVqsVw"
   },
   "outputs": [],
   "source": [
    "means = [0]*episodes\n",
    "high = [0]*episodes\n",
    "low  = [0]*episodes\n",
    "for i in range(episodes):\n",
    "    start = max(0, i - 20)\n",
    "    end = min(episodes, i + 21)\n",
    "    slice = history[ start : end ]\n",
    "    mean = sum(slice)/(end - start)\n",
    "    std = np.std(slice)\n",
    "    means[i] = mean\n",
    "    high[i] = mean + std\n",
    "    low[i] = mean - std\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "with plt.style.context(\"seaborn-pastel\"):\n",
    "    x = [k for k in range(episodes)]\n",
    "    plt.fill_between(x, low, high, alpha=0.2)\n",
    "    plt.plot(x, means, linewidth=4)\n",
    "    plt.plot(x, history)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Prioritized Experience Replay.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
