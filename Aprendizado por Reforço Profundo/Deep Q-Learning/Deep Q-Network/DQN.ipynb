{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWyEwLwn10hd"
   },
   "source": [
    "# Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui faremos a adaptação do algoritmo de Q-Learning com a utilização de redes neurais, assim como feito na implementação da DeepMind: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aconselhemos a leitura da nossa implementação de Q-Learning caso você não esteja tão acostumado com o algoritmo original e os conceitos básicos, pois isso facilitará seu entendimento de uma ```DQN``` (abreviação de Deep Q-Network). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por que \"deep\"? Ou, por que aprendizado \"profundo\"?\n",
    "\n",
    "Q-Learning por si só já é um algoritmo simples e relativamente eficiente para muitos ambientes, mas o que acontece se nosso ambiente for muito grande? E se precisarmos de uma tabela com milhares de estados e ações? \n",
    "\n",
    "\n",
    "Será que nosso agente consegue passar por todas as possibilidades de estados e ações para ter uma boa estimativa do Q-Learning? E mais ainda, será que nosso agente **precisa** passar por todos várias vezes?\n",
    "\n",
    "O uso de **redes neurais** nesse algoritmo veio para que consigamos aproximar Q-Valores a partir de outros estados e ações e assim não precisamos salvar o valor específico de cada configuração."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, basicamente, vamos sintetizar a diferença do Q-Learning para DQN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Imagens/Tabela-NN.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Criamos uma tabela-Q\n",
    "- Para cada configuração de ```Estado/Ação``` armazenamos a estimativa do nosso agente do ```Q-valor```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Criamos uma rede neural com ```N-estados``` neurônios de entrada e ```N-ações``` neurônios de saída\n",
    "- Colocamos camadas escondidas de acordo com nossa implementação\n",
    "- A rede recebe o estado e tenta predizer o ```Q-valor``` para cada ação disponível"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função *loss* será o ```erro quadrático médio``` entre Q-valor estimado pela rede e o valor real, entretando, por se tratar de um problema de aprendizado por reforço, **não temos o valor real!**. Assim, usamos outra estimativa para o valor real, o que chamamos de *bootstrap*. \n",
    "\n",
    "Assim temos:\n",
    "- Predição: $Q(S_t, A_t)$\n",
    "- Target: $max_a Q(S_{t+1}, a)$\n",
    "\n",
    "E nossa função fica, assim como já visto em Q-Learning:\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma max_a Q(S_{t+1}, a) - Q(S_t, A_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelo jeito que as redes neurais \"aprendem\", não é muito eficiente passar os dados coletados pelo agente enquanto ele explora o ambiente, é mais efetivo que nosso agente colete várias experiências e, após termos muitos dados, ir \"alimentando\" nossa rede. \n",
    "\n",
    "Esse tipo de aprendizado é chamado de ```off-policy```, porque ele não acontece ao mesmo tempo em que nosso agente explora o ambiente.\n",
    "\n",
    "Aqui nós utilizaremos o ```Experience Replay```, em que armazemos as experiências do nosso agente na forma $(s_t, a_t, r_t, s_{t+1})$, ou seja, o **estado** $s_t$ em que ele estava, a **ação** $a_t$ que ele tomou naquele estado, a **recompensa** $r_t$ que ele recebeu por tomar aquela ação naquele estado e o **próximo estado** $s_{t+1}$ que o agente foi após aquela ação.\n",
    "\n",
    "Após coletarmos um certo número dessas tuplas $(s_t, a_t, r_t, s_{t+1})$, nós passamos aleatoriamente esses dados para a rede aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zkpagykkFj3"
   },
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui faremos a implementação com ```pytorch```, além de usar algumas das bibliotecas mais comuns. O motivo de importarmos Pillow e torchvision é para a implementação da DQN com ConvNet, que está implementada mas não usaremos neste notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgCJLiwZkId-"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfdnGQ1CkKF2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo vamos importar a classe de Replay Buffer, muito importante para DQN's, que você pode ver a implementação no nosso repositório em Aprendizado por Reforço Profundo -> Deep Q-Learning -> Experience Replay -> ExperienceReplay.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A organização da pasta é a seguinte:\n",
    "```\n",
    "/\n",
    "+-- Deep Q-Network\n",
    "    +-- DQN.ipynb\n",
    "+-- Experience Replay\n",
    "    +-- ReplayBuffer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código necessário para importar funções de outro arquivo de um diretório diferente\n",
    "sys.path.insert(1, '../Experience Replay')\n",
    "from ReplayBuffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKyLGJFyoFjR"
   },
   "source": [
    "## Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui implementamos uma rede neural linear para servir como nossa DQN, ela é útil para ambientes em que você não está recebendo a imagem como estado do seu problema.\n",
    "\n",
    "O tamanho da entrada da rede, ou seja, o que nossa rede recebe de ```input``` são os estados disponíveis do ambiente, o ```shape```do estado, assim, é este valor que deverá ser passado em ```in_dim```.\n",
    "\n",
    "A saída da nossa rede é o ```Q-valor``` para cada ação disponível, assim, o tamanho de saída é o ```shape``` do estado de ações, que aqui está como ```out_dim```.\n",
    "\n",
    "O tamanho das camadas escondidas é arbitrário, pode ser variado de uma aplicação para outra e é um dos parâmetros que valem a pena ser modificados para garantir uma maior eficiência da sua rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4F49tlkvPfq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Cria uma rede neural para DQN\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        Inicializa a rede\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        in_dim: int\n",
    "        Dimensão de entrada da rede, ou seja, o shape do estado do ambiente\n",
    "        \n",
    "        out_dim: int\n",
    "        Número de ações do agente neste ambiente\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        super(LinearNetwork, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propaga uma entrada pela rede\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também temos implementado uma rede convolucional, para o caso em que seu estado sera uma imagem, mas não a usaremos neste notebook.\n",
    "\n",
    "O funcionamento é similar, a saída segue o mesmo padrão, mas na entrada deve ser inserido o tamanho da imagem do frame da sua aplicação, assim como garantir que o número de canais está certo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0hxjmX4oxLh8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, h, w, outputs):\n",
    "        \"\"\"\n",
    "        Cria uma rede convolucional \n",
    "        \n",
    "        in_channels: int\n",
    "        Shape do estado do ambiente\n",
    "        \n",
    "        h: int\n",
    "        A altura da imagem\n",
    "        \n",
    "        w: int\n",
    "        A largura da imagem\n",
    "        \n",
    "        outputs: int\n",
    "        Número de ações do agente neste ambiente\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            \"\"\"\n",
    "            Calcula o tamanho de saída da rede conv\n",
    "            \"\"\"\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Faz a forward propagation pela rede\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWrD9V1kvWth"
   },
   "source": [
    "## Agente DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como a maioria das aplicações famosas de aprendizado por reforço, uma boa prática é criar uma classe simples para ser seu agente. Aqui, nosso **agente DQN** terá alguns dos parâmetros clássicos de RL:\n",
    "- ```lr```: a taxa de aprendizado, ou seja, o quanto que levaremos em conta um novo cálculo na atualização do nosso Q-valor\n",
    "- ```gamma```: o fator de desconto, que diz o quanto consideramos recompensas futuras em relação a recompensas instantâneas\n",
    "- ```max_memory```: o número de tuplas que iremos armazenar no nosso replay buffer \n",
    "- ```epsilon_init```: o valor inicial do nosso $\\epsilon$, que diz o quanto nosso agente explora ou explota\n",
    "- ```epsilon_decay```: o quanto que iremos diminuir o $\\epsilon$\n",
    "- ```min_epsilon```: o menor valor de $\\epsilon$\n",
    "- ```network```: o tipo de rede que usaremos no nosso problema, ou seja, linear ou convolucional\n",
    "\n",
    "Os métodos implementados para nosso agente são:\n",
    "- ```act```: usado para escolher uma ação com base na política atual, em relação ao estado presente\n",
    "- ```remember```: salva no Replay Buffer uma tupla de experiência\n",
    "- ```train```: treina o agente\n",
    "- ```save_model```: salva um arquivo contendo os pesos da nossa DQN\n",
    "- ```load_model```: carrega os pesos existêntes de uma DQN no nosso agente\n",
    "\n",
    "Podemos notas que esses parâmetros e métodos são muito comuns em várias aplicações de RL, ou seja, assim que essa sequência for entendida, não fica muito difícil aplicar em outros algoritmos de RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMXF17a7oIqJ"
   },
   "outputs": [],
   "source": [
    "class DQNagent:\n",
    "    \"\"\"\n",
    "    Uma classe que cria um agente DQN que utiliza ReplayBuffer como memória\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space, \n",
    "                 lr=3e-4, \n",
    "                 gamma=0.99, \n",
    "                 max_memory=100000,\n",
    "                 epsilon_init=0.5,\n",
    "                 epsilon_decay=0.9995,\n",
    "                 min_epsilon=0.01,\n",
    "                 network='linear'):\n",
    "      \n",
    "        \"\"\"\n",
    "        Inicializa o agente com os parâmetros dados\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        \n",
    "        observation_space: gym.spaces\n",
    "        O espaço de observação do gym\n",
    "         \n",
    "        action_space: gym.spaces\n",
    "        O espaço de ações do agente modelado no gym\n",
    "        \n",
    "        lr: floar, default=3e-4\n",
    "        A taxa de aprendizado do agente\n",
    "        \n",
    "        gamma: float, default=0.99\n",
    "        O fator de desconto. Se perto de 1. as recompensas futuras terão grande importância,\n",
    "        se perto de 0. as recompensas mais instantâneas terão maior importância\n",
    "        \n",
    "        max_memory: int, default=100000\n",
    "        O número máximo de transições armazenadas no buffer de memória\n",
    "        \n",
    "        epsilon_init: float, default=0.5\n",
    "        O epsilon inicial do agente. Se próximo de 1. o agente tomará muitas ações\n",
    "        aleatórias, se proóximo de 0. o agente escolherá as ações com maior\n",
    "        Q-valor\n",
    "        \n",
    "        epsilon_decay: float, default=0.9995\n",
    "        A taxa de decaimento do epsilon do agente. A cada treinamento o agente tende\n",
    "        a escolher meno ações aleatórias se epsilon_decay<1\n",
    "        \n",
    "        min_epsilon: float, default=0.01\n",
    "        O menor epsilon possível\n",
    "        \n",
    "        \n",
    "        network: str, default='linear'\n",
    "        O tipo de rede a ser utilizada para o agente DQN. Por padrão é usada uma rede linear, mas\n",
    "        pode ser usada uma rede convolucional se o parâmetro for 'conv'\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "      \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.memory = ReplayBuffer(max_memory, observation_space.shape[0])\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.epsilon = epsilon_init\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "        if network == 'linear':\n",
    "            self.dqn = LinearNetwork(observation_space.shape[0], action_space.n).to(self.device)\n",
    "        \n",
    "        elif network == 'conv': \n",
    "            h = observation_space.shape[0]\n",
    "            w = observation_space.shape[1]\n",
    "            in_channels = observation_space.shape[2]\n",
    "            outputs = action_space.n\n",
    "            self.dqn = ConvNetwork(in_channels, h, w, outputs)\n",
    "\n",
    "        self.optimizer  = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Método para o agente escolher uma ação\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        \n",
    "        state\n",
    "        O estado do agente\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        \n",
    "        action\n",
    "        A ação escolhida pelo agente\n",
    "        \"\"\"\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "            return action\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            action = self.dqn.forward(state).argmax(dim=-1)\n",
    "            action = action.cpu().numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        \"\"\"\n",
    "        Método para armazenar uma sequência estado, ação, recompensa, próximo estado e done\n",
    "        no buffer de memória\n",
    "        \"\"\"\n",
    "        self.memory.update(state, action, reward, new_state, done)\n",
    "\n",
    "    def train(self, batch_size=128, epochs=1, save=True):\n",
    "        \"\"\"\n",
    "        Método para o agente treinar\n",
    "        \"\"\"\n",
    "        # Se temos menos experiências que o batch size\n",
    "        # não começamos o treinamento\n",
    "        if batch_size * 10 > self.memory.size:\n",
    "            return\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Pegamos uma amostra das nossas experiências para treinamento\n",
    "            (states, actions, rewards, next_states, dones) = self.memory.sample(batch_size)\n",
    "\n",
    "            # Transformar nossas experiências em tensores\n",
    "            states = torch.as_tensor(states).to(self.device)\n",
    "            actions = torch.as_tensor(actions).to(self.device).unsqueeze(-1)\n",
    "            rewards = torch.as_tensor(rewards).to(self.device).unsqueeze(-1)\n",
    "            next_states = torch.as_tensor(next_states).to(self.device)\n",
    "            dones = torch.as_tensor(dones).to(self.device).unsqueeze(-1)\n",
    "\n",
    "            q = self.dqn.forward(states).gather(-1, actions.long())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q2 = self.dqn.forward(next_states).max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "                target = (rewards + (1 - dones) * self.gamma * q2).to(self.device)\n",
    "\n",
    "            loss = F.mse_loss(q, target)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        if save:\n",
    "            self.save_model()\n",
    "            \n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        Salva os parâmetros da rede do agente\n",
    "        \"\"\"\n",
    "        file_name = \"Modelos salvos/DQN-\" + str(env.unwrapped.spec.id)\n",
    "        torch.save(self.dqn.state_dict(), file_name)\n",
    "            \n",
    "    def load_model(self, PATH):\n",
    "        \"\"\"\n",
    "        Carrega os parâmetros de uma rede salva, se possível\n",
    "        \"\"\"\n",
    "        self.dqn.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QS47MlN8pga6"
   },
   "source": [
    "### Definição de parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8vR8vZS1_A7"
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ARH8j14pfFT"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_INIT = 0.7\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.999\n",
    "MAX_MEMORY = 100000\n",
    "NETWORK = 'linear'\n",
    "OBS_SPACE = env.observation_space\n",
    "ACT_SPACE = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OBS_SPACE, ACT_SPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwOrceIGpm_N"
   },
   "source": [
    "### Criando a DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjfkvnvCpxLq"
   },
   "outputs": [],
   "source": [
    "dqn_net = DQNagent(observation_space=OBS_SPACE, \n",
    "                   action_space=ACT_SPACE, \n",
    "                   lr=3e-4, \n",
    "                   gamma=GAMMA, \n",
    "                   max_memory=MAX_MEMORY,\n",
    "                   epsilon_init=EPS_INIT,\n",
    "                   epsilon_decay=EPS_DECAY,\n",
    "                   min_epsilon=EPS_END,\n",
    "                   network=NETWORK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmYPxiROsQ9v"
   },
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BNgn8pnrrdR"
   },
   "outputs": [],
   "source": [
    "def train(agent, env, timesteps, render=False):\n",
    "    \"\"\"\n",
    "    Função para treinar um agente em um determinado ambiente em gym\n",
    "    \n",
    "    Parâmetros\n",
    "    ----------\n",
    "    agent\n",
    "    \n",
    "    env\n",
    "    \n",
    "    timesteps: int\n",
    "    \n",
    "    render: bool\n",
    "    \n",
    "    Retorna\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Rodar o número de episódios especificados\n",
    "    for timestep in range(1, timesteps+1):\n",
    "        \n",
    "        # Resetar o ambiente e armazenar o estado inicial\n",
    "        state = env.reset()\n",
    "        done = False        \n",
    "        \n",
    "        # Pegar a ação escolhida pelo agente de acordo com\n",
    "        # o estado atual\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Tomar a ação escolhida\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Guardar as informações geradas pela ação\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        # Treinar a rede com base no ReplayBuffer\n",
    "        agent.train(save=False)\n",
    "\n",
    "        # Atualiza o estado\n",
    "        state = next_state\n",
    "\n",
    "        if render:\n",
    "        # Mostra o ambiente\n",
    "            env.render()\n",
    "\n",
    "        if timestep % 100 == 0:\n",
    "            print(f\"Episódio: {timestep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episódio: 100\n",
      "Episódio: 200\n",
      "Episódio: 300\n",
      "Episódio: 400\n",
      "Episódio: 500\n"
     ]
    }
   ],
   "source": [
    "timesteps = 500\n",
    "train(dqn_net, env, timesteps, render=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
