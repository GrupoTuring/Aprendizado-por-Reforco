{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWyEwLwn10hd"
   },
   "source": [
    "# Implementação de uma Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zkpagykkFj3"
   },
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgCJLiwZkId-"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfdnGQ1CkKF2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo vamos importar a classe de Replay Buffer, muito importante para DQN's, que você pode ver a implementação no nosso repositório em Aprendizado por Reforço Profundo -> Deep Q-Learning -> Experience Replay -> ExperienceReplay.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código necessário para importar funções de outro arquivo de um diretório diferente\n",
    "sys.path.insert(1, '../Experience Replay')\n",
    "from ReplayBuffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKyLGJFyoFjR"
   },
   "source": [
    "## Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4F49tlkvPfq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Cria uma rede neural para DQN\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        Inicializa a rede\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        in_dim: int\n",
    "        Dimensão de entrada da rede, ou seja, o shape do estado do ambiente\n",
    "        \n",
    "        out_dim: int\n",
    "        Número de ações do agente neste ambiente\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        super(LinearNetwork, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propaga uma entrada pela rede\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0hxjmX4oxLh8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, h, w, outputs):\n",
    "        \"\"\"\n",
    "        Cria uma rede convolucional \n",
    "        \n",
    "        in_channels: int\n",
    "        Shape do estado do ambiente\n",
    "        \n",
    "        h: int\n",
    "        A altura da imagem\n",
    "        \n",
    "        w: int\n",
    "        A largura da imagem\n",
    "        \n",
    "        outputs: int\n",
    "        Número de ações do agente neste ambiente\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            \"\"\"\n",
    "            Calcula o tamanho de saída da rede conv\n",
    "            \"\"\"\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Faz a forward propagation pela rede\n",
    "        \"\"\"\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWrD9V1kvWth"
   },
   "source": [
    "## Agente DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMXF17a7oIqJ"
   },
   "outputs": [],
   "source": [
    "class DQNagent:\n",
    "    \"\"\"\n",
    "    Uma classe que cria um agente DQN que utiliza ReplayBuffer como memória\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space, \n",
    "                 lr=3e-4, \n",
    "                 gamma=0.99, \n",
    "                 max_memory=100000,\n",
    "                 epsilon_init=0.5,\n",
    "                 epsilon_decay=0.9995,\n",
    "                 min_epsilon=0.01,\n",
    "                 network='linear'):\n",
    "      \n",
    "        \"\"\"\n",
    "        Inicializa o agente com os parâmetros dados\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        \n",
    "        observation_space: gym.spaces\n",
    "        O espaço de observação do gym\n",
    "         \n",
    "        action_space: gym.spaces\n",
    "        O espaço de ações do agente modelado no gym\n",
    "        \n",
    "        lr: floar, default=3e-4\n",
    "        A taxa de aprendizado do agente\n",
    "        \n",
    "        gamma: float, default=0.99\n",
    "        O fator de desconto. Se perto de 1. as recompensas futuras terão grande importância,\n",
    "        se perto de 0. as recompensas mais instantâneas terão maior importância\n",
    "        \n",
    "        max_memory: int, default=100000\n",
    "        O número máximo de transições armazenadas no buffer de memória\n",
    "        \n",
    "        epsilon_init: float, default=0.5\n",
    "        O epsilon inicial do agente. Se próximo de 1. o agente tomará muitas ações\n",
    "        aleatórias, se proóximo de 0. o agente escolherá as ações com maior\n",
    "        Q-valor\n",
    "        \n",
    "        epsilon_decay: float, default=0.9995\n",
    "        A taxa de decaimento do epsilon do agente. A cada treinamento o agente tende\n",
    "        a escolher meno ações aleatórias se epsilon_decay<1\n",
    "        \n",
    "        min_epsilon: float, default=0.01\n",
    "        O menor epsilon possível\n",
    "        \n",
    "        \n",
    "        network: str, default='linear'\n",
    "        O tipo de rede a ser utilizada para o agente DQN. Por padrão é usada uma rede linear, mas\n",
    "        pode ser usada uma rede convolucional se o parâmetro for 'conv'\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "      \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.memory = ReplayBuffer(max_memory, observation_space.shape[0])\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.epsilon = epsilon_init\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "        if network == 'linear':\n",
    "            self.dqn = LinearNetwork(observation_space.shape[0], action_space.n).to(self.device)\n",
    "        \n",
    "        elif network == 'conv': \n",
    "            h = observation_space.shape[0]\n",
    "            w = observation_space.shape[1]\n",
    "            in_channels = observation_space.shape[2]\n",
    "            outputs = action_space.n\n",
    "            self.dqn = ConvNetwork(in_channels, h, w, outputs)\n",
    "\n",
    "        self.optimizer  = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Método para o agente escolher uma ação\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        \n",
    "        state\n",
    "        O estado do agente\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        \n",
    "        action\n",
    "        A ação escolhida pelo agente\n",
    "        \"\"\"\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "            return action\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            action = self.dqn.forward(state).argmax(dim=-1)\n",
    "            action = action.cpu().numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        \"\"\"\n",
    "        Método para armazenar uma sequência estado, ação, recompensa, próximo estado e done\n",
    "        no buffer de memória\n",
    "        \"\"\"\n",
    "        self.memory.update(state, action, reward, new_state, done)\n",
    "\n",
    "    def train(self, batch_size=128, epochs=1, save=True):\n",
    "        \"\"\"\n",
    "        Método para o agente treinar\n",
    "        \"\"\"\n",
    "        # Se temos menos experiências que o batch size\n",
    "        # não começamos o treinamento\n",
    "        if batch_size * 10 > self.memory.size:\n",
    "            return\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Pegamos uma amostra das nossas experiências para treinamento\n",
    "            (states, actions, rewards, next_states, dones) = self.memory.sample(batch_size)\n",
    "\n",
    "            # Transformar nossas experiências em tensores\n",
    "            states = torch.as_tensor(states).to(self.device)\n",
    "            actions = torch.as_tensor(actions).to(self.device).unsqueeze(-1)\n",
    "            rewards = torch.as_tensor(rewards).to(self.device).unsqueeze(-1)\n",
    "            next_states = torch.as_tensor(next_states).to(self.device)\n",
    "            dones = torch.as_tensor(dones).to(self.device).unsqueeze(-1)\n",
    "\n",
    "            q = self.dqn.forward(states).gather(-1, actions.long())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q2 = self.dqn.forward(next_states).max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "                target = (rewards + (1 - dones) * self.gamma * q2).to(self.device)\n",
    "\n",
    "            loss = F.mse_loss(q, target)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        if save:\n",
    "            self.save_model()\n",
    "            \n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        Salva os parâmetros da rede do agente\n",
    "        \"\"\"\n",
    "        file_name = \"Modelos salvos/DQN-\" + str(env.unwrapped.spec.id)\n",
    "        torch.save(self.dqn.state_dict(), file_name)\n",
    "            \n",
    "    def load_model(self, PATH):\n",
    "        \"\"\"\n",
    "        Carrega os parâmetros de uma rede salva, se possível\n",
    "        \"\"\"\n",
    "        self.dqn.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QS47MlN8pga6"
   },
   "source": [
    "### Definição de parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8vR8vZS1_A7"
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ARH8j14pfFT"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_INIT = 0.7\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.999\n",
    "MAX_MEMORY = 100000\n",
    "NETWORK = 'linear'\n",
    "OBS_SPACE = env.observation_space\n",
    "ACT_SPACE = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OBS_SPACE, ACT_SPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwOrceIGpm_N"
   },
   "source": [
    "### Criando a DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjfkvnvCpxLq"
   },
   "outputs": [],
   "source": [
    "dqn_net = DQNagent(observation_space=OBS_SPACE, \n",
    "                   action_space=ACT_SPACE, \n",
    "                   lr=3e-4, \n",
    "                   gamma=GAMMA, \n",
    "                   max_memory=MAX_MEMORY,\n",
    "                   epsilon_init=EPS_INIT,\n",
    "                   epsilon_decay=EPS_DECAY,\n",
    "                   min_epsilon=EPS_END,\n",
    "                   network=NETWORK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmYPxiROsQ9v"
   },
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BNgn8pnrrdR"
   },
   "outputs": [],
   "source": [
    "def train(agent, env, timesteps, render=False):\n",
    "    \"\"\"\n",
    "    Função para treinar um agente em um determinado ambiente em gym\n",
    "    \n",
    "    Parâmetros\n",
    "    ----------\n",
    "    agent\n",
    "    \n",
    "    env\n",
    "    \n",
    "    timesteps: int\n",
    "    \n",
    "    render: bool\n",
    "    \n",
    "    Retorna\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Rodar o número de episódios especificados\n",
    "    for timestep in range(1, timesteps+1):\n",
    "        \n",
    "        # Resetar o ambiente e armazenar o estado inicial\n",
    "        state = env.reset()\n",
    "        done = False        \n",
    "        \n",
    "        # Pegar a ação escolhida pelo agente de acordo com\n",
    "        # o estado atual\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Tomar a ação escolhida\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Guardar as informações geradas pela ação\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        # Treinar a rede com base no ReplayBuffer\n",
    "        agent.train(save=False)\n",
    "\n",
    "        # Atualiza o estado\n",
    "        state = next_state\n",
    "\n",
    "        if render:\n",
    "        # Mostra o ambiente\n",
    "            env.render()\n",
    "\n",
    "        if timestep % 100 == 0:\n",
    "            print(f\"Episódio: {timestep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episódio: 100\n",
      "Episódio: 200\n",
      "Episódio: 300\n",
      "Episódio: 400\n",
      "Episódio: 500\n"
     ]
    }
   ],
   "source": [
    "timesteps = 500\n",
    "train(dqn_net, env, timesteps, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
