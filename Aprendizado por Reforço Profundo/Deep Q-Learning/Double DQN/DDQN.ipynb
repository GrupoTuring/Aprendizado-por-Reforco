{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Double DQN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Relembrando alguns conceitos\n",
    "\n",
    "**Algoritmo de Q-Learning:** \n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha [R_{t+1} + \\gamma \\cdot  \\max_{a'} Q'(s', a') - Q(s,a)]\n",
    "$$\n",
    "\n",
    "Esse algoritmo, como sabemos, tem como objetivo atualizar as estimativas dos Q valores para os pares estado-ação do ambiente. O valor de Q é atualizado por meio de uma \"parcela de correção de erro\" (parcela de atualização), com uma taxa de aprendizado α.\n",
    "\n",
    "A atualização é composta pela recompensa imediata pela última ação tomada $R_{t+1}$, o fator de desconto $γ$ multiplicando o maior valor possível esperado de retorno naquele novo estado (após a última ação) menos o valor atual de $Q$.\n",
    "\n",
    "**DQN:**\n",
    "\n",
    "Para problemas em que o método tabular não dá conta, como visto no [notebook de DQN](../Deep%20Q-Network), utilizamos uma rede neural para estimar os valores Q. Porém, ao utilizar somente uma rede neural, os valores de Q tendem a ser superestimados, gerando treinamento menos eficiente e podendo levar a políticas sub-ótimas. Para tentar estabilizar o treinamento, utilizaremos então uma segunda rede neural, chegando no algoritmo DDQN.\n",
    "\n",
    "## Teoria\n",
    "Talvez você tenha notado que no algoritmo de Q-learning nós utilizamos uma estimativa do q-valor para fazer uma estimativa do nosso q-valor, as DDQN's surgiram com o objetivo de lidar com este problema.\n",
    "\n",
    "Nossa equação para o bootstrap é a seguinte:\n",
    "\n",
    "$$\n",
    "Q_{bootstrap}(s,a) = R_{t+1} + \\gamma \\cdot \\max_{a}Q(s',a)\n",
    "$$\n",
    "\n",
    "O Q target vira a soma da recompensa ao tomar a ação $a$ no estado $s$, mais o valor máximo de **Q** dentre todas as possíveis ações. Repare que, basicamente o que estamos fazendo é criar uma estimativa nova que depende dela mesma; que depende de uma estimativa anterior que está constantemente mudando:\n",
    "\n",
    "A função de custo (J) que usaremos para os pesos da rede é dado pela fórmula:\n",
    "\n",
    "$$\n",
    "J(w) = \\mathbf{E} [(q(s,a) - Q_w(s,a))^2]\n",
    "$$\n",
    "\n",
    "Em que **q(s,a)** é a função valor-ação real do nosso problema, e **Qw(s,a)** é o valor estimado a partir dos pesos da rede neural.\n",
    "\n",
    "Mas o problema enfrentado é que não temos o **q(s,a)**.\n",
    "\n",
    "O que fazemos então é aproximar nosso **q(s,a)**, obtendo também um **J(w)** aproximado:\n",
    "\n",
    "$$\n",
    "J(w)_{bootstrap} = \\mathbf{E} [(Q_{bootstrap}(s,a) - Q_w(s,a))^2]\n",
    "$$\n",
    "\n",
    "Mas o que pode acontecer a partir disso é que estaremos escolhendo ações que possuem o maior q-valor sem ter tanta certeza de que isso não é um falso positivo, de que não estamos obtendo um q-valor maior para ações não ótimas do que para ações ótimas.\n",
    "\n",
    "### Solução:\n",
    "\n",
    "Quando calcularemos o $Q_{bootstrap}$ nós usaremos duas redes idênticas para separar a escolha de melhor ação do cálculo do q-valor. \n",
    "\n",
    "  - Usamos uma rede DQN $Q_{local}$ para selecionar qual é a melhor ação a ser tomada no próximo estado. (Ação com maior q-valor).\n",
    "\n",
    "  -  Usamos uma rede DQN $Q_{target}$ para calcular o q-valor de tomar essa ação no próximo estado.\n",
    "\n",
    "Ou seja:\n",
    "\n",
    "  - Rede DQN para escolher melhor ação para o próximo estado:\n",
    "    $\\arg\\max_a(Q_{local}(s',a))$\n",
    "\n",
    "  - Rede $Q_{fixo}$ calculando $Q$ valor da escolha acima:\n",
    "    $Q_{target}(s', \\arg\\max_a Q_{local}(s',a))$\n",
    "\n",
    "E seguimos com nossa expressão do TD Target:\n",
    "$Q_{local}(s,a) = R_{t+1} + \\gamma \\cdot Q_{target}(s', \\arg\\max_a Q_{local}(s',a))$\n",
    "\n",
    "  Dessa maneira, com um $Q_{target}$ nós conseguimos \"fixar\" um valor para ser aproximado pelo $Q_{local}$, simplificando como a rede pode maximizar o q-valor com um viés menor e de maneira mais estável. \n",
    "\n",
    "Importante notar que faremos apenas o proceso de *backpropagation* na rede local. Para a rede alvejada nós copiaremos os parâmetros de uma para a outra, com um parâmetro $\\tau$ definindo quanto uma influênciará a outra."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importações"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import math\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "source": [
    "Utilizaremos o mesmo Experience Replay que utilizamos na DQN comum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '../Experience Replay')\n",
    "from ReplayBuffer import ReplayBuffer"
   ]
  },
  {
   "source": [
    "## Rede Neural"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Como você pode ver a Rede Neural é a mesma da DQN comum, a diferença é que iremos utilizá-la duas vezes. Aqui utilizaremos apenas a rede linear, se você se interessar, pode ser um exercício interessante adaptar esse código à rede convolucional."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"\n",
    "    Cria uma rede neural para DQN\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "source": [
    "## Agente"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Muitos dos parâmetros utilizados pelo Agente Double-DQN são análogos ao Agente DQN:\n",
    "- ```lr```: a taxa de aprendizado, ou seja, o quanto que levaremos em conta um novo cálculo na atualização do nosso Q-valor\n",
    "- ```gamma```: o fator de desconto, que diz o quanto consideramos recompensas futuras em relação a recompensas instantâneas\n",
    "- ```max_memory```: o número de tuplas que iremos armazenar no nosso replay buffer \n",
    "- ```epsilon_init```: o valor inicial do nosso $\\epsilon$, que diz o quanto nosso agente explora ou explota\n",
    "- ```epsilon_decay```: o quanto que iremos diminuir o $\\epsilon$\n",
    "- ```min_epsilon```: o menor valor de $\\epsilon$\n",
    "- ```network```: o tipo de rede que usaremos no nosso problema, ou seja, linear ou convolucional\n",
    "\n",
    "O parâmetro introduzido na DDQN é:\n",
    "- ```tau```: parâmetro mediador entre as duas redes neurais; o quanto a rede local irá influenciar a rede alvejada.\n",
    "\n",
    "Os métodos implementados para nosso agente são:\n",
    "- ```act```: usado para escolher uma ação com base na política atual, em relação ao estado presente\n",
    "- ```remember```: salva no Replay Buffer uma tupla de experiência\n",
    "- ```train```: treina o agente\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    \"\"\"\n",
    "    Uma classe que cria um agente com Double-DQN, com um experience replay padrão.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                obs_space,\n",
    "                action_space,\n",
    "                lr=3e-4,\n",
    "                gamma=0.99,\n",
    "                tau=0.001,\n",
    "                max_memory=100000,\n",
    "                batch_size=32,\n",
    "                eps_init=0.9,\n",
    "                eps_decay=0.9995,\n",
    "                eps_min=0.01):\n",
    "\n",
    "        \"\"\"\n",
    "        Inicializa o agente com os parâmetros dados\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        \n",
    "        observation_space: gym.spaces\n",
    "        O espaço de observação do gym\n",
    "         \n",
    "        action_space: gym.spaces\n",
    "        O espaço de ações do agente modelado no gym\n",
    "        \n",
    "        lr: floar, default=3e-4\n",
    "        A taxa de aprendizado do agente\n",
    "        \n",
    "        gamma: float, default=0.99\n",
    "        O fator de desconto. Se perto de 1. as recompensas futuras terão grande importância,\n",
    "        se perto de 0. as recompensas mais instantâneas terão maior importância\n",
    "        \n",
    "        tau: float, default=0.001\n",
    "        O quanto a rede local irá influenciar a rede alvejada.\n",
    "\n",
    "        max_memory: int, default=100000\n",
    "        O número máximo de transições armazenadas no buffer de memória\n",
    "        \n",
    "        epsilon_init: float, default=0.5\n",
    "        O epsilon inicial do agente. Se próximo de 1. o agente tomará muitas ações\n",
    "        aleatórias, se proóximo de 0. o agente escolherá as ações com maior\n",
    "        Q-valor\n",
    "        \n",
    "        epsilon_decay: float, default=0.9995\n",
    "        A taxa de decaimento do epsilon do agente. A cada treinamento o agente tende\n",
    "        a escolher meno ações aleatórias se epsilon_decay<1\n",
    "        \n",
    "        min_epsilon: float, default=0.01\n",
    "        O menor epsilon possível\n",
    "        \n",
    "        \n",
    "        network: str, default='linear'\n",
    "        O tipo de rede a ser utilizada para o agente DQN. Por padrão é usada uma rede linear, mas\n",
    "        pode ser usada uma rede convolucional se o parâmetro for 'conv'\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.memory = ReplayBuffer(max_memory, obs_space.shape[0])\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.eps = eps_init\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "\n",
    "        self.dqn = Network(obs_space.shape[0], action_space.n).to(self.device)\n",
    "        self.target_dqn = Network(obs_space.shape[0], action_space.n).to(self.device)\n",
    "        self.target_dqn.eval()\n",
    "\n",
    "        # Copia os parâmetros do modelo aos parâmetros do modelo alvo\n",
    "        for target_param, param in zip(self.dqn.parameters(),self.target_dqn.parameters()):\n",
    "            target_param.data.copy_(param)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        self.eps *= self.eps_decay\n",
    "        self.eps = max(self.eps, self.eps_min)\n",
    "\n",
    "        if np.random.random() < self.eps:\n",
    "            action = self.action_space.sample()\n",
    "            return action\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            action = self.dqn.forward(state).argmax(dim=-1)\n",
    "            action = action.cpu().numpy()\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.update(state, action, reward, next_state, done)\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        # Se temos menos experiências que o batch size\n",
    "        # não começamos o treinamento\n",
    "        if self.batch_size * 10 > self.memory.size:\n",
    "            return\n",
    "\n",
    "        # Pegamos uma amostra das nossas experiências para treinamento\n",
    "        (states, actions, rewards, next_states, dones) = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # Transformar nossas experiências em tensores\n",
    "        states = torch.as_tensor(states).to(self.device)\n",
    "        actions = torch.as_tensor(actions).to(self.device).unsqueeze(-1)\n",
    "        rewards = torch.as_tensor(rewards).to(self.device).unsqueeze(-1)\n",
    "        next_states = torch.as_tensor(next_states).to(self.device)\n",
    "        dones = torch.as_tensor(dones).to(self.device).unsqueeze(-1)\n",
    "\n",
    "        curr_q = self.dqn.forward(states).gather(-1, actions.long())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_dqn.forward(next_states).max(dim=-1,keepdim=True)[0]\n",
    "\n",
    "            target = (rewards + (1 - dones) * self.gamma * next_q).to(self.device)\n",
    "\n",
    "        loss = F.mse_loss(curr_q, target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Atualizar a rede alvejada\n",
    "        for target_param, param in zip(self.target_dqn.parameters(), self.dqn.parameters()):\n",
    "            target_param.data.copy_(self.tau * param + (1 - self.tau) * target_param)"
   ]
  },
  {
   "source": [
    "### Definação de parâmetros"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "TAU = 0.01\n",
    "EPS_INIT = 0.7\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.9995\n",
    "MAX_MEMORY = 100000\n",
    "OBS_SPACE = env.observation_space\n",
    "ACT_SPACE = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDQNAgent(obs_space = OBS_SPACE,\n",
    "                    action_space = ACT_SPACE,\n",
    "                    lr=3e-4,\n",
    "                    gamma=GAMMA,\n",
    "                    tau=TAU,\n",
    "                    max_memory=MAX_MEMORY,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    eps_init=EPS_INIT,\n",
    "                    eps_decay=EPS_DECAY,\n",
    "                    eps_min=EPS_END)"
   ]
  },
  {
   "source": [
    "## Treinando e Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, timesteps, render=False):\n",
    "    \n",
    "    total_reward = 0\n",
    "    episode_returns = deque(maxlen=20)\n",
    "    avg_returns = []\n",
    "    episode = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False \n",
    "    \n",
    "    for timestep in range(1, timesteps+1):       \n",
    "        \n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        agent.train()  \n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            episode_returns.append(total_reward)\n",
    "            episode += 1\n",
    "            next_state = env.reset()\n",
    "        \n",
    "        if episode_returns:\n",
    "            avg_returns.append(np.mean(episode_returns))\n",
    "            \n",
    "        total_reward *= 1 - done\n",
    "        ratio = math.ceil(100 * timestep / timesteps)\n",
    "        avg_return = avg_returns[-1] if avg_returns else np.nan\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        print(f\"\\r[{ratio:3d}%] timestep = {timestep}/{timesteps}, episode = {episode:3d}, avg_return = {avg_return:10.4f}\", end=\"\")   \n",
    "            \n",
    "    return avg_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[100%] timestep = 40000/40000, episode = 278, avg_return =   210.3500"
     ]
    }
   ],
   "source": [
    "timesteps = 40000\n",
    "train_returns = train(agent, env, timesteps, render=False)"
   ]
  },
  {
   "source": [
    "### Curva de aprendizado"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"250.155972pt\" version=\"1.1\" viewBox=\"0 0 376.097362 250.155972\" width=\"376.097362pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-11-15T14:49:53.490823</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 250.155972 \nL 376.097362 250.155972 \nL 376.097362 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 226.277847 \nL 368.0875 226.277847 \nL 368.0875 8.837847 \nL 33.2875 8.837847 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mfa97a65fcc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.505682\" xlink:href=\"#mfa97a65fcc\" y=\"226.277847\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(45.324432 240.876285)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"86.566361\" xlink:href=\"#mfa97a65fcc\" y=\"226.277847\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5000 -->\n      <g transform=\"translate(73.841361 240.876285)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"124.627039\" xlink:href=\"#mfa97a65fcc\" y=\"226.277847\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10000 -->\n      <g transform=\"translate(108.720789 240.876285)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"162.687718\" xlink:href=\"#mfa97a65fcc\" y=\"226.277847\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15000 -->\n      <g transform=\"translate(146.781468 240.876285)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"200.748397\" xlink:href=\"#mfa97a65fcc\" y=\"226.277847\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20000 -->\n      <g transform=\"translate(184.842147 240.876285)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.809076\" xlink:href=\"#mfa97a65fcc\" y=\"226.277847\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25000 -->\n      <g transform=\"translate(222.902826 240.876285)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"276.869755\" xlink:href=\"#mfa97a65fcc\" y=\"226.277847\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30000 -->\n      <g transform=\"translate(260.963505 240.876285)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"314.930434\" xlink:href=\"#mfa97a65fcc\" y=\"226.277847\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 35000 -->\n      <g transform=\"translate(299.024184 240.876285)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"352.991112\" xlink:href=\"#mfa97a65fcc\" y=\"226.277847\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 40000 -->\n      <g transform=\"translate(337.084862 240.876285)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m84b2807fd3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m84b2807fd3\" y=\"183.5638\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 187.363018)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m84b2807fd3\" y=\"140.422654\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 144.221873)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m84b2807fd3\" y=\"97.281509\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 150 -->\n      <g transform=\"translate(7.2 101.080728)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m84b2807fd3\" y=\"54.140364\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 57.939583)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m84b2807fd3\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 250 -->\n      <g transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p2d565a37f0)\" d=\"M 48.505682 212.899778 \nL 48.635088 212.899778 \nL 48.74927 212.036955 \nL 48.779719 212.036955 \nL 48.878676 211.46174 \nL 49.000471 213.072343 \nL 49.274508 213.072343 \nL 49.38869 209.736094 \nL 49.457199 209.736094 \nL 49.540932 209.201966 \nL 49.548544 210.203457 \nL 49.556157 210.203457 \nL 49.754072 210.203457 \nL 49.868254 209.352617 \nL 49.891091 209.352617 \nL 50.005273 209.534769 \nL 50.081394 209.534769 \nL 50.195576 209.134733 \nL 50.286922 209.134733 \nL 50.401104 208.657566 \nL 50.462001 208.657566 \nL 50.576183 208.519293 \nL 50.591407 208.519293 \nL 50.705589 208.770554 \nL 50.804547 208.770554 \nL 50.911117 208.355578 \nL 51.032911 209.296224 \nL 51.215602 209.296224 \nL 51.322172 208.825337 \nL 51.436354 209.130605 \nL 51.641882 209.130605 \nL 51.733227 208.197393 \nL 51.756064 208.369958 \nL 51.931143 208.369958 \nL 52.045325 208.844511 \nL 52.060549 208.844511 \nL 52.136671 208.671946 \nL 52.167119 208.801369 \nL 52.24324 208.801369 \nL 52.365035 210.354451 \nL 52.448768 210.354451 \nL 52.540114 210.225027 \nL 52.66952 212.080096 \nL 52.768478 212.080096 \nL 52.890272 213.201766 \nL 53.034903 213.288049 \nL 53.149085 214.064589 \nL 53.164309 214.064589 \nL 53.232818 213.935166 \nL 53.278491 213.978307 \nL 53.316552 213.978307 \nL 53.430734 215.013694 \nL 53.484019 215.013694 \nL 53.598201 216.307929 \nL 53.788504 216.35107 \nL 53.895074 216.178505 \nL 54.001644 216.307929 \nL 54.085377 216.135364 \nL 54.108214 216.264788 \nL 54.268069 216.221646 \nL 54.382251 216.394211 \nL 54.443148 216.394211 \nL 54.55733 216.221646 \nL 54.740021 216.307929 \nL 54.838979 215.9628 \nL 54.854203 216.049082 \nL 54.998834 216.049082 \nL 55.113016 216.307929 \nL 55.227198 216.005941 \nL 55.744823 216.005941 \nL 55.859005 215.099977 \nL 55.973187 215.099977 \nL 56.087369 214.323436 \nL 56.254836 214.323436 \nL 56.369018 213.33119 \nL 56.384242 213.33119 \nL 56.498424 213.201766 \nL 56.521261 213.201766 \nL 56.635443 212.899778 \nL 56.703952 212.899778 \nL 56.818134 212.425226 \nL 56.977989 212.425226 \nL 57.092171 211.864391 \nL 57.274862 211.864391 \nL 57.389044 210.656439 \nL 57.815324 210.656439 \nL 57.929506 207.981688 \nL 57.94473 207.981688 \nL 58.058912 207.765982 \nL 58.203543 207.765982 \nL 58.317725 207.075724 \nL 58.485192 207.075724 \nL 58.599374 206.040336 \nL 58.652659 206.040336 \nL 58.766841 205.522642 \nL 58.858186 205.522642 \nL 58.972368 204.832384 \nL 59.185508 204.832384 \nL 59.29969 203.624432 \nL 59.451933 203.624432 \nL 59.566115 202.632186 \nL 59.984783 202.632186 \nL 60.098965 200.043717 \nL 60.388226 200.043717 \nL 60.502408 198.361212 \nL 60.517632 198.361212 \nL 60.631814 198.016083 \nL 60.72316 198.016083 \nL 60.837342 198.490636 \nL 61.134215 198.490636 \nL 61.248397 197.455248 \nL 61.530046 197.455248 \nL 61.644228 196.808131 \nL 62.146629 196.808131 \nL 62.260811 194.047098 \nL 62.458727 194.047098 \nL 62.572909 193.054851 \nL 62.824109 193.054851 \nL 62.938291 192.019464 \nL 63.098146 192.019464 \nL 63.212328 190.940935 \nL 63.699505 190.940935 \nL 63.813687 188.611314 \nL 64.270415 188.611314 \nL 64.384597 187.058232 \nL 64.491167 187.058232 \nL 64.605349 188.87016 \nL 64.909834 188.87016 \nL 65.024016 187.230797 \nL 65.473132 187.230797 \nL 65.587314 185.505151 \nL 65.983146 185.505151 \nL 66.097328 184.210917 \nL 66.538831 184.210917 \nL 66.653013 182.010718 \nL 67.132578 182.010718 \nL 67.24676 179.81052 \nL 67.794834 179.81052 \nL 67.909016 177.91231 \nL 68.951878 177.91231 \nL 69.066061 172.864796 \nL 70.512366 172.864796 \nL 70.626548 167.040741 \nL 71.326865 167.040741 \nL 71.441047 164.711119 \nL 72.392564 164.711119 \nL 72.506746 159.404758 \nL 73.549608 159.404758 \nL 73.663791 154.012115 \nL 74.935017 154.012115 \nL 75.049199 148.490048 \nL 76.152959 148.490048 \nL 76.267141 143.830805 \nL 77.218658 143.830805 \nL 77.33284 141.285477 \nL 78.307193 141.285477 \nL 78.421375 136.88508 \nL 79.776336 136.88508 \nL 79.890518 130.629614 \nL 82.326401 130.629614 \nL 82.440583 117.730412 \nL 83.445385 117.730412 \nL 83.559567 114.796814 \nL 84.686163 114.796814 \nL 84.800345 111.000393 \nL 85.759474 111.000393 \nL 85.873656 106.168585 \nL 87.045925 106.168585 \nL 87.160107 101.250494 \nL 88.789104 101.250494 \nL 88.903286 94.563617 \nL 91.704552 94.563617 \nL 91.818734 80.931015 \nL 93.394446 80.931015 \nL 93.508628 74.502984 \nL 95.137626 74.502984 \nL 95.251808 67.988672 \nL 96.568707 67.988672 \nL 96.682889 63.631416 \nL 98.159643 63.631416 \nL 98.273825 61.172371 \nL 100.740157 61.172371 \nL 100.854339 55.391457 \nL 103.899194 55.391457 \nL 104.013376 42.103984 \nL 105.802228 42.103984 \nL 105.91641 37.358458 \nL 107.865117 37.358458 \nL 107.979299 32.224662 \nL 109.425604 32.224662 \nL 109.539786 31.232416 \nL 110.932807 31.232416 \nL 111.046989 29.593052 \nL 112.493295 29.593052 \nL 112.607477 26.788878 \nL 113.79497 26.788878 \nL 113.909152 25.580926 \nL 115.477252 25.580926 \nL 115.591434 24.372974 \nL 116.877885 24.372974 \nL 116.992067 30.887287 \nL 118.780919 30.887287 \nL 118.895101 26.443749 \nL 120.059758 26.443749 \nL 120.17394 26.228043 \nL 122.06175 26.228043 \nL 122.175932 20.964823 \nL 123.744032 20.964823 \nL 123.858214 18.721484 \nL 125.144665 18.721484 \nL 125.258847 20.662835 \nL 127.177105 20.662835 \nL 127.291287 25.667208 \nL 128.532065 25.667208 \nL 128.646247 27.565419 \nL 129.673885 27.565419 \nL 129.788067 30.973569 \nL 131.371392 30.973569 \nL 131.485574 29.463629 \nL 132.680679 29.463629 \nL 132.794861 31.059851 \nL 134.165046 31.059851 \nL 134.279228 37.272176 \nL 135.588515 37.272176 \nL 135.702697 47.108357 \nL 137.088106 47.108357 \nL 137.202288 49.394838 \nL 138.679042 49.394838 \nL 138.793224 52.069589 \nL 140.110124 52.069589 \nL 140.224306 52.802988 \nL 141.541205 52.802988 \nL 141.655387 53.2344 \nL 143.139754 53.2344 \nL 143.253936 53.018694 \nL 145.096073 53.018694 \nL 145.210255 49.308556 \nL 146.54999 49.308556 \nL 146.664172 50.60279 \nL 148.247497 50.60279 \nL 148.361679 48.920285 \nL 149.670966 48.920285 \nL 149.785148 51.638178 \nL 151.376085 51.638178 \nL 151.490267 49.222273 \nL 153.842417 49.222273 \nL 153.956599 46.590664 \nL 155.220213 46.590664 \nL 155.334395 48.316309 \nL 156.537113 48.316309 \nL 156.651295 48.790862 \nL 157.998643 48.790862 \nL 158.112825 52.026448 \nL 159.673313 52.026448 \nL 159.787495 50.21452 \nL 161.188128 50.21452 \nL 161.30231 48.100604 \nL 162.65727 48.100604 \nL 162.771452 49.394838 \nL 164.179697 49.394838 \nL 164.293879 48.186886 \nL 166.394828 48.186886 \nL 166.50901 44.045336 \nL 167.711728 44.045336 \nL 167.82591 44.649312 \nL 169.043852 44.649312 \nL 169.158034 45.598417 \nL 170.467321 45.598417 \nL 170.581503 46.547522 \nL 172.020197 46.547522 \nL 172.134379 45.857264 \nL 173.5274 45.857264 \nL 173.641582 45.425853 \nL 176.305829 45.425853 \nL 176.420011 38.738975 \nL 178.58947 38.738975 \nL 178.703652 36.883906 \nL 179.784575 36.883906 \nL 179.898757 38.350705 \nL 181.093863 38.350705 \nL 181.208045 40.550903 \nL 183.111079 40.550903 \nL 183.225261 37.185894 \nL 184.466039 37.185894 \nL 184.580221 39.170387 \nL 186.133096 39.170387 \nL 186.247278 43.700207 \nL 188.995259 43.613925 \nL 189.109442 42.751102 \nL 190.319771 42.751102 \nL 190.433953 43.527642 \nL 192.565351 43.527642 \nL 192.679533 40.292056 \nL 194.780483 40.292056 \nL 194.894665 36.323071 \nL 198.320126 36.366212 \nL 198.434308 33.216909 \nL 200.177487 33.216909 \nL 200.291669 35.244542 \nL 201.456326 35.244542 \nL 201.570508 35.460248 \nL 205.551655 35.54653 \nL 205.665837 36.6682 \nL 206.982736 36.6682 \nL 207.096918 37.099612 \nL 208.337696 37.099612 \nL 208.451878 45.167006 \nL 210.080876 45.167006 \nL 210.195058 48.230027 \nL 211.869727 48.230027 \nL 211.983909 44.865018 \nL 213.201851 44.865018 \nL 213.316033 44.735594 \nL 214.389344 44.735594 \nL 214.503526 49.437979 \nL 215.797589 49.437979 \nL 215.911772 49.135991 \nL 217.144938 49.135991 \nL 217.25912 50.947919 \nL 218.735874 50.947919 \nL 218.850056 49.826249 \nL 221.278327 49.826249 \nL 221.392509 43.743348 \nL 222.808367 43.743348 \nL 222.922549 42.578537 \nL 224.14049 42.578537 \nL 224.254672 47.755474 \nL 225.685754 47.755474 \nL 225.799936 51.551895 \nL 227.413709 51.551895 \nL 227.527891 50.041955 \nL 229.567943 50.041955 \nL 229.682125 49.610544 \nL 230.808721 49.610544 \nL 230.922903 53.104976 \nL 232.255027 53.104976 \nL 232.369209 52.155871 \nL 233.518642 52.155871 \nL 233.632824 52.457859 \nL 234.881214 52.457859 \nL 234.995396 52.802988 \nL 236.32752 52.802988 \nL 236.441702 52.285295 \nL 239.166846 52.242154 \nL 239.281028 51.983307 \nL 240.60554 51.983307 \nL 240.719722 53.708952 \nL 242.341107 53.708952 \nL 242.455289 54.010941 \nL 243.772188 54.010941 \nL 243.88637 53.450106 \nL 245.058639 53.450106 \nL 245.172821 52.889271 \nL 246.299417 52.889271 \nL 246.4136 53.838376 \nL 248.933216 53.881517 \nL 249.047399 55.564022 \nL 250.607886 55.564022 \nL 250.722068 60.482112 \nL 252.960036 60.482112 \nL 253.074218 55.822869 \nL 255.18278 55.822869 \nL 255.296962 50.775355 \nL 257.519706 50.775355 \nL 257.633888 46.288676 \nL 258.790932 46.288676 \nL 258.905114 48.877144 \nL 260.260074 48.877144 \nL 260.374257 52.759847 \nL 261.622647 52.759847 \nL 261.736829 52.069589 \nL 263.304929 52.069589 \nL 263.419111 50.732213 \nL 264.713174 50.732213 \nL 264.827356 49.912532 \nL 265.885443 49.912532 \nL 265.999625 50.99106 \nL 267.19473 50.99106 \nL 267.308912 51.767601 \nL 268.51163 51.767601 \nL 268.625812 52.457859 \nL 269.721959 52.457859 \nL 269.836141 53.536388 \nL 271.206326 53.536388 \nL 271.320508 53.277541 \nL 272.782038 53.277541 \nL 272.89622 54.183505 \nL 274.578502 54.183505 \nL 274.692684 52.11273 \nL 275.834504 52.11273 \nL 275.948686 52.285295 \nL 277.973514 52.285295 \nL 278.087696 47.19464 \nL 280.637762 47.280922 \nL 280.751944 47.022075 \nL 281.924213 47.022075 \nL 282.038395 49.222273 \nL 283.385743 49.222273 \nL 283.499925 54.269787 \nL 285.243104 54.269787 \nL 285.357286 56.340562 \nL 287.267932 56.340562 \nL 287.382114 58.109349 \nL 289.285148 58.109349 \nL 289.39933 53.881517 \nL 290.632496 53.881517 \nL 290.746678 54.571775 \nL 292.710609 54.571775 \nL 292.824791 50.516508 \nL 294.004672 50.516508 \nL 294.118854 52.716706 \nL 295.557548 52.716706 \nL 295.67173 51.897024 \nL 297.118036 51.897024 \nL 297.232218 49.696826 \nL 298.34359 49.696826 \nL 298.457772 50.171379 \nL 299.789896 50.171379 \nL 299.904078 49.437979 \nL 302.3019 49.437979 \nL 302.416082 42.060843 \nL 305.422876 41.974561 \nL 305.537058 41.715714 \nL 306.64843 41.715714 \nL 306.762612 44.9513 \nL 308.186081 44.9513 \nL 308.300263 43.355078 \nL 309.472532 43.355078 \nL 309.586714 48.186886 \nL 311.170038 48.186886 \nL 311.284221 46.07297 \nL 312.403204 46.07297 \nL 312.517387 46.676946 \nL 313.948468 46.676946 \nL 314.06265 45.210147 \nL 315.371937 45.210147 \nL 315.486119 45.425853 \nL 316.871528 45.425853 \nL 316.98571 47.453486 \nL 318.097082 47.453486 \nL 318.211264 51.983307 \nL 319.269351 51.983307 \nL 319.383533 56.771974 \nL 321.880313 56.815115 \nL 321.994496 61.388076 \nL 323.105867 61.388076 \nL 323.220049 61.776347 \nL 324.407543 61.776347 \nL 324.521725 63.200004 \nL 325.990867 63.200004 \nL 326.105049 63.070581 \nL 327.543742 63.070581 \nL 327.657925 61.215512 \nL 329.401104 61.215512 \nL 329.515286 58.88589 \nL 330.611433 58.88589 \nL 330.725615 66.263026 \nL 333.846591 66.306167 \nL 333.960773 65.615909 \nL 335.437527 65.615909 \nL 335.551709 63.545134 \nL 337.888635 63.545134 \nL 338.002817 58.368196 \nL 339.411062 58.368196 \nL 339.525244 57.030821 \nL 340.804083 57.030821 \nL 340.918265 58.756466 \nL 342.570098 58.756466 \nL 342.684281 55.736586 \nL 344.67866 55.736586 \nL 344.792842 52.544142 \nL 346.330494 52.544142 \nL 346.444676 51.249907 \nL 348.149794 51.249907 \nL 348.263976 49.437979 \nL 349.824464 49.437979 \nL 349.938646 46.892652 \nL 351.05763 46.892652 \nL 351.171812 46.547522 \nL 352.633342 46.547522 \nL 352.747524 45.210147 \nL 352.869318 45.210147 \nL 352.869318 45.210147 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 226.277847 \nL 33.2875 8.837847 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 226.277847 \nL 368.0875 8.837847 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 226.277847 \nL 368.0875 226.277847 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 8.837847 \nL 368.0875 8.837847 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p2d565a37f0\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"8.837847\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(train_returns, 'r')\n",
    "plt.show()"
   ]
  }
 ]
}
