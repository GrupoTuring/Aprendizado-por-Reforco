{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ReplayBuffer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xtKzzOaYrN7z","colab_type":"text"},"source":["# üîÅ Experience Replay\n","\n","Uma grande desvantagem das redes neurais √© a necessidade de treinar com uma grande quantidade de dados para obter um bom aprendizado. Isso torna seu uso em algoritmos \"online\" como os de Temporal Difference bem dif√≠cil, j√° que ela recebe apenas uma transi√ß√£o a cada instante de tempo para o treinamento.\n","\n","Entretanto, como Q-Learning √© um algoritmo off-policy, n√≥s podemos aproveitar as experi√™ncias anteriores do nosso agente para utilizar em um batch no treinamento da nossa rede. √â dessa ideia que surge o conceito do **Experience Replay**, um buffer para guardar todas as experi√™ncias passadas do nosso agente.\n","\n","Para entender como isso funciona, vamos relembrar da atualiza√ß√£o do Q-Learning:\n","\n","$$Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma \\max_{a}Q(S', a) - Q(S, A)]$$\n","\n","Observe que para atualizar o valor *Q* de um par estado-a√ß√£o, precisamos saber apenas o estado *S*, a a√ß√£o tomada *A*, a recompensa recebida *R* e o estado seguinte *S'*. Como esse update n√£o depende da pol√≠tica no momento da escolha da a√ß√£o, podemos usar uma experi√™ncia $(s_t, a_t, r_t, s_{t+1})$ para treinamento a qualquer momento.\n","\n","Dessa forma, o que podemos fazer √© guardar esses pares $(s_t, a_t, r_t, s_{t+1})$ em um buffer, e amostrar uma batch dessas experi√™ncias passadas para cada treinamento. Assim, conseguimos reaproveitar as experi√™ncias obtidas pelo nosso agente e aumentar a *sample efficiency* do nosso algoritmo, ou seja, sua efici√™ncia dado uma quantidade limitada de experi√™ncias.\n","\n","A seguir, segue uma implementa√ß√£o desse Buffer de experi√™ncias:"]},{"cell_type":"code","metadata":{"id":"0pzu8HKRjX6l","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","class ReplayBuffer:\n","    \"\"\"Experience Replay Buffer para DQNs.\"\"\"\n","    def __init__(self, max_length, observation_space):\n","        \"\"\"Cria um Replay Buffer.\n","\n","        Par√¢metros\n","        ----------\n","        max_length: int\n","            Tamanho m√°ximo do Replay Buffer.\n","        observation_space: int\n","            Tamanho do espa√ßo de observa√ß√£o.\n","        \"\"\"\n","        self.index, self.size, self.max_length = 0, 0, max_length\n","\n","        self.states = np.zeros((max_length, observation_space), dtype=np.float32)\n","        self.actions = np.zeros((max_length), dtype=np.int32)\n","        self.rewards = np.zeros((max_length), dtype=np.float32)\n","        self.next_states = np.zeros((max_length, observation_space), dtype=np.float32)\n","        self.dones = np.zeros((max_length), dtype=np.float32)\n","\n","    def __len__(self):\n","        \"\"\"Retorna o tamanho do buffer.\"\"\"\n","        return self.size\n","\n","    def update(self, state, action, reward, next_state, done):\n","        \"\"\"Adiciona uma experi√™ncia ao Replay Buffer.\n","\n","        Par√¢metros\n","        ----------\n","        state: np.array\n","            Estado da transi√ß√£o.\n","        action: int\n","            A√ß√£o tomada.\n","        reward: float\n","            Recompensa recebida.\n","        state: np.array\n","            Estado seguinte.\n","        done: int\n","            Flag indicando se o epis√≥dio acabou.\n","        \"\"\"\n","        self.states[self.index] = state\n","        self.actions[self.index] = action\n","        self.rewards[self.index] = reward\n","        self.next_states[self.index] = next_state\n","        self.dones[self.index] = done\n","        \n","        self.index = (self.index + 1) % self.max_length\n","        if self.size < self.max_length:\n","            self.size += 1\n","            \n","    def sample(self, batch_size):\n","        \"\"\"Retorna um batch de experi√™ncias.\n","        \n","        Par√¢metros\n","        ----------\n","        batch_size: int\n","            Tamanho do batch de experi√™ncias.\n","\n","        Retorna\n","        -------\n","        states: np.array\n","            Batch de estados.\n","        actions: np.array\n","            Batch de a√ß√µes.\n","        rewards: np.array\n","            Batch de recompensas.\n","        next_states: np.array\n","            Batch de estados seguintes.\n","        dones: np.array\n","            Batch de flags indicando se o epis√≥dio acabou.\n","        \"\"\"\n","        # Escolhe √≠ndices aleatoriamente do Replay Buffer\n","        idxs = np.random.randint(0, self.size, size=batch_size)\n","\n","        return (self.states[idxs], self.actions[idxs], self.rewards[idxs], self.next_states[idxs], self.dones[idxs])"],"execution_count":null,"outputs":[]}]}