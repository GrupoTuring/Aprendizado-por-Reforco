{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWyEwLwn10hd"
   },
   "source": [
    "# N-Step DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-R_VyNqyAoq"
   },
   "source": [
    "# Conceito\n",
    "Em algoritmos de Monte Carlo, o nosso modelo ¨aprende¨ em cada transição com base em **toda** a sequência de recompensas em um episódio, ou seja, o retorno completo ($G_{t}$). Já em one-step Temporal Diference, o aprendizado é feito observando apenas uma recompensa no futuro ($R_{t + 1}$) e aproximamos o restante do retorno ($G_{t+1}$) como sendo o valor do próximo estado ($V(S_{t + 1})$).\n",
    "\n",
    "\n",
    "Agora, em N-step, tomamos uma abordagem intermediária a esses dois algoritmos. Não chegamos a utilizar a totalidade do retorno ($G_{t}$), mas **n** passos a frente do presente (**t**). Dessa forma, obtemos a seguinte expressão:\n",
    "\n",
    "$$ G_t = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{T -t -1} R_{T} $$\n",
    "\n",
    "\n",
    "$$ G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n V_{t +n -1}(S_{t + n}) $$\n",
    "\n",
    "\n",
    "\n",
    "onde $G_t$ representa o retorno completo (usado em Monte Carlo) e $G_{t:t + n}$ a aproximação do retorno com n-step, utilizando bootstraping do Valor do estado no instante t + n ($V_{t +n -1}(S_{t + n})$).\n",
    "\n",
    "![N-Step](https://media.discordapp.net/attachments/688564171973197869/752614671974006844/unknown.png)\n",
    "\n",
    "Vale notar que, no início do episódio, o agente não possui todas as experiências necessárias para fazer a estimativa do retorno. Para contornar isso, fazemos mudanças no replay buffer (explicadas mais a frente) para possibilitar o cálculo.\n",
    "\n",
    "## Sarsa para $n$-Step\n",
    "Agora que temos uma noção de como o $n$-step funciona, podemos nos preocupar em como nosso agente pode fazer uma escolha se baseando nesse processo. Para isso, mudamos nossa previsão para que ela preveja ações e não estados. Com esse objetivo, chegamos às seguintes expressões:\n",
    "\n",
    "$$G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n Q_{t +n -1}(S_{t + n}, A_{t + n}) $$\n",
    "\n",
    "$$Q_{t +n}(S_{t}, A_{t}) = Q_{t +n -1}(S_{t}, A_{t}) + \\alpha [G_{t:t + n} - Q_{t +n -1}(S_{t}, A_{t})]$$\n",
    "\n",
    "\n",
    "Tendo esse algoritmo, só precisamos de uma política $\\pi$, por exemplo, $\\varepsilon$-greedy.\n",
    "\n",
    "## *Off-policy* $n$-step\n",
    "Para fazermos uma implementação tecnicamente ¨completa¨ de $n$-step *off-policy*, seria necessário implementar a funcionalidade de *importance sampling* no $n$-step buffer. Isso necessariamente requer que avaliemos a *importance sampling ratio*($\\rho_{t:t+n-1}$) para cada passo a frente ($n$) que queremos avaliar.\n",
    "\n",
    "No entanto, com base nos artigos:\n",
    "> \"Rainbow: Combining Improvements in Deep Reinforcement Learning\": https://arxiv.org/pdf/1710.02298.pdf\n",
    "\n",
    "> \"Understanding Multi-Step Deep Reinforcement\n",
    "Learning: A Systematic Study of the DQN Target\": https://arxiv.org/pdf/1901.07510.pdf\n",
    "\n",
    "decidimos por não integrar *importance sampling* ao modelo pois, para valores de $n$ pequenos comumente como $n=3$, não aparentam existir impactos significativos da falta desse recurso. Além disso, para valores de $n$ maiores, o treinamento seria mais demorado, sem benefícios claros.\n",
    "\n",
    "Logo, a única alteração feita para off-policy em relação a on-policy $n$-step, é utilizar o $max_{a'} Q(S_{t+n}, a')$ ao invés do $Q(S_{t+n}, A_{t+n})$.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "$$G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n max_{a'} Q_{t +n -1}(S_{t + n}, a') $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zkpagykkFj3"
   },
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgCJLiwZkId-"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfdnGQ1CkKF2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUP-IhpJkt73"
   },
   "source": [
    "## N-Step Buffer\n",
    "\n",
    "Para implementar nosso algoritmo n-step, vamos modificar o ReplayBuffer para retornar o n-step reward ($R_{t:t+n}$) em vez da recompensa no instante seguinte ($R_{t+1}$). \n",
    "\n",
    "$$R_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... + \\gamma^{n-1}R_{t + n}$$\n",
    "\n",
    "Também alteraremos o estado seguinte de $S_{t+1}$ para $S_{t+n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCnTllt7oCi6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NStepBuffer:\n",
    "    \"\"\"Experience Replay Buffer com n-step para DQNs.\"\"\"\n",
    "    def __init__(self, max_length, observation_space, gamma, n_step=3):\n",
    "        \"\"\"Cria um Replay Buffer.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        max_length: int\n",
    "            Tamanho máximo do Replay Buffer.\n",
    "        observation_space: int\n",
    "            Tamanho do espaço de observação.\n",
    "        gamma: float\n",
    "            Fator de desconto.\n",
    "        n_step: int\n",
    "            Timesteps considerados.\n",
    "        \"\"\"\n",
    "        self.gamma, self.n_step, self.episode_step = gamma, n_step, 0\n",
    "        self.index, self.size, self.max_length = 0, 0, max_length\n",
    "\n",
    "        self.states = np.zeros((max_length, observation_space), dtype=np.float32)\n",
    "        self.actions = np.zeros((max_length), dtype=np.int32)\n",
    "        self.n_step_return = np.zeros((max_length), dtype=np.float32)\n",
    "        self.next_states = np.zeros((max_length, observation_space), dtype=np.float32)\n",
    "        self.dones = np.zeros((max_length), dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Retorna o tamanho do buffer.\"\"\"\n",
    "        return self.size\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Adiciona uma experiência ao Replay Buffer.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        state: np.array\n",
    "            Estado da transição.\n",
    "        action: int\n",
    "            Ação tomada.\n",
    "        reward: float\n",
    "            Recompensa recebida.\n",
    "        state: np.array\n",
    "            Estado seguinte.\n",
    "        done: int\n",
    "            Flag indicando se o episódio acabou.\n",
    "        \"\"\"\n",
    "        self.states[self.index] = state\n",
    "        self.actions[self.index] = action\n",
    "        self.dones[self.index] = done\n",
    "        self.n_step_return[self.index] = 0\n",
    "        \n",
    "        # Soma o reward atual aos n instantes passados\n",
    "        for n in range(self.n_step):\n",
    "            if self.episode_step - n >= 0:\n",
    "                self.n_step_return[(self.index - n) % self.max_length] += reward * self.gamma**n\n",
    "        \n",
    "        if done:\n",
    "            self.episode_step = -1\n",
    "            self.dones[self.index - self.n_step + 1:self.index] = done\n",
    "        \n",
    "        if self.episode_step - self.n_step + 1 >= 0:\n",
    "            self.next_states[(self.index - self.n_step + 1) % self.max_length] = next_state\n",
    "        \n",
    "        self.episode_step += 1\n",
    "        self.index = (self.index + 1) % self.max_length\n",
    "        if self.size < self.max_length:\n",
    "            self.size = self.index\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Retorna um batch de experiências.\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            Tamanho do batch de experiências.\n",
    "\n",
    "        Retorna\n",
    "        -------\n",
    "        states: np.array\n",
    "            Batch de estados.\n",
    "        actions: np.array\n",
    "            Batch de ações.\n",
    "        n_step_return: np.array\n",
    "            Batch de retornos.\n",
    "        next_states: np.array\n",
    "            Batch de estados seguintes.\n",
    "        dones: np.array\n",
    "            Batch de flags indicando se o episódio acabou.\n",
    "        \"\"\"\n",
    "        # Escolhe índices aleatoriamente do Replay Buffer\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        return (self.states[idxs], self.actions[idxs], self.n_step_return[idxs], self.next_states[idxs], self.dones[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKyLGJFyoFjR"
   },
   "source": [
    "## Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4F49tlkvPfq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWrD9V1kvWth"
   },
   "source": [
    "## Agente DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMXF17a7oIqJ"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Uma classe que cria um agente DQN que utiliza NStepBuffer como memória\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space, \n",
    "                 lr=7e-4, \n",
    "                 gamma=0.99,\n",
    "                 max_memory=100000,\n",
    "                 epsilon_init=0.5,\n",
    "                 epsilon_decay=0.9995,\n",
    "                 min_epsilon=0.01,\n",
    "                 n_step=3):\n",
    "        \"\"\"\n",
    "        Inicializa o agente com os parâmetros dados\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.n_step = n_step\n",
    "        self.memory = NStepBuffer(max_memory, observation_space.shape[0], gamma, n_step)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.epsilon = epsilon_init\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "        self.dqn = Network(observation_space.shape[0], action_space.n).to(self.device)\n",
    "\n",
    "        self.optimizer  = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "            return action\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            action = self.dqn.forward(state).argmax(dim=-1)\n",
    "            action = action.cpu().numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.update(state, action, reward, new_state, done)\n",
    "\n",
    "    def train(self, batch_size=32, epochs=1):\n",
    "        # Se temos menos experiências que o batch size\n",
    "        # não começamos o treinamento\n",
    "        if batch_size > self.memory.size:\n",
    "            return\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Pegamos uma amostra das nossas experiências para treinamento\n",
    "            (states, actions, rewards, next_states, dones) = self.memory.sample(batch_size)\n",
    "\n",
    "            # Transformar nossas experiências em tensores\n",
    "            states = torch.as_tensor(states).to(self.device)\n",
    "            actions = torch.as_tensor(actions).to(self.device).unsqueeze(-1)\n",
    "            rewards = torch.as_tensor(rewards).to(self.device).unsqueeze(-1)\n",
    "            next_states = torch.as_tensor(next_states).to(self.device)\n",
    "            dones = torch.as_tensor(dones).to(self.device).unsqueeze(-1)\n",
    "\n",
    "            q = self.dqn.forward(states).gather(-1, actions.long())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q2 = self.dqn.forward(next_states).max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "                target = (rewards + (1 - dones) * (self.gamma ** self.n_step) * q2).to(self.device)\n",
    "\n",
    "            loss = F.mse_loss(q, target)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QS47MlN8pga6"
   },
   "source": [
    "### Definição de parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8vR8vZS1_A7"
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ARH8j14pfFT"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_INIT = 0.7\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.999\n",
    "MAX_MEMORY = 100000\n",
    "OBS_SPACE = env.observation_space\n",
    "ACT_SPACE = env.action_space\n",
    "N_STEP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwOrceIGpm_N"
   },
   "source": [
    "### Criando a DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjfkvnvCpxLq"
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(observation_space=OBS_SPACE, \n",
    "                 action_space=ACT_SPACE, \n",
    "                 lr=7e-4, \n",
    "                 gamma=GAMMA, \n",
    "                 max_memory=MAX_MEMORY,\n",
    "                 epsilon_init=EPS_INIT,\n",
    "                 epsilon_decay=EPS_DECAY,\n",
    "                 min_epsilon=EPS_END,\n",
    "                 n_step=N_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmYPxiROsQ9v"
   },
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLk74CbAIffE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import deque\n",
    "\n",
    "def train(agent, env, total_timesteps):\n",
    "    total_reward = 0\n",
    "    episode_returns = deque(maxlen=20)\n",
    "    avg_returns = []\n",
    "\n",
    "    state = env.reset()\n",
    "    timestep = 0\n",
    "    episode = 0\n",
    "\n",
    "    while timestep < total_timesteps:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.train()\n",
    "        \n",
    "        timestep += 1\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "        if done:\n",
    "            episode_returns.append(total_reward)\n",
    "            episode += 1\n",
    "            next_state = env.reset()\n",
    "\n",
    "        if any(G for G in episode_returns):\n",
    "            avg_returns.append(np.mean(episode_returns))\n",
    "\n",
    "        total_reward *= 1 - done\n",
    "        state = next_state\n",
    "\n",
    "        ratio = math.ceil(100 * timestep / total_timesteps)\n",
    "\n",
    "        avg_return = avg_returns[-1] if avg_returns else np.nan\n",
    "        \n",
    "        print(f\"\\r[{ratio:3d}%] timestep = {timestep}/{total_timesteps}, episode = {episode:3d}, avg_return = {avg_return:10.4f}\", end=\"\")\n",
    "\n",
    "    return avg_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 87674,
     "status": "ok",
     "timestamp": 1599511816287,
     "user": {
      "displayName": "Bernardo Coutinho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjbcuim7oGIm-uXpKRCJxDYg0Nhguq2a4_xKQcpjw=s64",
      "userId": "08343358744938767290"
     },
     "user_tz": 180
    },
    "id": "-vVnEAzLI4WX",
    "outputId": "9309817a-2819-4363-9cb6-22d36812f580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100%] timestep = 40000/40000, episode = 122, avg_return =   493.6000"
     ]
    }
   ],
   "source": [
    "returns = train(agent, env, 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88021,
     "status": "ok",
     "timestamp": 1599511816666,
     "user": {
      "displayName": "Bernardo Coutinho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjbcuim7oGIm-uXpKRCJxDYg0Nhguq2a4_xKQcpjw=s64",
      "userId": "08343358744938767290"
     },
     "user_tz": 180
    },
    "id": "2VQQJa21NdlO",
    "outputId": "0714bb3a-52b0-4dbe-edf2-7fac113b02dc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(returns, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7yJS829tGg6r"
   },
   "source": [
    "Comparando com a performance de uma DQN sem $n$-step:\n",
    "\n",
    "![DQN](https://media.discordapp.net/attachments/688564171973197869/752599003962540094/unknown.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNLK9rllIVTx"
   },
   "source": [
    "## Testando nosso Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gt3TIhQc_5U-"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def evaluate(agent, env, episodes=10):\n",
    "    total_reward = 0\n",
    "    episode_returns = deque(maxlen=episodes)\n",
    "    \n",
    "    episode = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while episode < episodes:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)        \n",
    "       \n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            episode_returns.append(total_reward)\n",
    "            episode += 1\n",
    "            next_state = env.reset()\n",
    "\n",
    "        total_reward *= 1 - done\n",
    "        state = next_state\n",
    "\n",
    "        ratio = math.ceil(100 * episode / episodes)\n",
    "        \n",
    "        print(f\"\\r[{ratio:3d}%] episode = {episode:3d}, avg_return = {np.mean(episode_returns):10.4f}\", end=\"\")\n",
    "\n",
    "    return np.mean(episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91593,
     "status": "ok",
     "timestamp": 1599511820292,
     "user": {
      "displayName": "Bernardo Coutinho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjbcuim7oGIm-uXpKRCJxDYg0Nhguq2a4_xKQcpjw=s64",
      "userId": "08343358744938767290"
     },
     "user_tz": 180
    },
    "id": "dLCaC4Hy_6-5",
    "outputId": "cbb3f08a-900b-4ec4-f040-8138543e2e47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100%] episode =  10, avg_return =   500.0000"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(agent, env, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NcxQb1uBjk4"
   },
   "source": [
    "## Referências\n",
    "\n",
    "\"Reinforcement Learning: An Introduction\"\n",
    "\n",
    "\"Rainbow: Combining Improvements in Deep Reinforcement Learning\": https://arxiv.org/pdf/1710.02298.pdf\n",
    "\n",
    "\"Understanding Multi-Step Deep Reinforcement\n",
    "Learning: A Systematic Study of the DQN Target\": https://arxiv.org/pdf/1901.07510.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "N-Step DQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
