{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWyEwLwn10hd"
   },
   "source": [
    "# N-Step DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-R_VyNqyAoq"
   },
   "source": [
    "# Conceito\n",
    "Em algoritmos de Monte Carlo, o nosso modelo ¨aprende¨ em cada transição com base em **toda** a sequência de recompensas em um episódio, ou seja, o retorno completo ($G_{t}$). Já em one-step Temporal Diference, o aprendizado é feito observando apenas uma recompensa no futuro ($R_{t + 1}$) e aproximamos o restante do retorno ($G_{t+1}$) como sendo o valor do próximo estado ($V(S_{t + 1})$).\n",
    "\n",
    "\n",
    "Agora, em N-step, tomamos uma abordagem intermediária a esses dois algoritmos. Não chegamos a utilizar a totalidade do retorno ($G_{t}$), mas **n** passos a frente do presente (**t**). Dessa forma, obtemos a seguinte expressão:\n",
    "\n",
    "$$ G_t = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{T -t -1} R_{T} $$\n",
    "\n",
    "\n",
    "$$ G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n V_{t +n -1}(S_{t + n}) $$\n",
    "\n",
    "\n",
    "\n",
    "onde $G_t$ representa o retorno completo (usado em Monte Carlo) e $G_{t:t + n}$ a aproximação do retorno com n-step, utilizando bootstraping do Valor do estado no instante t + n ($V_{t +n -1}(S_{t + n})$).\n",
    "\n",
    "![N-Step](https://media.discordapp.net/attachments/688564171973197869/752614671974006844/unknown.png)\n",
    "\n",
    "Vale notar que, no início do episódio, o agente não possui todas as experiências necessárias para fazer a estimativa do retorno. Para contornar isso, fazemos mudanças no replay buffer (explicadas mais a frente) para possibilitar o cálculo.\n",
    "\n",
    "## Sarsa para $n$-Step\n",
    "Agora que temos uma noção de como o $n$-step funciona, podemos nos preocupar em como nosso agente pode fazer uma escolha se baseando nesse processo. Para isso, mudamos nossa previsão para que ela preveja ações e não estados. Com esse objetivo, chegamos às seguintes expressões:\n",
    "\n",
    "$$G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n Q_{t +n -1}(S_{t + n}, A_{t + n}) $$\n",
    "\n",
    "$$Q_{t +n}(S_{t}, A_{t}) = Q_{t +n -1}(S_{t}, A_{t}) + \\alpha [G_{t:t + n} - Q_{t +n -1}(S_{t}, A_{t})]$$\n",
    "\n",
    "\n",
    "Tendo esse algoritmo, só precisamos de uma política $\\pi$, por exemplo, $\\varepsilon$-greedy.\n",
    "\n",
    "## *Off-policy* $n$-step\n",
    "Na implementação Off-Policy de n-Step, nosso objetivo é usar as informações obtidas a partir da nossa política atual para obter uma política ótima, da mesma forma como acontecia com o Q-Learning. Para obter a equação do Q-Learning, referente ao 1-step, bastou substituir o Q que dependia da ação escolhida pela nossa política, $Q(S_{t + 1}, A_{t + 1})$, por $max_{a'}Q(S_{t + 1}, a')$, referente à política ótima:\n",
    "\n",
    "$$G_{t} = R_{t+1} + \\gamma Q(S_{t + 1}, A_{t + 1})$$\n",
    "$$G_{t} = R_{t+1} + \\gamma max_{a'} Q(S_{t + 1}, a')$$\n",
    "\n",
    "Entretanto, na equação de n-Step Sarsa, os termos $R_{t+i}$ também são dependentes da política que estamos seguindo, com exceção do $R_{t+1}$, que depende somente da ação tomada no tempo _t_. Basicamente, isso significa que, seguindo a política ótima, obteríamos  recompensas diferentes $R_{t+i}$, já que tomamos ações diferentes ao longo da trajetória.\n",
    "\n",
    "$$G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n Q_{t +n -1}(S_{t + n}, A_{t + n})$$\n",
    "\n",
    "Dessa forma, seria necessário implementar uma \"correção\" no nosso cálculo chamada *importance sampling*. Isso necessariamente requer que avaliemos a *importance sampling ratio* ($\\rho_{t:t+n-1}$) para cada passo a frente (n) que queremos avaliar.\n",
    "\n",
    "No entanto, com base nos artigos:\n",
    "> \"Rainbow: Combining Improvements in Deep Reinforcement Learning\": https://arxiv.org/pdf/1710.02298.pdf\n",
    "\n",
    "> \"Understanding Multi-Step Deep Reinforcement\n",
    "Learning: A Systematic Study of the DQN Target\": https://arxiv.org/pdf/1901.07510.pdf\n",
    "\n",
    "decidimos por não integrar *importance sampling* ao modelo pois, para valores de $n$ pequenos comumente como $n=3$, não aparentam existir impactos significativos da falta desse recurso. Além disso, para valores de $n$ maiores, o treinamento seria mais demorado, sem benefícios claros.\n",
    "\n",
    "Logo, a única alteração feita para off-policy em relação a on-policy $n$-step, é utilizar o $max_{a'} Q(S_{t+n}, a')$ ao invés do $Q(S_{t+n}, A_{t+n})$.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "$$G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n max_{a'} Q_{t +n -1}(S_{t + n}, a') $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zkpagykkFj3"
   },
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgCJLiwZkId-"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfdnGQ1CkKF2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUP-IhpJkt73"
   },
   "source": [
    "## N-Step Buffer\n",
    "\n",
    "Para implementar nosso algoritmo n-step, vamos modificar o ReplayBuffer para retornar o n-step reward ($R_{t:t+n}$) em vez da recompensa no instante seguinte ($R_{t+1}$). \n",
    "\n",
    "$$R_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... + \\gamma^{n-1}R_{t + n}$$\n",
    "\n",
    "Também alteraremos o estado seguinte de $S_{t+1}$ para $S_{t+n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCnTllt7oCi6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NStepBuffer:\n",
    "    \"\"\"Experience Replay Buffer com n-step para DQNs.\"\"\"\n",
    "    def __init__(self, max_length, observation_space, gamma, n_step=3):\n",
    "        \"\"\"Cria um Replay Buffer.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        max_length: int\n",
    "            Tamanho máximo do Replay Buffer.\n",
    "        observation_space: int\n",
    "            Tamanho do espaço de observação.\n",
    "        gamma: float\n",
    "            Fator de desconto.\n",
    "        n_step: int\n",
    "            Timesteps considerados.\n",
    "        \"\"\"\n",
    "        self.gamma, self.n_step, self.episode_step = gamma, n_step, 0\n",
    "        self.index, self.size, self.max_length = 0, 0, max_length\n",
    "\n",
    "        self.states = np.zeros((max_length, observation_space), dtype=np.float32)\n",
    "        self.actions = np.zeros((max_length), dtype=np.int32)\n",
    "        self.n_step_return = np.zeros((max_length), dtype=np.float32)\n",
    "        self.next_states = np.zeros((max_length, observation_space), dtype=np.float32)\n",
    "        self.dones = np.zeros((max_length), dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Retorna o tamanho do buffer.\"\"\"\n",
    "        return self.size\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Adiciona uma experiência ao Replay Buffer.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        state: np.array\n",
    "            Estado da transição.\n",
    "        action: int\n",
    "            Ação tomada.\n",
    "        reward: float\n",
    "            Recompensa recebida.\n",
    "        state: np.array\n",
    "            Estado seguinte.\n",
    "        done: bool\n",
    "            Flag indicando se o episódio acabou.\n",
    "        \"\"\"\n",
    "        self.states[self.index] = state\n",
    "        self.actions[self.index] = action\n",
    "        self.dones[self.index] = done\n",
    "        self.n_step_return[self.index] = 0\n",
    "        \n",
    "        # Soma o reward atual aos n instantes passados\n",
    "        for n in range(self.n_step):\n",
    "            if self.episode_step - n >= 0:\n",
    "                self.n_step_return[(self.index - n) % self.max_length] += reward * self.gamma**n\n",
    "        \n",
    "        if done:\n",
    "            self.episode_step = -1\n",
    "            self.dones[self.index - self.n_step + 1:self.index] = done\n",
    "        \n",
    "        if self.episode_step - self.n_step + 1 >= 0:\n",
    "            self.next_states[(self.index - self.n_step + 1) % self.max_length] = next_state\n",
    "        \n",
    "        self.episode_step += 1\n",
    "        self.index = (self.index + 1) % self.max_length\n",
    "        if self.size < self.max_length:\n",
    "            self.size = self.index\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Retorna um batch de experiências.\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            Tamanho do batch de experiências.\n",
    "\n",
    "        Retorna\n",
    "        -------\n",
    "        states: np.array\n",
    "            Batch de estados.\n",
    "        actions: np.array\n",
    "            Batch de ações.\n",
    "        n_step_return: np.array\n",
    "            Batch de retornos.\n",
    "        next_states: np.array\n",
    "            Batch de estados seguintes.\n",
    "        dones: np.array\n",
    "            Batch de flags indicando se o episódio acabou.\n",
    "        \"\"\"\n",
    "        # Escolhe índices aleatoriamente do Replay Buffer\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        return (self.states[idxs], self.actions[idxs], self.n_step_return[idxs], self.next_states[idxs], self.dones[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKyLGJFyoFjR"
   },
   "source": [
    "## Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4F49tlkvPfq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWrD9V1kvWth"
   },
   "source": [
    "## Agente DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMXF17a7oIqJ"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Uma classe que cria um agente DQN que utiliza NStepBuffer como memória\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space, \n",
    "                 lr=7e-4, \n",
    "                 gamma=0.99,\n",
    "                 max_memory=100000,\n",
    "                 epsilon_init=0.5,\n",
    "                 epsilon_decay=0.9995,\n",
    "                 min_epsilon=0.01,\n",
    "                 n_step=3):\n",
    "        \"\"\"\n",
    "        Inicializa o agente com os parâmetros dados\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.n_step = n_step\n",
    "        self.memory = NStepBuffer(max_memory, observation_space.shape[0], gamma, n_step)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.epsilon = epsilon_init\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "        self.dqn = Network(observation_space.shape[0], action_space.n).to(self.device)\n",
    "\n",
    "        self.optimizer  = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "            return action\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            action = self.dqn.forward(state).argmax(dim=-1)\n",
    "            action = action.cpu().numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.update(state, action, reward, new_state, done)\n",
    "\n",
    "    def train(self, batch_size=32, epochs=1):\n",
    "        # Se temos menos experiências que o batch size\n",
    "        # não começamos o treinamento\n",
    "        if batch_size > self.memory.size:\n",
    "            return\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Pegamos uma amostra das nossas experiências para treinamento\n",
    "            (states, actions, rewards, next_states, dones) = self.memory.sample(batch_size)\n",
    "\n",
    "            # Transformar nossas experiências em tensores\n",
    "            states = torch.as_tensor(states).to(self.device)\n",
    "            actions = torch.as_tensor(actions).to(self.device).unsqueeze(-1)\n",
    "            rewards = torch.as_tensor(rewards).to(self.device).unsqueeze(-1)\n",
    "            next_states = torch.as_tensor(next_states).to(self.device)\n",
    "            dones = torch.as_tensor(dones).to(self.device).unsqueeze(-1)\n",
    "\n",
    "            q = self.dqn.forward(states).gather(-1, actions.long())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q2 = self.dqn.forward(next_states).max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "                target = (rewards + (1 - dones) * (self.gamma ** self.n_step) * q2).to(self.device)\n",
    "\n",
    "            loss = F.mse_loss(q, target)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QS47MlN8pga6"
   },
   "source": [
    "### Definição de parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8vR8vZS1_A7"
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ARH8j14pfFT"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_INIT = 0.7\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.999\n",
    "MAX_MEMORY = 100000\n",
    "OBS_SPACE = env.observation_space\n",
    "ACT_SPACE = env.action_space\n",
    "N_STEP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwOrceIGpm_N"
   },
   "source": [
    "### Criando a DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjfkvnvCpxLq"
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(observation_space=OBS_SPACE, \n",
    "                 action_space=ACT_SPACE, \n",
    "                 lr=7e-4, \n",
    "                 gamma=GAMMA, \n",
    "                 max_memory=MAX_MEMORY,\n",
    "                 epsilon_init=EPS_INIT,\n",
    "                 epsilon_decay=EPS_DECAY,\n",
    "                 min_epsilon=EPS_END,\n",
    "                 n_step=N_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmYPxiROsQ9v"
   },
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLk74CbAIffE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import deque\n",
    "\n",
    "def train(agent, env, total_timesteps):\n",
    "    total_reward = 0\n",
    "    episode_returns = deque(maxlen=20)\n",
    "    avg_returns = []\n",
    "\n",
    "    state = env.reset()\n",
    "    timestep = 0\n",
    "    episode = 0\n",
    "\n",
    "    while timestep < total_timesteps:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.train()\n",
    "        \n",
    "        timestep += 1\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "        if done:\n",
    "            episode_returns.append(total_reward)\n",
    "            episode += 1\n",
    "            next_state = env.reset()\n",
    "\n",
    "        if episode_returns:\n",
    "            avg_returns.append(np.mean(episode_returns))\n",
    "\n",
    "        total_reward *= 1 - done\n",
    "        state = next_state\n",
    "\n",
    "        ratio = math.ceil(100 * timestep / total_timesteps)\n",
    "\n",
    "        avg_return = avg_returns[-1] if avg_returns else np.nan\n",
    "        \n",
    "        print(f\"\\r[{ratio:3d}%] timestep = {timestep}/{total_timesteps}, episode = {episode:3d}, avg_return = {avg_return:10.4f}\", end=\"\")\n",
    "\n",
    "    return avg_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 87674,
     "status": "ok",
     "timestamp": 1599511816287,
     "user": {
      "displayName": "Bernardo Coutinho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjbcuim7oGIm-uXpKRCJxDYg0Nhguq2a4_xKQcpjw=s64",
      "userId": "08343358744938767290"
     },
     "user_tz": 180
    },
    "id": "-vVnEAzLI4WX",
    "outputId": "9309817a-2819-4363-9cb6-22d36812f580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100%] timestep = 40000/40000, episode = 135, avg_return =   493.0000"
     ]
    }
   ],
   "source": [
    "returns = train(agent, env, 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88021,
     "status": "ok",
     "timestamp": 1599511816666,
     "user": {
      "displayName": "Bernardo Coutinho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjbcuim7oGIm-uXpKRCJxDYg0Nhguq2a4_xKQcpjw=s64",
      "userId": "08343358744938767290"
     },
     "user_tz": 180
    },
    "id": "2VQQJa21NdlO",
    "outputId": "0714bb3a-52b0-4dbe-edf2-7fac113b02dc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(returns, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7yJS829tGg6r"
   },
   "source": [
    "Comparando com a performance de uma DQN sem $n$-step:\n",
    "\n",
    "![DQN](https://media.discordapp.net/attachments/688564171973197869/752599003962540094/unknown.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNLK9rllIVTx"
   },
   "source": [
    "## Testando nosso Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gt3TIhQc_5U-"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def evaluate(agent, env, episodes=10):\n",
    "    total_reward = 0\n",
    "    episode_returns = deque(maxlen=episodes)\n",
    "    \n",
    "    episode = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while episode < episodes:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)        \n",
    "       \n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            episode_returns.append(total_reward)\n",
    "            episode += 1\n",
    "            next_state = env.reset()\n",
    "\n",
    "        total_reward *= 1 - done\n",
    "        state = next_state\n",
    "\n",
    "        ratio = math.ceil(100 * episode / episodes)\n",
    "        \n",
    "        print(f\"\\r[{ratio:3d}%] episode = {episode:3d}, avg_return = {np.mean(episode_returns):10.4f}\", end=\"\")\n",
    "\n",
    "    return np.mean(episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91593,
     "status": "ok",
     "timestamp": 1599511820292,
     "user": {
      "displayName": "Bernardo Coutinho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjbcuim7oGIm-uXpKRCJxDYg0Nhguq2a4_xKQcpjw=s64",
      "userId": "08343358744938767290"
     },
     "user_tz": 180
    },
    "id": "dLCaC4Hy_6-5",
    "outputId": "cbb3f08a-900b-4ec4-f040-8138543e2e47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100%] episode =  10, avg_return =   471.1000"
     ]
    },
    {
     "data": {
      "text/plain": [
       "471.1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(agent, env, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NcxQb1uBjk4"
   },
   "source": [
    "## Referências\n",
    "\n",
    "\"Reinforcement Learning: An Introduction\"\n",
    "\n",
    "\"Rainbow: Combining Improvements in Deep Reinforcement Learning\": https://arxiv.org/pdf/1710.02298.pdf\n",
    "\n",
    "\"Understanding Multi-Step Deep Reinforcement\n",
    "Learning: A Systematic Study of the DQN Target\": https://arxiv.org/pdf/1901.07510.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "N-Step DQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
