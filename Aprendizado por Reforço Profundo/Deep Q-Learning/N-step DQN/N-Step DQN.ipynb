{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWyEwLwn10hd"
   },
   "source": [
    "# N-Step DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-R_VyNqyAoq"
   },
   "source": [
    "# Conceito\n",
    "Em algoritmos de Monte Carlo, o nosso modelo ¨aprende¨ em cada transição com base em **toda** a sequência de recompensas em um episódio, ou seja, o retorno completo ($G_{t}$). Já em one-step Temporal Diference, o aprendizado é feito observando apenas uma recompensa no futuro ($R_{t + 1}$) e aproximamos o restante do retorno ($G_{t+1}$) como sendo o valor do próximo estado ($V(S_{t + 1})$).\n",
    "\n",
    "\n",
    "Agora, em N-step, tomamos uma abordagem intermediária a esses dois algoritmos. Não chegamos a utilizar a totalidade do retorno ($G_{t}$), mas **n** passos a frente do presente (**t**). Dessa forma, obtemos a seguinte expressão:\n",
    "\n",
    "$$ G_t = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{T -t -1} R_{T} $$\n",
    "\n",
    "\n",
    "$$ G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n V_{t +n -1}(S_{t + n}) $$\n",
    "\n",
    "\n",
    "\n",
    "onde $G_t$ representa o retorno completo (usado em Monte Carlo) e $G_{t:t + n}$ a aproximação do retorno com n-step, utilizando bootstraping do Valor do estado no instante t + n ($V_{t +n -1}(S_{t + n})$).\n",
    "\n",
    "![N-Step](https://media.discordapp.net/attachments/688564171973197869/752614671974006844/unknown.png)\n",
    "\n",
    "Vale notar que, no início do episódio, o agente não possui todas as experiências necessárias para fazer a estimativa do retorno. Para contornar isso, fazemos mudanças no replay buffer (explicadas mais a frente) para possibilitar o cálculo.\n",
    "\n",
    "## Sarsa para $n$-Step\n",
    "Agora que temos uma noção de como o $n$-step funciona, podemos nos preocupar em como nosso agente pode fazer uma escolha se baseando nesse processo. Para isso, mudamos nossa previsão para que ela preveja ações e não estados. Com esse objetivo, chegamos às seguintes expressões:\n",
    "\n",
    "$$G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n Q_{t +n -1}(S_{t + n}, A_{t + n}) $$\n",
    "\n",
    "$$Q_{t +n}(S_{t}, A_{t}) = Q_{t +n -1}(S_{t}, A_{t}) + \\alpha [G_{t:t + n} - Q_{t +n -1}(S_{t}, A_{t})]$$\n",
    "\n",
    "\n",
    "Tendo esse algoritmo, só precisamos de uma política $\\pi$, por exemplo, $\\varepsilon$-greedy.\n",
    "\n",
    "## *Off-policy* $n$-step\n",
    "Para fazermos uma implementação tecnicamente ¨completa¨ de $n$-step *off-policy*, seria necessário implementar a funcionalidade de *importance sampling* no $n$-step buffer. Isso necessariamente requer que avaliemos a *importance sampling ratio*($\\rho_{t:t+n-1}$) para cada passo a frente ($n$) que queremos avaliar.\n",
    "\n",
    "No entanto, com base nos artigos:\n",
    "> \"Rainbow: Combining Improvements in Deep Reinforcement Learning\": https://arxiv.org/pdf/1710.02298.pdf\n",
    "\n",
    "> \"Understanding Multi-Step Deep Reinforcement\n",
    "Learning: A Systematic Study of the DQN Target\": https://arxiv.org/pdf/1901.07510.pdf\n",
    "\n",
    "decidimos por não integrar *importance sampling* ao modelo pois, para valores de $n$ pequenos comumente como $n=3$, não aparentam existir impactos significativos da falta desse recurso. Além disso, para valores de $n$ maiores, o treinamento seria mais demorado, sem benefícios claros.\n",
    "\n",
    "Logo, a única alteração feita para off-policy em relação a on-policy $n$-step, é utilizar o $max_{a'} Q(S_{t+n}, a')$ ao invés do $Q(S_{t+n}, A_{t+n})$.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "$$G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n max_{a'} Q_{t +n -1}(S_{t + n}, a') $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zkpagykkFj3"
   },
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgCJLiwZkId-"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfdnGQ1CkKF2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUP-IhpJkt73"
   },
   "source": [
    "## N-Step Buffer\n",
    "\n",
    "Para implementar nosso algoritmo n-step, vamos modificar o ReplayBuffer para retornar o n-step reward ($R_{t:t+n}$) em vez da recompensa no instante seguinte ($R_{t+1}$). \n",
    "\n",
    "$$R_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... + \\gamma^{n-1}R_{t + n}$$\n",
    "\n",
    "Também alteraremos o estado seguinte de $S_{t+1}$ para $S_{t+n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCnTllt7oCi6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NStepBuffer:\n",
    "    \"\"\"Experience Replay Buffer com n-step para DQNs.\"\"\"\n",
    "    def __init__(self, max_length, observation_space, gamma, n_step=3):\n",
    "        \"\"\"Cria um Replay Buffer.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        max_length: int\n",
    "            Tamanho máximo do Replay Buffer.\n",
    "        observation_space: int\n",
    "            Tamanho do espaço de observação.\n",
    "        gamma: float\n",
    "            Fator de desconto.\n",
    "        n_step: int\n",
    "            Timesteps considerados.\n",
    "        \"\"\"\n",
    "        self.gamma, self.n_step, self.episode_step = gamma, n_step, 0\n",
    "        self.index, self.size, self.max_length = 0, 0, max_length\n",
    "\n",
    "        self.states = np.zeros((max_length, observation_space), dtype=np.float32)\n",
    "        self.actions = np.zeros((max_length), dtype=np.int32)\n",
    "        self.n_step_rewards = np.zeros((max_length), dtype=np.float32)\n",
    "        self.next_states = np.zeros((max_length, observation_space), dtype=np.float32)\n",
    "        self.dones = np.zeros((max_length), dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Retorna o tamanho do buffer.\"\"\"\n",
    "        return self.size\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Adiciona uma experiência ao Replay Buffer.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        state: np.array\n",
    "            Estado da transição.\n",
    "        action: int\n",
    "            Ação tomada.\n",
    "        reward: float\n",
    "            Recompensa recebida.\n",
    "        state: np.array\n",
    "            Estado seguinte.\n",
    "        done: int\n",
    "            Flag indicando se o episódio acabou.\n",
    "        \"\"\"\n",
    "        self.states[self.index] = state\n",
    "        self.actions[self.index] = action\n",
    "        self.dones[self.index] = done\n",
    "        \n",
    "        # Soma o reward atual aos n instantes passados\n",
    "        for n in range(self.n_step):\n",
    "            if self.episode_step - n >= 0:\n",
    "                self.n_step_rewards[self.index - n] += reward * self.gamma**n\n",
    "        \n",
    "        if done:\n",
    "            self.episode_step = 0\n",
    "            self.dones[self.index - self.n_step:self.index] = done\n",
    "        \n",
    "        if self.episode_step - self.n_step >= 0:\n",
    "            self.next_states[self.index - self.n_step] = next_state\n",
    "        \n",
    "        self.episode_step += 1\n",
    "        self.index = (self.index + 1) % self.max_length\n",
    "        if self.size < self.max_length:\n",
    "            self.size = self.index\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Retorna um batch de experiências.\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            Tamanho do batch de experiências.\n",
    "\n",
    "        Retorna\n",
    "        -------\n",
    "        states: np.array\n",
    "            Batch de estados.\n",
    "        actions: np.array\n",
    "            Batch de ações.\n",
    "        n_step_rewards: np.array\n",
    "            Batch de retornos.\n",
    "        next_states: np.array\n",
    "            Batch de estados seguintes.\n",
    "        dones: np.array\n",
    "            Batch de flags indicando se o episódio acabou.\n",
    "        \"\"\"\n",
    "        # Escolhe índices aleatoriamente do Replay Buffer\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        return (self.states[idxs], self.actions[idxs], self.n_step_rewards[idxs], self.next_states[idxs], self.dones[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKyLGJFyoFjR"
   },
   "source": [
    "## Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4F49tlkvPfq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWrD9V1kvWth"
   },
   "source": [
    "## Agente DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMXF17a7oIqJ"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Uma classe que cria um agente DQN que utiliza NStepBuffer como memória\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space, \n",
    "                 lr=7e-4, \n",
    "                 gamma=0.99,\n",
    "                 max_memory=100000,\n",
    "                 epsilon_init=0.5,\n",
    "                 epsilon_decay=0.9995,\n",
    "                 min_epsilon=0.01,\n",
    "                 n_step=3):\n",
    "        \"\"\"\n",
    "        Inicializa o agente com os parâmetros dados\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.n_step = n_step\n",
    "        self.memory = NStepBuffer(max_memory, observation_space.shape[0], gamma, n_step)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.epsilon = epsilon_init\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "        self.dqn = Network(observation_space.shape[0], action_space.n).to(self.device)\n",
    "\n",
    "        self.optimizer  = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "            return action\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            action = self.dqn.forward(state).argmax(dim=-1)\n",
    "            action = action.cpu().numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.update(state, action, reward, new_state, done)\n",
    "\n",
    "    def train(self, batch_size=32, epochs=1):\n",
    "        # Se temos menos experiências que o batch size\n",
    "        # não começamos o treinamento\n",
    "        if batch_size > self.memory.size:\n",
    "            return\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Pegamos uma amostra das nossas experiências para treinamento\n",
    "            (states, actions, rewards, next_states, dones) = self.memory.sample(batch_size)\n",
    "\n",
    "            # Transformar nossas experiências em tensores\n",
    "            states = torch.as_tensor(states).to(self.device)\n",
    "            actions = torch.as_tensor(actions).to(self.device).unsqueeze(-1)\n",
    "            rewards = torch.as_tensor(rewards).to(self.device).unsqueeze(-1)\n",
    "            next_states = torch.as_tensor(next_states).to(self.device)\n",
    "            dones = torch.as_tensor(dones).to(self.device).unsqueeze(-1)\n",
    "\n",
    "            q = self.dqn.forward(states).gather(-1, actions.long())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q2 = self.dqn.forward(next_states).max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "                target = (rewards + (1 - dones) * (self.gamma ** self.n_step) * q2).to(self.device)\n",
    "\n",
    "            loss = F.mse_loss(q, target)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QS47MlN8pga6"
   },
   "source": [
    "### Definição de parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8vR8vZS1_A7"
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ARH8j14pfFT"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_INIT = 0.7\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.999\n",
    "MAX_MEMORY = 100000\n",
    "OBS_SPACE = env.observation_space\n",
    "ACT_SPACE = env.action_space\n",
    "N_STEP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwOrceIGpm_N"
   },
   "source": [
    "### Criando a DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjfkvnvCpxLq"
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(observation_space=OBS_SPACE, \n",
    "                 action_space=ACT_SPACE, \n",
    "                 lr=7e-4, \n",
    "                 gamma=GAMMA, \n",
    "                 max_memory=MAX_MEMORY,\n",
    "                 epsilon_init=EPS_INIT,\n",
    "                 epsilon_decay=EPS_DECAY,\n",
    "                 min_epsilon=EPS_END,\n",
    "                 n_step=N_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmYPxiROsQ9v"
   },
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLk74CbAIffE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import deque\n",
    "\n",
    "def train(agent, env, total_timesteps):\n",
    "    total_reward = 0\n",
    "    episode_returns = deque(maxlen=20)\n",
    "    avg_returns = []\n",
    "\n",
    "    state = env.reset()\n",
    "    timestep = 0\n",
    "    episode = 0\n",
    "\n",
    "    while timestep < total_timesteps:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.train()\n",
    "        \n",
    "        timestep += 1\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "        if done:\n",
    "            episode_returns.append(total_reward)\n",
    "            episode += 1\n",
    "            next_state = env.reset()\n",
    "\n",
    "        if any(G for G in episode_returns):\n",
    "            avg_returns.append(np.mean(episode_returns))\n",
    "\n",
    "        total_reward *= 1 - done\n",
    "        state = next_state\n",
    "\n",
    "        ratio = math.ceil(100 * timestep / total_timesteps)\n",
    "\n",
    "        avg_return = avg_returns[-1] if avg_returns else np.nan\n",
    "        \n",
    "        print(f\"\\r[{ratio:3d}%] timestep = {timestep}/{total_timesteps}, episode = {episode:3d}, avg_return = {avg_return:10.4f}\", end=\"\")\n",
    "\n",
    "    return avg_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 87674,
     "status": "ok",
     "timestamp": 1599511816287,
     "user": {
      "displayName": "Bernardo Coutinho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjbcuim7oGIm-uXpKRCJxDYg0Nhguq2a4_xKQcpjw=s64",
      "userId": "08343358744938767290"
     },
     "user_tz": 180
    },
    "id": "-vVnEAzLI4WX",
    "outputId": "9309817a-2819-4363-9cb6-22d36812f580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100%] timestep = 40000/40000, episode = 124, avg_return =   481.0500"
     ]
    }
   ],
   "source": [
    "returns = train(agent, env, 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88021,
     "status": "ok",
     "timestamp": 1599511816666,
     "user": {
      "displayName": "Bernardo Coutinho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjbcuim7oGIm-uXpKRCJxDYg0Nhguq2a4_xKQcpjw=s64",
      "userId": "08343358744938767290"
     },
     "user_tz": 180
    },
    "id": "2VQQJa21NdlO",
    "outputId": "0714bb3a-52b0-4dbe-edf2-7fac113b02dc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAcD0lEQVR4nO3de5QU5Z3/8feXuzcChMEgYMBLvMagmXgJHqKQiKBZSI66UUlQOUtWcU/iZSO6ibmcbI7G9Wc0P1eDl4i6rrddDsSQCPEW3RjCoGhUVEaCOkIE5bImKDgz3/3jeTrTM8z0dM90d3VXf17n9Kmqp6q7v1M985mnn6quNndHRETSpU/SBYiISPEp3EVEUkjhLiKSQgp3EZEUUriLiKRQv6QLABg+fLiPHTs26TJERKrKypUr33H3us7WVUS4jx07loaGhqTLEBGpKmb2elfrNCwjIpJCCncRkRRSuIuIpJDCXUQkhfIKdzNbZ2Z/NLNVZtYQ24aZ2TIzWxOnQ2O7mdkNZtZoZs+b2VGl/AFERGRXhfTcT3T38e5eH5fnAY+4+4HAI3EZYCpwYLzNAW4qVrEiIpKf3gzLTAcWxPkFwIys9js9+D0wxMxG9uJ5RESkQPme5+7AUjNz4GfuPh/Y2903ALj7BjMbEbcdBbyZdd+m2LYh+wHNbA6hZ8++++7b859AKsuTT8KyZW3LRxwBp53W+8dtaYHXuzilt18/GDMGzHr/PCIpkW+4T3D39THAl5nZyzm27ewvbJeLxsd/EPMB6uvrdVH5tLjiCnjqqRC07rDbbvDhh/ndd/hw+MIXOl930UXw0592fd+f/xzOOafgckXSKq9wd/f1cbrRzBYCRwNvm9nI2GsfCWyMmzcBY7LuPhpYX8SapZJt3w6nngq/+AXceiv8wz/AWWflf/9PfAJGdjKK98ILsP/+cOWV7dvdQ6h31auX6pP5AiH3tlvHZTMYMCC5GqtAt+FuZnsAfdz9vTh/EvADYDEwC7gqThfFuywGLjSze4FjgG2Z4RupATt2tP3RzZ4NJ54Izc3d32/jRvjOd7pef/jhcOaZ8LWv7bru/PPhnnvg+ee7f57jjw/vAqQyzJ4Nd9zRPsTzdeONcMEFJSkrDfLpue8NLLQwntkPuMfdf21mK4D7zWw28AZwetx+CTANaAS2A+cWvWqpHO+/D6tWtf1hbtsGAweGebPQ287HQQfB44/3rIazzoLly+HVV3Nvt359GDJSuOf2zjthmGvnzl17zplpZ2251nW1/eLFcMgh8OUvh2Wztluu5R/9KLyb660PP4S33ur94xSqf38YNaqkT9FtuLv7WuBTnbS/C0zupN2BuUWpTirf978PV1/dvm3YsPLWcOut+W13ySXws59Ba2tYzg4MaXPPPfCtbxV+v44hnE9bnz5h6O4b3yjsuW65BZYs6fpg/bhxcOmlYX6vvWD33TvfbvZsuOuuwp67WO6+G84+u2QPXxFXhZQqsmJF+IPasSMsb9sWzlTJDthjjkmmtu4MHgx//Sv07dv1NvkEVD7B9clPwu9+131N7jBzJrycdY7CGWfAZZd1vv3LL0NTU+fr+vWDz36292PR770Xpn/5S3is7n7eJJxxBvzyl+33W8aLL4bpv/1bmA4ZAhs2wKBB4TX54Q/b/sEvXx7O6Crnuzl3OO88eOyx8LdzwAGwzz5FfxrzQse5SqC+vt51yd8qkTlIOnMm7LFHaJs8GU4/Pff9KkFTEyxY0HYMoNjDDJn5hgZ44olwDKFv3/ZDCx1vLS3w7W/DYYeF3uYzz4SD0p2dNbRzJyxatGt7tq7Goa+4An71q9z3zdiwAd59N/+znCrN1q3wwAPhdX766dAz/8UvYMQIuP56uO8+qK9v2/7882HWrPLWOGIEbNoU5m+6Cf7xH3v0MGa2MuuDpe3XKdylIP/+7zB3bjgAWtfpdwTI0qXhjKF8w7FPnzDEMGVKeKv+ox91va17CIJPf3rXdSeeGHqAmS++OftsmDMnzI8dG3qrRx6ZX03jx4cht2r30EPwxS+2bzv4YFi9Opl6MtauhXXrwvxBB/V4/D1XuGtYRgqzc2eY9u+fbB2V7KST2h+M7O7Wp08YMoDwjmjmzJ497ze/Gd41QDhz6NprYc89w/LmzfDVr4aefS2ZOhUefTQc+M84+ODk6snYb79wKyGFuxQmE+46x7h72WPU5XDNNW3zF1wQ3u5nH7AbM2bX+6Rd377hHU0NUrhLblu3wrRpsGVLWH7nnTBVz72y3XBD6Mln9OlT8p6iVBaFu+T22mvhoNSECW3jggcfrHCvdP36hU/7Ss1SuEtumTNLrrgi9OBFpCrom5gkt8wZH+qpi1QVhbvkpnAXqUoKd8ktE+79NIInUk30Fyu7uvHGtg95ZC6lq567SFVRuEt77vBP/xQ+VJO52NJ++4WPxotI1dCwjLTX2hoC/vLLwznt77wTToccMaL7+4pIxVC4S3uZUx81xi5S1RTu0l4m3HNdFldEKp7CXdpraQlT9dxFqprCXdrTsIxIKijcpT2Fu0gq6C9YYN688O00oHAXSQn9BQs8/HAI9UmTwvKAAeFLDkSkaincJQT7Zz4Tvl9URFJBY+4SzpDRMIxIqijcJfTcdV67SKoo3CWEu3ruIqmicJcwLKOeu0iqKNxFPXeRFNJfdC1atgyuvz5c/RHClR/VcxdJFfXca9F998HSpbBxY7h96lM6r10kZdRzr0XNzbDPPrBiRdKViEiJqOdei3QAVST1FO61SOe1i6Re3uFuZn3N7FkzeygujzOz5Wa2xszuM7MBsX1gXG6M68eWpnTpMX0iVST1Cum5fwNYnbV8NXCdux8IbAFmx/bZwBZ3PwC4Lm4nlUTDMiKpl1e4m9lo4BTg1rhswCTgwbjJAmBGnJ8el4nrJ8ftpVJoWEYk9fLtuf8E+BbQGpc/Cmx193jxb5qAUXF+FPAmQFy/LW7fjpnNMbMGM2vYtGlTD8uXHtGwjEjqdRvuZnYqsNHdV2Y3d7Kp57GurcF9vrvXu3t9XV1dXsVKD/3mN7D77iHQ+/WDX/4yXLNdRFIrn+7bBODvzGwaMAgYTOjJDzGzfrF3PhpYH7dvAsYATWbWD/gIsLnolUv+Xn4Z3n8fLroohDzA5MnJ1iQiJdVtuLv75cDlAGZ2AnCpu59tZg8ApwH3ArOARfEui+Py03H9o+6+S89dyijz1XlXXglDhiRbi4iURW/Oc78MuNjMGglj6rfF9tuAj8b2i4F5vStRei0T7jqIKlIzCjqq5u6PA4/H+bXA0Z1s8wFwehFqk2JpaQlTHUQVqRn6hGotUM9dpOYo3GtBJtzVcxepGQr3WpAZlumjl1ukVqgrl0bvvgszZsC2bWH5z3/WkIxIjVG4p9Err8BTT8FnPwt77w0HHACf/GTSVYlIGSnc0ygzDPODH+jDSiI1SoOwaZQJdw3FiNQshXsa6ewYkZqncE8j9dxFap7CPY0U7iI1T+GeRhqWEal5Cvc0Us9dpOapa5cGra1wyCGwZk1YzlxhWV/IIVKzFO5psGMHvPoqTJoEEyaEtmHD4KCDkq1LRBKjcE+DzBj7ySfDP/9zsrWISEXQmHsa6HrtItKBwj0NdABVRDpQuKeBwl1EOlC4p4HCXUQ6ULingb5GT0Q6ULingQ6oikgHSoNqNXcuLFwY5nW5ARHpQGlQrR59FHbbre3LOAYNgpNOSrYmEakYCvdq1dICxxwD8+cnXYmIVCCNuVerlhYdQBWRLincq1Vzs8bYRaRLCvdqpZ67iOSgcK9WCncRyUHhXq0U7iKSg8K9WjU3K9xFpEs6Ilct7rwTfvzjtuWtWxXuItIlhXu1+PWvYd268IUcAIceCl/5SqIliUjl6jbczWwQ8FtgYNz+QXf/rpmNA+4FhgHPAF91951mNhC4E/g08C7w9+6+rkT1146WFhg9Gh58MOlKRKQK5DPmvgOY5O6fAsYDJ5vZscDVwHXufiCwBZgdt58NbHH3A4Dr4nbSWzqAKiIF6DbcPfhLXOwfbw5MAjLdyAXAjDg/PS4T1082MytaxbVKB1BFpAB5nS1jZn3NbBWwEVgGvAZsdfd4OUKagFFxfhTwJkBcvw34aCePOcfMGsysYdOmTb37KWpBS4s+kSoiecsr3N29xd3HA6OBo4FDOtssTjvrpfsuDe7z3b3e3evr6uryrbd2aVhGRApQ0Hnu7r4VeBw4FhhiZpmu5GhgfZxvAsYAxPUfATYXo9iapmEZESlAt+FuZnVmNiTO7wZ8HlgNPAacFjebBSyK84vjMnH9o+6+S89dCqRhGREpQD5pMRJYYGZ9Cf8M7nf3h8zsJeBeM/sh8CxwW9z+NuAuM2sk9Nh1MnZPPPEEnHde27cs/fnPcOyxydYkIlWj23B39+eBIztpX0sYf+/Y/gFwelGqq2UrVsDatXD22dC/f2ibMSP3fUREIr3Pr1SZL72+5ZbwdXoiIgXQhcMqVWY4RgdRRaQHFO6VKtNz10FUEekBhXulyoR7H71EIlI4JUel0oeWRKQXFO6VSh9aEpFeULhXKn1oSUR6QelRKV5/HT73OXjvvbD817/CwIHJ1iQiVUvhXikaG0PAf/nLsM8+oe3IXT47JiKSF4V7pcicHXPxxTBhQrK1iEjV05h7pdCHlkSkiBTulUIfWhKRIlK4V4pMuKvnLiJFoHCvFAp3ESkihXulULiLSBEp3CuFDqiKSBHp6F1SWlvDOe3r1oXlzfFrZhXuIlIECvekbNsGixbB4YfD/vvD2LEwZQqMG5d0ZSKSAgr3pGSGYb7+dbjwwmRrEZHU0Zh7UnReu4iUkMI9KZmeu8JdREpA4Z4UhbuIlJDCPSk6r11ESkjhnhT13EWkhJQs5fTSSzBzJnzwAezYEdrUcxeRElC4l9PKlfDsszB1Kuy5Jxx/PEycmHRVIpJCCvdyygzF3HQTfPzjydYiIqmmMfdy0ji7iJSJwr2cdHEwESkThXs56VOpIlImCvdy0rCMiJSJUqbU3noLtm4N801NYapwF5ESU8qU0vr1MGYMuLe19e8PAwYkV5OI1IRuw93MxgB3Ah8DWoH57n69mQ0D7gPGAuuAM9x9i5kZcD0wDdgOnOPuz5Sm/Ar37rsh2C++GI49NrTtu6/CXURKLp+eezNwibs/Y2Z7ASvNbBlwDvCIu19lZvOAecBlwFTgwHg7BrgpTmtPZox94kSYPj3ZWkSkpnR7QNXdN2R63u7+HrAaGAVMBxbEzRYAM+L8dOBOD34PDDGzkUWvvBro4mAikpCCzpYxs7HAkcByYG933wDhHwAwIm42Cngz625Nsa3jY80xswYza9i0aVPhlVcDnR0jIgnJO9zNbE/gv4Bvuvv/5tq0kzbfpcF9vrvXu3t9XV1dvmVUF/XcRSQheYW7mfUnBPt/uPt/x+a3M8MtcboxtjcBY7LuPhpYX5xyq4x67iKSkG7DPZ79chuw2t3/X9aqxcCsOD8LWJTV/jULjgW2ZYZvasIbb8D//E+4PfdcaFO4i0iZ5ZM6E4CvAn80s1Wx7QrgKuB+M5sNvAGcHtctIZwG2Ug4FfLcolZc6erroeMxhMGDk6lFRGpWt+Hu7k/R+Tg6wOROtndgbi/rql5btsCZZ8K58X/a4MFwxBHJ1iQiNUfjBcXW3AwHHABf+ELSlYhIDdOFw4qptTVMNcYuIglTuBeTTn0UkQqhcC8mnfooIhVC4V5M6rmLSIVQuBeTvkZPRCqExg96a+lSePjhMP/BB2GqYRkRSZhSqLe+9z1Yvhx23z0sDx0Khx2WaEkiIgr33vrwQ5gyBZYsSboSEZG/0Zh7bzU3a4xdRCqOwr23Wlo0xi4iFUfh3lstLeq5i0jFUbj3loZlRKQCKdx7S8MyIlKBFO69pWEZEalA6nIWaulSuOoq8Pi1sOvXK9xFpOKo516ohQvhySfD5X1bW+G44+BLX0q6KhGRdtRzL1RLC9TVwRNPJF2JiEiX1HMvVHOzDqCKSMVTuBdK4S4iVUDhXiiFu4hUAYV7oRTuIlIFFO6FUriLSBVQuHfn2Wdhjz2gT59wW7gQBgxIuioRkZzUBe3OmjWwfTucfz4MHx7aTjgh0ZJERLqjcO/Ojh1hesklsP/+ydYiIpInDct0Z+fOMNVQjIhUEfXcu/LBB9DQAC+9FJYV7iJSRRTuXbn66vDl1xDOjtljj0TLEREphMK9K1u2hEBfvBg+9jHYc8+kKxIRyZvCvSvNzTBoEEyalHQlIiIF0wHVruhLOESkinUb7mZ2u5ltNLMXstqGmdkyM1sTp0Nju5nZDWbWaGbPm9lRpSy+pPRJVBGpYvn03O8ATu7QNg94xN0PBB6JywBTgQPjbQ5wU3HKTIC++FpEqli34e7uvwU2d2ieDiyI8wuAGVntd3rwe2CImY0sVrFlpS++FpEq1tMx973dfQNAnI6I7aOAN7O2a4ptuzCzOWbWYGYNmzZt6mEZJaRhGRGpYsU+oGqdtHlnG7r7fHevd/f6urq6IpfRQ2edBUOHhtsDDyjcRaRq9TS93jazke6+IQ67bIztTcCYrO1GA+t7U2BZPfkk7LMPfP7zYflzn0u2HhGRHuppuC8GZgFXxemirPYLzexe4BhgW2b4pirs2AETJ8L11yddiYhIr3Qb7mb2n8AJwHAzawK+Swj1+81sNvAGcHrcfAkwDWgEtgPnlqDm0tm5U9eQEZFU6Dbc3f3MLlZN7mRbB+b2tqiye+klWLkS3n8fBg5MuhoRkV7TEUOAc86BFSvC/MjqPHNTRCSbwh1g61b44hfDWPvYsUlXIyLSa7Ud7q2t8PTTIdxHjIBx45KuSESkKGo73Jctg5PjlRUq5Vx7EZEiqN1wf/ttOPXUML9wIUyZkmw9IiJFVJvh3tICixaFSwyMHx/G23WRMBFJkdq8nvuFF8LXvx7mlyxRsItI6tReuG/eDDffDJ/4BDz2mE59FJFUqr1wz3zp9XHHwQknJFmJiEjJ1Fa4b98OP/1pmL/99mRrEREpodoK93nxC6O+/W3oU1s/uojUFguXg0lWfX29NzQ0lPZJWlvbDpzu3An9+5f2+URESszMVrp7fWfraqf7evPNYXrBBQp2EUm92gh3d5gbL1Z5zTXJ1iIiUga1Ee5XXBGmU6bA7rsnW4uISBnURrg/+miY3n9/snWIiJRJ+sPdPXxwacYMGDw46WpERMoi/eE+ezY0NsKwYUlXIiJSNukOd/dwiQGAyy5LthYRkTJKd7hfdhmsWwezZoVryYiI1IjqD/eJE2Hq1F3bN2xoO+3xu98tb00iIgmr7uu5t7bCk0+G+ZaW9pfuzXzD0ne+o6/PE5GaU9099w8/bJvfurX9uu3bYcgQ+P73y1uTiEgFSE+4v/562/y114YzZM44A8zKX5eISMKqO9x37mybX7UqTFevhksvDfPnnVf+mkREKkB1h/sTT7TNz54Nr7wCp5wSlu+6C445Jpm6REQSVt0HVF96KQy7ZC5bfPDBYTpyJMycmVxdIiIJq+6e++WXQ3NzOGvmtNPa2n/zm+RqEhGpANXdc8/+NqX582HgQPjSl+DQQ5OrSUSkAlR3uGcbOhTuvjvpKkREKkJ1D8uIiEinFO4iIilUknA3s5PN7BUzazSzeaV4DhER6VrRw93M+gI3AlOBQ4EzzUxHOEVEyqgUPfejgUZ3X+vuO4F7gekleB4REelCKcJ9FPBm1nJTbGvHzOaYWYOZNWzatKkEZYiI1K5ShHtnV+ryXRrc57t7vbvX19XVlaAMEZHaVYpwbwLGZC2PBtaX4HlERKQL5r5Lp7p3D2jWD3gVmAy8BawAznL3F3PcZxPwelfruzEceKeH9y0l1VWYSq0LKrc21VWYNNb1cXfvdOij6J9QdfdmM7sQeBjoC9yeK9jjfXo8LmNmDe5e39P7l4rqKkyl1gWVW5vqKkyt1VWSyw+4+xJgSSkeW0REuqdPqIqIpFAawn1+0gV0QXUVplLrgsqtTXUVpqbqKvoBVRERSV4aeu4iItKBwl1EJIWqOtyTuPqkma0zsz+a2Soza4htw8xsmZmtidOhsd3M7IZY3/NmdlTW48yK268xs1k9qON2M9toZi9ktRWtDjP7dPw5G+N9O/vkcb51fc/M3or7bJWZTctad3l8jlfMbEpWe6evrZmNM7Plsd77zGxAnnWNMbPHzGy1mb1oZt+ohH2Wo65E95mZDTKzP5jZc7Gu7+d6LDMbGJcb4/qxPa23h3XdYWZ/ytpf42N72X734337mtmzZvZQ4vvL3avyRjiH/jVgP2AA8BxwaBmedx0wvEPbj4F5cX4ecHWcnwb8inBJhmOB5bF9GLA2TofG+aEF1jEROAp4oRR1AH8Ajov3+RUwtRd1fQ+4tJNtD42v20BgXHw9++Z6bYH7ga/E+ZuB8/OsayRwVJzfi/BBu0OT3mc56kp0n8WfYc843x9YHvdDp48FXADcHOe/AtzX03p7WNcdwGmdbF+23/1434uBe4CHcu37cuyvau65V9LVJ6cDC+L8AmBGVvudHvweGGJmI4EpwDJ33+zuW4BlwMmFPKG7/xbYXIo64rrB7v60h9+4O7Meqyd1dWU6cK+773D3PwGNhNe109c29qAmAQ928jN2V9cGd38mzr8HrCZc0C7RfZajrq6UZZ/Fn/svcbF/vHmOx8rejw8Ck+NzF1RvL+rqStl+981sNHAKcGtczrXvS76/qjnc87r6ZAk4sNTMVprZnNi2t7tvgPDHCozopsZS1V6sOkbF+WLWd2F8W3y7xaGPHtT1UWCruzf3pq74FvhIQq+vYvZZh7og4X0WhxhWARsJ4fdajsf62/PH9dvicxf9b6BjXe6e2V//GvfXdWY2sGNdeT5/b17HnwDfAlrjcq59X/L9Vc3hntfVJ0tggrsfRfgykrlmNjHHtl3VWO7aC62j2PXdBOwPjAc2ANcmVZeZ7Qn8F/BNd//fXJuWs7ZO6kp8n7l7i7uPJ1z872jgkByPlVhdZnY4cDlwMPAZwlDLZeWsy8xOBTa6+8rs5hyPVfK6qjncE7n6pLuvj9ONwELCL/3b8e0ccbqxmxpLVXux6miK80Wpz93fjn+QrcAthH3Wk7reIbyt7tehPS9m1p8QoP/h7v8dmxPfZ53VVSn7LNayFXicMGbd1WP97fnj+o8QhudK9jeQVdfJcXjL3X0H8HN6vr96+jpOAP7OzNYRhkwmEXryye2vXAPylXwjXBdnLeGgQ+YAw2Elfs49gL2y5n9HGCu/hvYH5X4c50+h/cGcP3jbwZw/EQ7kDI3zw3pQz1jaH7gsWh2Eq3keS9tBpWm9qGtk1vxFhDFFgMNof/BoLeHAUZevLfAA7Q9QXZBnTUYYP/1Jh/ZE91mOuhLdZ0AdMCTO7wY8CZza1WMBc2l/gPD+ntbbw7pGZu3PnwBXJfG7H+9/Am0HVBPbX4mHdG9uhCPhrxLGAv+lDM+3X9ypzwEvZp6TMFb2CLAmTjO/JEb4PtnXgD8C9VmPdR7hYEkjcG4PavlPwtv1Dwn/1WcXsw6gHngh3uf/Ez/N3MO67orP+zywmPbB9S/xOV4h66yErl7b+Br8Idb7ADAwz7qOJ7yNfR5YFW/Tkt5nOepKdJ8BRwDPxud/Abgy12MBg+JyY1y/X0/r7WFdj8b99QJwN21n1JTtdz/r/ifQFu6J7S9dfkBEJIWqecxdRES6oHAXEUkhhbuISAop3EVEUkjhLiKSQgp3EZEUUriLiKTQ/wHVkOX1tsKVCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(returns, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7yJS829tGg6r"
   },
   "source": [
    "Comparando com a performance de uma DQN sem $n$-step:\n",
    "\n",
    "![DQN](https://media.discordapp.net/attachments/688564171973197869/752599003962540094/unknown.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNLK9rllIVTx"
   },
   "source": [
    "## Testando nosso Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gt3TIhQc_5U-"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def evaluate(agent, env, episodes=10):\n",
    "    total_reward = 0\n",
    "    episode_returns = deque(maxlen=episodes)\n",
    "    \n",
    "    episode = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while episode < episodes:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)        \n",
    "       \n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            episode_returns.append(total_reward)\n",
    "            episode += 1\n",
    "            next_state = env.reset()\n",
    "\n",
    "        total_reward *= 1 - done\n",
    "        state = next_state\n",
    "\n",
    "        ratio = math.ceil(100 * episode / episodes)\n",
    "        \n",
    "        print(f\"\\r[{ratio:3d}%] episode = {episode:3d}, avg_return = {np.mean(episode_returns):10.4f}\", end=\"\")\n",
    "\n",
    "    return np.mean(episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91593,
     "status": "ok",
     "timestamp": 1599511820292,
     "user": {
      "displayName": "Bernardo Coutinho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjbcuim7oGIm-uXpKRCJxDYg0Nhguq2a4_xKQcpjw=s64",
      "userId": "08343358744938767290"
     },
     "user_tz": 180
    },
    "id": "dLCaC4Hy_6-5",
    "outputId": "cbb3f08a-900b-4ec4-f040-8138543e2e47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100%] episode =  10, avg_return =   500.0000"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(agent, env, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NcxQb1uBjk4"
   },
   "source": [
    "## Referências\n",
    "\n",
    "\"Reinforcement Learning: An Introduction\"\n",
    "\n",
    "\"Rainbow: Combining Improvements in Deep Reinforcement Learning\": https://arxiv.org/pdf/1710.02298.pdf\n",
    "\n",
    "\"Understanding Multi-Step Deep Reinforcement\n",
    "Learning: A Systematic Study of the DQN Target\": https://arxiv.org/pdf/1901.07510.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "N-Step DQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
