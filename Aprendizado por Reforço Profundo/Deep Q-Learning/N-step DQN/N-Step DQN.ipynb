{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWyEwLwn10hd"
   },
   "source": [
    "# N-Step DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-R_VyNqyAoq"
   },
   "source": [
    "# Conceito\n",
    "Em algoritmos de Monte Carlo, o nosso modelo ¨aprende¨ em cada transição com base em **toda** a sequência de recompensas em um episódio, ou seja, o retorno completo ($G_{t}$). Já em one-step Temporal Diference, o aprendizado é feito observando apenas uma recompensa no futuro ($R_{t + 1}$) e aproximamos o restante do retorno ($G_{t+1}$) como sendo o valor do próximo estado ($V(S_{t + 1})$).\n",
    "\n",
    "\n",
    "Agora, em N-step, tomamos uma abordagem intermediária a esses dois algoritmos. Não chegamos a utilizar a totalidade do retorno ($G_{t}$), mas **n** passos a frente do presente (**t**). Dessa forma, obtemos a seguinte expressão:\n",
    "\n",
    "$$ G_t = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{T -t -1} R_{T} $$\n",
    "\n",
    "\n",
    "$$ G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n V_{t +n -1}(S_{t + n}) $$\n",
    "\n",
    "\n",
    "\n",
    "onde $G_t$ representa o retorno completo (usado em Monte Carlo) e $G_{t:t + n}$ a aproximação do retorno com n-step, utilizando bootstraping do Valor do estado no instante t + n ($V_{t +n -1}(S_{t + n})$).\n",
    "\n",
    "![N-Step](https://media.discordapp.net/attachments/688564171973197869/752614671974006844/unknown.png)\n",
    "\n",
    "Vale notar que, no início do episódio, o agente não possui todas as experiências necessárias para fazer a estimativa do retorno. Para contornar isso, fazemos mudanças no replay buffer (explicadas mais a frente) para possibilitar o cálculo.\n",
    "\n",
    "## Sarsa para $n$-Step\n",
    "Agora que temos uma noção de como o $n$-step funciona, podemos nos preocupar em como nosso agente pode fazer uma escolha se baseando nesse processo. Para isso, mudamos nossa previsão para que ela preveja ações e não estados. Com esse objetivo, chegamos às seguintes expressões:\n",
    "\n",
    "$$G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n Q_{t +n -1}(S_{t + n}, A_{t + n}) $$\n",
    "\n",
    "$$Q_{t +n}(S_{t}, A_{t}) = Q_{t +n -1}(S_{t}, A_{t}) + \\alpha [G_{t:t + n} - Q_{t +n -1}(S_{t}, A_{t})]$$\n",
    "\n",
    "\n",
    "Tendo esse algoritmo, só precisamos de uma política $\\pi$, por exemplo, $\\varepsilon$-greedy.\n",
    "\n",
    "## *Off-policy* $n$-step\n",
    "Para fazermos uma implementação tecnicamente ¨completa¨ de $n$-step *off-policy*, seria necessário implementar a funcionalidade de *importance sampling* no $n$-step buffer. Isso necessariamente requer que avaliemos a *importance sampling ratio*($\\rho_{t:t+n-1}$) para cada passo a frente ($n$) que queremos avaliar.\n",
    "\n",
    "No entanto, com base nos artigos:\n",
    "> \"Rainbow: Combining Improvements in Deep Reinforcement Learning\": https://arxiv.org/pdf/1710.02298.pdf\n",
    "\n",
    "> \"Understanding Multi-Step Deep Reinforcement\n",
    "Learning: A Systematic Study of the DQN Target\": https://arxiv.org/pdf/1901.07510.pdf\n",
    "\n",
    "decidimos por não integrar *importance sampling* ao modelo pois, para valores de $n$ pequenos comumente como $n=3$, não aparentam existir impactos significativos da falta desse recurso. Além disso, para valores de $n$ maiores, o treinamento seria mais demorado, sem benefícios claros.\n",
    "\n",
    "Logo, a única alteração feita para off-policy em relação a on-policy $n$-step, é utilizar o $max_{a'} Q(S_{t+n}, a')$ ao invés do $Q(S_{t+n}, A_{t+n})$.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "$$G_{t:t + n} = R_{t+1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^{n -1} R_{t + n} + \\gamma^n max_{a'} Q_{t +n -1}(S_{t + n}, a') $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zkpagykkFj3"
   },
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgCJLiwZkId-"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfdnGQ1CkKF2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUP-IhpJkt73"
   },
   "source": [
    "## N-Step Buffer\n",
    "\n",
    "Para implementar nosso algoritmo n-step, vamos modificar o ReplayBuffer para retornar o n-step reward ($R_{t:t+n}$) em vez da recompensa no instante seguinte ($R_{t+1}$). \n",
    "\n",
    "$$R_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... + \\gamma^{n-1}R_{t + n}$$\n",
    "\n",
    "Também alteraremos o estado seguinte de $S_{t+1}$ para $S_{t+n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCnTllt7oCi6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NStepBuffer:\n",
    "    \"\"\"Experience Replay Buffer com n-step para DQNs.\"\"\"\n",
    "    def __init__(self, max_length, observation_space, gamma, n_step=3):\n",
    "        \"\"\"Cria um Replay Buffer.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        max_length: int\n",
    "            Tamanho máximo do Replay Buffer.\n",
    "        observation_space: int\n",
    "            Tamanho do espaço de observação.\n",
    "        gamma: float\n",
    "            Fator de desconto.\n",
    "        n_step: int\n",
    "            Timesteps considerados.\n",
    "        \"\"\"\n",
    "        self.gamma, self.n_step, self.episode_step = gamma, n_step, 0\n",
    "        self.index, self.size, self.max_length = 0, 0, max_length\n",
    "\n",
    "        self.states = np.zeros((max_length, observation_space), dtype=np.float32)\n",
    "        self.actions = np.zeros((max_length), dtype=np.int32)\n",
    "        self.n_step_rewards = np.zeros((max_length), dtype=np.float32)\n",
    "        self.next_states = np.zeros((max_length, observation_space), dtype=np.float32)\n",
    "        self.dones = np.zeros((max_length), dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Retorna o tamanho do buffer.\"\"\"\n",
    "        return self.size\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Adiciona uma experiência ao Replay Buffer.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        state: np.array\n",
    "            Estado da transição.\n",
    "        action: int\n",
    "            Ação tomada.\n",
    "        reward: float\n",
    "            Recompensa recebida.\n",
    "        state: np.array\n",
    "            Estado seguinte.\n",
    "        done: int\n",
    "            Flag indicando se o episódio acabou.\n",
    "        \"\"\"\n",
    "        self.states[self.index] = state\n",
    "        self.actions[self.index] = action\n",
    "        self.dones[self.index] = done\n",
    "        \n",
    "        # Soma o reward atual aos n instantes passados\n",
    "        for n in range(self.n_step):\n",
    "            if self.episode_step - n >= 0:\n",
    "                self.n_step_rewards[self.index - n] += reward * self.gamma**n\n",
    "        \n",
    "        if done:\n",
    "            self.episode_step = 0\n",
    "            self.dones[self.index - self.n_step:self.index] = done\n",
    "        \n",
    "        if self.episode_step - self.n_step >= 0:\n",
    "            self.next_states[self.index - self.n_step] = next_state\n",
    "        \n",
    "        self.episode_step += 1\n",
    "        self.index = (self.index + 1) % self.max_length\n",
    "        if self.size < self.max_length:\n",
    "            self.size = self.index\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Retorna um batch de experiências.\n",
    "        \n",
    "        Parâmetros\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            Tamanho do batch de experiências.\n",
    "\n",
    "        Retorna\n",
    "        -------\n",
    "        states: np.array\n",
    "            Batch de estados.\n",
    "        actions: np.array\n",
    "            Batch de ações.\n",
    "        n_step_rewards: np.array\n",
    "            Batch de retornos.\n",
    "        next_states: np.array\n",
    "            Batch de estados seguintes.\n",
    "        dones: np.array\n",
    "            Batch de flags indicando se o episódio acabou.\n",
    "        \"\"\"\n",
    "        # Escolhe índices aleatoriamente do Replay Buffer\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        return (self.states[idxs], self.actions[idxs], self.n_step_rewards[idxs], self.next_states[idxs], self.dones[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKyLGJFyoFjR"
   },
   "source": [
    "## Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4F49tlkvPfq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWrD9V1kvWth"
   },
   "source": [
    "## Agente DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMXF17a7oIqJ"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Uma classe que cria um agente DQN que utiliza NStepBuffer como memória\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space, \n",
    "                 lr=7e-4, \n",
    "                 gamma=0.99,\n",
    "                 max_memory=100000,\n",
    "                 epsilon_init=0.5,\n",
    "                 epsilon_decay=0.9995,\n",
    "                 min_epsilon=0.01,\n",
    "                 n_step=3):\n",
    "        \"\"\"\n",
    "        Inicializa o agente com os parâmetros dados\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.n_step = n_step\n",
    "        self.memory = NStepBuffer(max_memory, observation_space.shape[0], gamma, n_step)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.epsilon = epsilon_init\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "        self.dqn = Network(observation_space.shape[0], action_space.n).to(self.device)\n",
    "\n",
    "        self.optimizer  = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "            return action\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            action = self.dqn.forward(state).argmax(dim=-1)\n",
    "            action = action.cpu().numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.update(state, action, reward, new_state, done)\n",
    "\n",
    "    def train(self, batch_size=32, epochs=1):\n",
    "        # Se temos menos experiências que o batch size\n",
    "        # não começamos o treinamento\n",
    "        if batch_size > self.memory.size:\n",
    "          N-Step Buffer  return\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Pegamos uma amostra das nossas experiências para treinamento\n",
    "            (states, actions, rewards, next_states, dones) = self.memory.sample(batch_size)\n",
    "\n",
    "            # Transformar nossas experiências em tensores\n",
    "            states = torch.as_tensor(states).to(self.device)\n",
    "            actions = torch.as_tensor(actions).to(self.device).unsqueeze(-1)\n",
    "            rewards = torch.as_tensor(rewards).to(self.device).unsqueeze(-1)\n",
    "            next_states = torch.as_tensor(next_states).to(self.device)\n",
    "            dones = torch.as_tensor(dones).to(self.device).unsqueeze(-1)\n",
    "\n",
    "            q = self.dqn.forward(states).gather(-1, actions.long())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q2 = self.dqn.forward(next_states).max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "                target = (rewards + (1 - dones) * (self.gamma ** self.n_step) * q2).to(self.device)\n",
    "\n",
    "            loss = F.mse_loss(q, target)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QS47MlN8pga6"
   },
   "source": [
    "### Definição de parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8vR8vZS1_A7"
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ARH8j14pfFT"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_INIT = 0.7\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.999\n",
    "MAX_MEMORY = 100000\n",
    "OBS_SPACE = env.observation_space\n",
    "ACT_SPACE = env.action_space\n",
    "N_STEP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwOrceIGpm_N"
   },
   "source": [
    "### Criando a DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjfkvnvCpxLq"
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(observation_space=OBS_SPACE, \n",
    "                 action_space=ACT_SPACE, \n",
    "                 lr=7e-4, \n",
    "                 gamma=GAMMA, \n",
    "                 max_memory=MAX_MEMORY,\n",
    "                 epsilon_init=EPS_INIT,\n",
    "                 epsilon_decay=EPS_DECAY,\n",
    "                 min_epsilon=EPS_END,\n",
    "                 n_step=N_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmYPxiROsQ9v"
   },
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLk74CbAIffE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import deque\n",
    "\n",
    "def train(agent, env, total_timesteps):\n",
    "    total_reward = 0\n",
    "    episode_returns = deque(maxlen=20)\n",
    "    avg_returns = []\n",
    "\n",
    "    state = env.reset()\n",
    "    timestep = 0\n",
    "    episode = 0\n",
    "\n",
    "    while timestep < total_timesteps:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.train()\n",
    "        \n",
    "        timestep += 1\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "        if done:\n",
    "            episode_returns.append(total_reward)\n",
    "            episode += 1\n",
    "            next_state = env.reset()\n",
    "\n",
    "        if any(G for G in episode_returns):\n",
    "            avg_returns.append(np.mean(episode_returns))\n",
    "\n",
    "        total_reward *= 1 - done\n",
    "        state = next_state\n",
    "\n",
    "        ratio = math.ceil(100 * timestep / total_timesteps)\n",
    "\n",
    "        avg_return = avg_returns[-1] if avg_returns else np.nan\n",
    "        \n",
    "        print(f\"\\r[{ratio:3d}%] timestep = {timestep}/{total_timesteps}, episode = {episode:3d}, avg_return = {avg_return:10.4f}\", end=\"\")\n",
    "\n",
    "    return avg_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 87674,
     "status": "ok",
     "timestamp": 1599511816287,
     "user": {
      "displayName": "Bernardo Coutinho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjbcuim7oGIm-uXpKRCJxDYg0Nhguq2a4_xKQcpjw=s64",
      "userId": "08343358744938767290"
     },
     "user_tz": 180
    },
    "id": "-vVnEAzLI4WX",
    "outputId": "9309817a-2819-4363-9cb6-22d36812f580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100%] timestep = 40000/40000, episode = 137, avg_return =   500.0000"
     ]
    }
   ],
   "source": [
    "returns = train(agent, env, 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88021,
     "status": "ok",
     "timestamp": 1599511816666,
     "user": {
      "displayName": "Bernardo Coutinho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjbcuim7oGIm-uXpKRCJxDYg0Nhguq2a4_xKQcpjw=s64",
      "userId": "08343358744938767290"
     },
     "user_tz": 180
    },
    "id": "2VQQJa21NdlO",
    "outputId": "0714bb3a-52b0-4dbe-edf2-7fac113b02dc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(returns, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7yJS829tGg6r"
   },
   "source": [
    "Comparando com a performance de uma DQN sem $n$-step:\n",
    "\n",
    "![DQN](https://media.discordapp.net/attachments/688564171973197869/752599003962540094/unknown.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNLK9rllIVTx"
   },
   "source": [
    "## Testando nosso Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gt3TIhQc_5U-"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def evaluate(agent, env, episodes=10):\n",
    "    total_reward = 0\n",
    "    episode_returns = deque(maxlen=episodes)\n",
    "    \n",
    "    episode = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while episode < episodes:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)        \n",
    "       \n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            episode_returns.append(total_reward)\n",
    "            episode += 1\n",
    "            next_state = env.reset()\n",
    "\n",
    "        total_reward *= 1 - done\n",
    "        state = next_state\n",
    "\n",
    "        ratio = math.ceil(100 * episode / episodes)\n",
    "        \n",
    "        print(f\"\\r[{ratio:3d}%] episode = {episode:3d}, avg_return = {np.mean(episode_returns):10.4f}\", end=\"\")\n",
    "\n",
    "    return np.mean(episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91593,
     "status": "ok",
     "timestamp": 1599511820292,
     "user": {
      "displayName": "Bernardo Coutinho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjbcuim7oGIm-uXpKRCJxDYg0Nhguq2a4_xKQcpjw=s64",
      "userId": "08343358744938767290"
     },
     "user_tz": 180
    },
    "id": "dLCaC4Hy_6-5",
    "outputId": "cbb3f08a-900b-4ec4-f040-8138543e2e47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100%] episode =  10, avg_return =   475.7000"
     ]
    },
    {
     "data": {
      "text/plain": [
       "475.7"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(agent, env, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NcxQb1uBjk4"
   },
   "source": [
    "## Referências\n",
    "\n",
    "\"Reinforcement Learning: An Introduction\"\n",
    "\n",
    "\"Rainbow: Combining Improvements in Deep Reinforcement Learning\": https://arxiv.org/pdf/1710.02298.pdf\n",
    "\n",
    "\"Understanding Multi-Step Deep Reinforcement\n",
    "Learning: A Systematic Study of the DQN Target\": https://arxiv.org/pdf/1901.07510.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "N-Step DQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
