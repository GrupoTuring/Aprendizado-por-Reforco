{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWyEwLwn10hd",
        "colab_type": "text"
      },
      "source": [
        "# Implementação de uma Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpETKTzX10Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zkpagykkFj3",
        "colab_type": "text"
      },
      "source": [
        "## Importações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgCJLiwZkId-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from PIL import Image\n",
        "from itertools import count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfdnGQ1CkKF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUP-IhpJkt73",
        "colab_type": "text"
      },
      "source": [
        "## Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCnTllt7oCi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Experience Replay Buffer para DQNs.\"\"\"\n",
        "    def __init__(self, max_length, observation_space):\n",
        "        \"\"\"Cria um Replay Buffer.\n",
        "\n",
        "        Parâmetros\n",
        "        ----------\n",
        "        max_length: int\n",
        "            Tamanho máximo do Replay Buffer.\n",
        "        observation_space: int\n",
        "            Tamanho do espaço de observação.\n",
        "        \"\"\"\n",
        "        self.index, self.size, self.max_length = 0, 0, max_length\n",
        "\n",
        "        self.states = np.zeros((max_length, observation_space), dtype=np.float32)\n",
        "        self.actions = np.zeros((max_length), dtype=np.int32)\n",
        "        self.rewards = np.zeros((max_length), dtype=np.float32)\n",
        "        self.next_states = np.zeros((max_length, observation_space), dtype=np.float32)\n",
        "        self.dones = np.zeros((max_length), dtype=np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Retorna o tamanho do buffer.\"\"\"\n",
        "        return self.size\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Adiciona uma experiência ao Replay Buffer.\n",
        "\n",
        "        Parâmetros\n",
        "        ----------\n",
        "        state: np.array\n",
        "            Estado da transição.\n",
        "        action: int\n",
        "            Ação tomada.\n",
        "        reward: float\n",
        "            Recompensa recebida.\n",
        "        state: np.array\n",
        "            Estado seguinte.\n",
        "        done: int\n",
        "            Flag indicando se o episódio acabou.\n",
        "        \"\"\"\n",
        "        self.states[self.index] = state\n",
        "        self.actions[self.index] = action\n",
        "        self.rewards[self.index] = reward\n",
        "        self.next_states[self.index] = next_state\n",
        "        self.dones[self.index] = done\n",
        "        \n",
        "        self.index = (self.index + 1) % self.max_length\n",
        "        if self.size < self.max_length:\n",
        "            self.size = self.index\n",
        "            \n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Retorna um batch de experiências.\n",
        "        \n",
        "        Parâmetros\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            Tamanho do batch de experiências.\n",
        "\n",
        "        Retorna\n",
        "        -------\n",
        "        states: np.array\n",
        "            Batch de estados.\n",
        "        actions: np.array\n",
        "            Batch de ações.\n",
        "        rewards: np.array\n",
        "            Batch de recompensas.\n",
        "        next_states: np.array\n",
        "            Batch de estados seguintes.\n",
        "        dones: np.array\n",
        "            Batch de flags indicando se o episódio acabou.\n",
        "        \"\"\"\n",
        "        # Escolhe índices aleatoriamente do Replay Buffer\n",
        "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "        return (self.states[idxs], self.actions[idxs], self.rewards[idxs], self.next_states[idxs], self.dones[idxs])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKyLGJFyoFjR",
        "colab_type": "text"
      },
      "source": [
        "## Rede Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4F49tlkvPfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LinearNetwork(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(LinearNetwork, self).__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_dim, 64), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64), \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(64, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hxjmX4oxLh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvNetwork(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, h, w, outputs):\n",
        "        \"\"\"\n",
        "        Creates the ConvNet \n",
        "        \n",
        "        h: int\n",
        "        The screen height\n",
        "        \n",
        "        w: int\n",
        "        The screen width\n",
        "        \n",
        "        outputs: int\n",
        "        The number of actions for the agent\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        # Calculate the output size of conv to be the input of linear\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        \n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        self.head = nn.Linear(linear_input_size, outputs)   \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Pass a batch through the ConvNet\n",
        "        \"\"\"\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWrD9V1kvWth",
        "colab_type": "text"
      },
      "source": [
        "## Agente DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMXF17a7oIqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNagent:\n",
        "    \"\"\"\n",
        "    Uma classe que cria um agente DQN que utiliza ReplayBuffer como memória\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 observation_space, \n",
        "                 action_space, \n",
        "                 lr=3e-4, \n",
        "                 gamma=0.99, \n",
        "                 max_memory=100000,\n",
        "                 epsilon_init=0.5,\n",
        "                 epsilon_decay=0.9995,\n",
        "                 min_epsilon=0.01,\n",
        "                 network='linear'):\n",
        "      \n",
        "        \"\"\"\n",
        "        Inicializa o agente com os parâmetros dados\n",
        "\n",
        "        \"\"\"\n",
        "      \n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.memory = ReplayBuffer(max_memory, observation_space.shape[0])\n",
        "        self.action_space = action_space\n",
        "\n",
        "        self.epsilon = epsilon_init\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "        if network == 'linear':\n",
        "          self.dqn = LinearNetwork(observation_space.shape[0], action_space.n).to(self.device)\n",
        "        \n",
        "        elif network == 'conv': \n",
        "          h = observation_space.shape[0]\n",
        "          w = observation_space.shape[1]\n",
        "          in_channels = observation_space.shape[2]\n",
        "          outputs = action_space.n\n",
        "          self.dqn = ConvNetwork(in_channels, h, w, outputs)\n",
        "\n",
        "        self.optimizer  = optim.Adam(self.dqn.parameters(), lr=lr)\n",
        "\n",
        "    def act(self, state):\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
        "\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = self.action_space.sample()\n",
        "            return action\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state).to(self.device)\n",
        "            action = self.dqn.forward(state).argmax(dim=-1)\n",
        "            action = action.cpu().numpy()\n",
        "\n",
        "        return action\n",
        "\n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "        self.memory.update(state, action, reward, new_state, done)\n",
        "\n",
        "    def train(self, batch_size=128, epochs=1):\n",
        "        # Se temos menos experiências que o batch size\n",
        "        # não começamos o treinamento\n",
        "        if batch_size > self.memory.size:\n",
        "            return\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Pegamos uma amostra das nossas experiências para treinamento\n",
        "            (states, actions, rewards, next_states, dones) = self.memory.sample(batch_size)\n",
        "\n",
        "            # Transformar nossas experiências em tensores\n",
        "            states = torch.as_tensor(states).to(self.device)\n",
        "            actions = torch.as_tensor(actions).to(self.device).unsqueeze(-1)\n",
        "            rewards = torch.as_tensor(rewards).to(self.device).unsqueeze(-1)\n",
        "            next_states = torch.as_tensor(next_states).to(self.device)\n",
        "            dones = torch.as_tensor(dones).to(self.device).unsqueeze(-1)\n",
        "\n",
        "            q = self.dqn.forward(states).gather(-1, actions.long())\n",
        "\n",
        "            with torch.no_grad():\n",
        "                q2 = self.dqn.forward(next_states).max(dim=-1, keepdim=True)[0]\n",
        "\n",
        "                target = (rewards + (1 - dones) * self.gamma * q2).to(self.device)\n",
        "\n",
        "            loss = F.mse_loss(q, target)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS47MlN8pga6",
        "colab_type": "text"
      },
      "source": [
        "### Definição de parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8vR8vZS1_A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = 'Acrobot-v1'\n",
        "env = gym.make(env_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ARH8j14pfFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_INIT = 0.7\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 0.999\n",
        "MAX_MEMORY = 100000\n",
        "NETWORK = 'linear'\n",
        "OBS_SPACE = env.observation_space\n",
        "ACT_SPACE = env.action_space"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwOrceIGpm_N",
        "colab_type": "text"
      },
      "source": [
        "### Criando a DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjfkvnvCpxLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqn_net = DQNagent(observation_space=OBS_SPACE, \n",
        "                   action_space=ACT_SPACE, \n",
        "                   lr=3e-4, \n",
        "                   gamma=GAMMA, \n",
        "                   max_memory=MAX_MEMORY,\n",
        "                   epsilon_init=EPS_INIT,\n",
        "                   epsilon_decay=EPS_DECAY,\n",
        "                   min_epsilon=EPS_END,\n",
        "                   network=NETWORK)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmYPxiROsQ9v",
        "colab_type": "text"
      },
      "source": [
        "## Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BNgn8pnrrdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}