# üß† Aprendizado por Refor√ßo Profundo

O **Aprendizado por Refor√ßo Profundo** √© a combina√ß√£o do Aprendizado por Refor√ßo com o Aprendizado Profundo (Deep Learning). Nesta √°rea, s√£o utilizadas **redes neurais**, potentes modelos de reconhecimento de padr√µes, para aprender e estimar importantes fun√ß√µes, como a pol√≠tica √≥tima de um agente ou a fun√ß√£o de valor de um problema.

Como todos os algoritmos ser√£o constru√≠dos a partir de redes neurais, √© recomendado utilizar algum framework de Deep Learning, como o PyTorch ou o Tensorflow. Neste reposit√≥rio, todas as redes ser√£o criadas usando o PyTorch. Caso n√£o esteja muito familiarizado com esta biblioteca, recomendamos os seguintes materiais:

 - **[üë®‚Äçüè´ Workshop de Redes Neurais com PyTorch](https://www.youtube.com/watch?v=DVtp6TnZ-fc)**
 - **[üìÉ Construindo uma Rede Neural do zero | Pytorch](https://medium.com/turing-talks/construindo-uma-rede-neural-do-zero-pytorch-671ee06fbbe1)**

## [Deep Q-Learning](Deep%20Q-Learning)

Um dos algoritmos mais comuns em aprendizado por refor√ßo, quando se trata de aprendizado profundo, √© o **Deep Q-Learning**, ou DQN na forma abreviada. DQN √© a vers√£o profunda do algoritmo cl√°ssico de Q-Learning, em que agora utilizamos uma rede neural que recebe nosso estado e devolve os Q-Valores para cada poss√≠vel a√ß√£o do nosso agente.

DQN's s√£o alternativas muito boas ao Q-Learning quando precisamos lidar com problemas que cont√™m diversos estados, tornando a computa√ß√£o mais leve e o aprendizado mais eficaz.

## [Policy Gradient](Policy%20Gradient)

Quase todos os m√©todos vistos anteriormente ([m√©todos tabulares](../Aprendizado%20por%20Refor√ßo$20Cl√°ssico) e [DQN](#Deep%20Q-Learning)) estimam a fun√ß√£o de valor √≥tima _Q_\*(_s_,_a_) e, a partir de de _Q_\*, obt√©m uma pol√≠tica (&epsilon;-)gulosa. Os m√©todos de policy gradient tem uma proposta alternativa: eles estimam diretamente a pol√≠tica √≥tima.

## [Actor Critic](Actor-Critic)

Os Actor Critics s√£o algoritmos de estado da arte que combinam estimadores de fun√ß√£o de valor, como a DQN, com estimadores de pol√≠tica √≥tima, como o Policy Gradient. Dessa forma, esses algoritmos tendem a ser bem mais robustos do que modelos individuais.
