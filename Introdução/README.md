# üë©‚Äçüè´ Introdu√ß√£o

> Se voc√™ se interessar por uma introdu√ß√£o mais pr√°tica ao assunto, temos um [**Workshop Introdut√≥rio de Aprendizado por Refor√ßo**](https://youtu.be/FxcWqI-l29E) dispon√≠vel gratuitamente no YouTube!

## O que √© Aprendizado por Refor√ßo?

O **Aprendizado por Refor√ßo** √© uma sub√°rea do Aprendizado de M√°quina que estuda programas que aprendem a realizar tarefas complexas por ***tentativa*** e ***erro***, a partir do feedback que ele recebe de suas a√ß√µes.

Esse tipo de aprendizado se assemelha muito com o processo de aprendizado intuitivo dos seres humanos, no qual o indiv√≠duo experimenta algo, e com base na resposta desse experimento, decide se ele vale a pena ou n√£o. Quando uma crian√ßa encosta o dedo em uma superf√≠cie quente, por exemplo, ela recebe uma resposta negativa, e n√£o repete a mesma a√ß√£o novamente.

As t√©cnicas de Aprendizado por Refor√ßo s√£o muito poderosas, j√° que conseguem gerar comportamentos extremamente complexos, muito dif√≠ceis de serem programados, como fazer um rob√¥ caminhar ou at√© correr sem cair.

## Conceitos Importantes

Para poder entender melhor os algoritmos de Aprendizado por Refor√ßo, primeiro √© preciso entender alguns **conceitos importantes**.

### Agente

O **Agente** √© o nosso algoritmo: √© a entidade que **toma as decis√µes** de como agir, e que **aprende** o melhor comportamento para uma determinada situa√ß√£o com base no feedback de a√ß√µes passadas.

O papel do agente √© an√°logo ao do nosso c√©rebro, √© ele que processa as informa√ß√µes e decide as pr√≥ximas a√ß√µes a se tomar.

### Ambiente

No Aprendizado por Refor√ßo, o **Ambiente** √© o espa√ßo que representa o nosso problema: o mundo com o qual o agente pode interagir, e no qual ele deve se basear para a tomada de decis√µes.

Em uma partida de xadrez, o Ambiente √© bem simples: ele consiste no conjunto das pe√ßas e do tabuleiro.

![Xadrez](https://miro.medium.com/max/625/0*7-LOSdL2eOEAd9R7)

### Estado

O **Estado** se refere √†s condi√ß√µes do Agente e do Ambiente em um determinado instante. No xadrez, por exemplo, o Estado √© a configura√ß√£o atual do tabuleiro, ou seja, a posi√ß√£o de todas as pe√ßas naquele turno. Essa informa√ß√£o √© repassada ao Agente, e √© a partir dela que o Agente deve tomar suas decis√µes.

![Estado](https://miro.medium.com/max/304/0*6PVAVr2qsP3oBO6k.jpg)

Uma observa√ß√£o interessante acerca do Estado √© que ele n√£o precisa ser informado em sua totalidade: o Agente, em muitos casos, n√£o possui a informa√ß√£o completa do Ambiente, cabendo a ele deduzir o restante do Estado. Basta pensar em jogos de estrat√©gia como Age of Empires ou Starcraft, em que a posi√ß√£o dos inimigos √© encoberta at√© eles entrarem no seu campo de vis√£o.

![Age of Empires](https://miro.medium.com/max/575/0*VQdhNC0eeBLFykU_)

### A√ß√£o

O conceito de **A√ß√£o** √© bem simples: s√£o os **comandos** que o Agente pode escolher em um instante para interagir com o Ambiente. No xadrez, uma A√ß√£o √© equivalente a um movimento.

![A√ß√£o](https://miro.medium.com/max/531/0*JoUU1spyVgWGDs0D.gif)

J√° o **Espa√ßo de A√ß√£o** √© o conjunto de todos as a√ß√µes poss√≠veis. Ou seja, no xadrez, nosso Espa√ßo de A√ß√£o √© o conjunto de todas as a√ß√µes poss√≠veis.

![Espa√ßo de A√ß√£o](https://img.itch.zone/aW1nLzEzODQ1NzQuZ2lm/original/8hWPC8.gif)

Uma observa√ß√£o interessante √© que o Espa√ßo de A√ß√µes pode ser discreto ou cont√≠nuo. No caso do xadrez, o Espa√ßo de A√ß√µes √© discreto, j√° que existe uma quantidade finita de movimentos que podem ser feitos. Entretanto, no caso de um carro aut√¥nomo, o Espa√ßo de A√ß√µes √© cont√≠nuo, visto que podemos mandar qualquer velocidade para as rodas. Lidar com espa√ßos cont√≠nuos √© consideravelmente mais complexo, e n√£o s√£o todos os algoritmos de RL que conseguem resolver esse tipo de problema.

### Recompensa

A cada a√ß√£o tomada, o Ambiente devolve um **feedback** ao Agente relatando a **efetividade** daquela a√ß√£o. Esse feedback √© denominado **Recompensa**, e √© representado por um n√∫mero, positivo, negativo ou nulo, tal qual uma pontua√ß√£o em um jogo.

No caso do Pac-Man, o agente recebe uma recompensa positiva para cada fantasma / bolota comidos, e uma recompensa negativa quando perde uma vida.

![Recompensa](https://miro.medium.com/max/875/0*cLrwq7tnLpqGDpbg.gif)

### Retorno

Como dissemos antes, o objetivo do nosso Agente √© maximizar a **soma de todas as recompensas**.

O **Retorno** (<img src="https://latex.codecogs.com/svg.latex?G_t" title="G_t" />) √© justamente esse conceito: ele representa o valor da recompensa total a partir de um determinado instante.

Ou seja, se a **Recompensa** era equivalente aos **Pontos** de um jogo, o **Retorno** √© an√°logo ao **Score Total**.

![Retorno](https://miro.medium.com/max/475/0*AiSIzhhdgaVXH2BG.png)

O Retorno √© obtido a partir da seguinte equa√ß√£o:

<img src="https://latex.codecogs.com/svg.latex?G_t&space;=&space;R_{t&plus;1}&space;&plus;&space;\gamma&space;R_{t&plus;2}&space;&plus;&space;\gamma^2&space;R_{t&plus;3}&space;&plus;&space;..." title="G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ..." />

No c√°lculo do Retorno, somamos todas as recompensas multiplicadas por um  **fator de desconto** (Œ≥) entre 0 e 1. Esse fator faz com que as recompensas mais para o futuro se tornem para vez menores, fazendo o Retorno convergir para um valor real.

Um Œ≥ pr√≥ximo de 1 significa que nosso Retorno leva mais em conta as **recompensas futuras**. Um Œ≥ pr√≥ximo de 0 significa que levamos mais em conta **recompensas recentes**.

### Pol√≠tica (œÄ)

A **Pol√≠tica** se refere ao processo de decis√£o do Agente: √© o algoritmo que ele usa para escolher uma a√ß√£o para cada estado. Ou seja, √© uma fun√ß√£o que recebe o **estado atual** e retorna a probabilidade de cada a√ß√£o ser escolhida.

Vamos pensar no caso do **Pedra, Papel ou Tesoura**: escolher qualquer uma das tr√™s a√ß√µes aleatoriamente √© um tipo de pol√≠tica, cuja probabilidade de cada a√ß√£o √© 1/3 para qualquer estado. Uma outra pol√≠tica poss√≠vel seria escolher sempre a a√ß√£o que o seu oponente jogou da √∫ltima vez. Essa segunda pol√≠tica √© chamada **determin√≠stica**, j√° que, para um mesmo estado, a a√ß√£o do nosso agente ser√° sempre a mesma.

![Pol√≠tica](https://miro.medium.com/max/625/0*q6V6Z-LdoTfDQT1y)

Nosso objetivo no Aprendizado por Refor√ßo √© descobrir a **Pol√≠tica √ìtima** para o nosso agente, que consiste na pol√≠tica que escolhe sempre a melhor a√ß√£o para cada estado. Essa melhor a√ß√£o √© definida como a a√ß√£o que vai garantir o maior retorno at√© o final do jogo.

![Pol√≠tica 2](https://miro.medium.com/max/875/0*L9R23HBRbrbLgrFk.gif)

### Valor de um Estado (V)

O **Valor de um Estado** espec√≠fico consiste no retorno esperado a partir daquele determinado estado.

<img src="https://latex.codecogs.com/svg.latex?v_\pi(s)&space;=&space;\mathbf{E}_\pi\left[G_t&space;|&space;S_t&space;=&space;s&space;\right]" title="v_\pi(s) = \mathbf{E}_\pi\left[G_t | S_t = s \right]" />

Basicamente o valor que representa a recompensa total que costumamos receber ap√≥s passar por aquele estado, ou seja, **qu√£o bom √© estar naquele estado**.

Com o **Valor de um Estado**, podemos escolher A√ß√µes que nos levem a Estados que tenham maior Valor. Se o valor de um estado S1 √© maior que o valor de um estado S2, devemos tentar chegar em S1.

![Valor de um Estado](https://miro.medium.com/max/875/0*VOBcy2zUf-1-efUm.jpg)

Esse labirinto √© um √≥timo exemplo de como usamos o **Valor**: se estamos no estado cujo valor √© -15, n√≥s podemos ir para os estados cujos valores s√£o -16, -16 ou -14. Como nesse caso o estado de maior valor √© o de -14, a a√ß√£o que devemos tomar √© ir para cima!

### Valor-A√ß√£o (q)

O **Valor de uma A√ß√£o** consiste no retorno esperado a partir do momento em que se toma aquela a√ß√£o.

<img src="https://latex.codecogs.com/svg.latex?q_\pi(s,&space;a)&space;=&space;\mathbf{E}_\pi\left[G_t&space;|&space;S_t&space;=&space;s,&space;A_t&space;=&space;a&space;\right]" title="q_\pi(s, a) = \mathbf{E}_\pi\left[G_t | S_t = s, A_t = a \right]" />

Dessa forma, o valor q de uma a√ß√£o representa sua **qualidade**, ou qu√£o bom √© tomar aquela a√ß√£o em um determinado estado.

![Valor-A√ß√£o](https://homes.cs.washington.edu/~izadinia/images/QValueUpdate_Q-Learning_epsilonGreedy.gif)

O objetivo de muitos algoritmos de Aprendizado por Refor√ßo √© **estimar** os valores q de cada a√ß√£o, para ent√£o escolher quais a√ß√µes tomar escolhendo aquela de maior q.


### Diagrama

No final, o diagrama que acaba representando a Aprendizado por Refor√ßo √© o seguinte:

![Diagrama](https://miro.medium.com/max/875/0*DcAwmRiUw8shV2Kh.png)

O **Agente** interage com o **Ambiente** por meio de uma **A√ß√£o** escolhida por uma **Pol√≠tica** com base no **Estado** atual, recebendo uma **Recompensa** indicando sua efetividade e o Estado seguinte, assim repetindo o ciclo.